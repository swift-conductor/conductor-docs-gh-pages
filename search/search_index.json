{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"Scalable Workflow Orchestration                 Conductor is a platform for orchestrating workflows that span across microservices.        Get Started NOTE: As of December 13, 2023 Netflix has abandoned the original Conductor OSS project. This is a fork of Conductor that is maintained by Swift Software Group.          Open Source                 Apache-2.0 license for commercial and non-commerical use. Freedom to deploy, modify and contribute back.         Modular                 A fully abstracted backend enables you choose your own database persistence layer and queueing service.         Proven                         Enterprise ready, Java Spring based platform that has been battle tested in production systems.         Control                 Powerful flow control constructs including Decisions, Dynamic Fork-Joins and Subworkflows. Variables and templates are supported.         Polyglot                 Client libraries in multiple languages allows workers to be implemented in Java, Python, Golang, and C#.         Scalable                 Distributed architecture for both orchestrator and workers scalable from a single workflow to millions of concurrent processes.                 Developer Experience        <ul> <li>Discover and visualize the process flows from the bundled UI</li> <li>Integrated interface to create, refine and validate workflows</li> <li>JSON based workflow definition DSL</li> <li>Full featured API for custom automation</li>          Observability        <ul> <li>Understand, debug and iterate on task and workflow executions.</li> <li>Fine grain operational control over workflows with the ability to pause, resume, restart, retry and terminate</li> </ul> Why Conductor?  Service Orchestration          <p>Workflow definitions are decoupled from task implementations. This allows the creation of process flows in which each individual task can be implemented            by an encapsulated microservice.</p>  Service Choreography                     Process flows are implicitly defined across multiple service implementations, often with           tight peer-to-peer coupling between services. Multiple event buses and complex           pub/sub models limit observability around process progress and capacity."},{"location":"devguide/bestpractices.html","title":"Best Practices","text":""},{"location":"devguide/bestpractices.html#response-timeout","title":"Response Timeout","text":"<ul> <li>Configure the responseTimeoutSeconds of each task to be &gt; 0.</li> <li>Should be less than or equal to timeoutSeconds.</li> </ul>"},{"location":"devguide/bestpractices.html#payload-sizes","title":"Payload sizes","text":"<ul> <li>Configure your workflows such that conductor is not used as a persistence store.</li> <li>Ensure that the output data in the task result set in your worker is used by your workflow for execution. If the values in the output payloads are not used by subsequent tasks in your workflow, this data should not be sent back to conductor in the task result.</li> <li>In cases where the output data of your task is used within subsequent tasks in your workflow but is substantially large (&gt; 100KB), consider uploading this data to an object store (S3 or similar) and set the location to the object in your task output. The subsequent tasks can then download this data from the given location and use it during execution.</li> </ul>"},{"location":"devguide/faq.html","title":"Frequently asked Questions","text":""},{"location":"devguide/faq.html#how-do-you-schedule-a-task-to-be-put-in-the-queue-after-some-time-eg-1-hour-1-day-etc","title":"How do you schedule a task to be put in the queue after some time (e.g. 1 hour, 1 day etc.)","text":"<p>After polling for the task update the status of the task to <code>IN_PROGRESS</code> and set the <code>callbackAfterSeconds</code> value to the desired time.  The task will remain in the queue until the specified second before worker polling for it will receive it again.</p> <p>If there is a timeout set for the task, and the <code>callbackAfterSeconds</code> exceeds the timeout value, it will result in task being TIMED_OUT.</p>"},{"location":"devguide/faq.html#how-long-can-a-workflow-be-in-running-state-can-i-have-a-workflow-that-keeps-running-for-days-or-months","title":"How long can a workflow be in running state?  Can I have a workflow that keeps running for days or months?","text":"<p>Yes.  As long as the timeouts on the tasks are set to handle long running workflows, it will stay in running state.</p>"},{"location":"devguide/faq.html#my-workflow-fails-to-start-with-missing-task-error","title":"My workflow fails to start with missing task error","text":"<p>Ensure all the tasks are registered via <code>/metadata/taskdefs</code> APIs.  Add any missing task definition (as reported in the error) and try again.</p>"},{"location":"devguide/faq.html#where-does-my-worker-run-how-does-conductor-run-my-tasks","title":"Where does my worker run?  How does conductor run my tasks?","text":"<p>Conductor does not run the workers.  When a task is scheduled, it is put into the queue maintained by Conductor.  Workers are required to poll for tasks using <code>/tasks/poll</code> API at periodic interval, execute the business logic for the task and report back the results using <code>POST /api/tasks</code> API call.  Conductor, however will run system tasks on the Conductor server.</p>"},{"location":"devguide/faq.html#how-can-i-schedule-workflows-to-run-at-a-specific-time","title":"How can I schedule workflows to run at a specific time?","text":"<p>Conductor itself does not provide any scheduling mechanism.  But there is a community project Schedule Conductor Workflows which provides workflow scheduling capability as a pluggable module as well as workflow server. Other way is you can use any of the available scheduling systems to make REST calls to Conductor to start a workflow.  Alternatively, publish a message to a supported eventing system like SQS to trigger a workflow. More details about eventing.</p>"},{"location":"devguide/faq.html#how-do-i-setup-dynomite-cluster","title":"How do I setup Dynomite cluster?","text":"<p>Visit Dynomite's Github page to find details on setup and support mechanism.</p>"},{"location":"devguide/faq.html#can-i-use-conductor-with-ruby-go-python","title":"Can I use conductor with Ruby / Go / Python?","text":"<p>Yes.  Workers can be written any language as long as they can poll and update the task results via HTTP endpoints.</p> <p>Conductor provides frameworks for Java and Python to simplify the task of polling and updating the status back to Conductor server.</p> <p>Note: Python and Go clients have been contributed by the community.</p>"},{"location":"devguide/faq.html#how-can-i-get-help-with-dynomite","title":"How can I get help with Dynomite?","text":"<p>Visit Dynomite's Github page to find details on setup and support mechanism.</p>"},{"location":"devguide/faq.html#my-workflow-is-running-and-the-task-is-scheduled-but-it-is-not-being-processed","title":"My workflow is running and the task is SCHEDULED but it is not being processed.","text":"<p>Make sure that the worker is actively polling for this task. Navigate to the <code>Task Queues</code> tab on the Conductor UI and select your task name in the search box. Ensure that <code>Last Poll Time</code> for this task is current.</p> <p>In Conductor 3.x, <code>conductor.redis.availabilityZone</code> defaults to <code>us-east-1c</code>.  Ensure that this matches where your workers are, and that it also matches<code>conductor.redis.hosts</code>.</p>"},{"location":"devguide/faq.html#how-do-i-configure-a-notification-when-my-workflow-completes-or-fails","title":"How do I configure a notification when my workflow completes or fails?","text":"<p>When a workflow fails, you can configure a \"failure workflow\" to run using the<code>failureWorkflow</code> parameter. By default, three parameters are passed:</p> <ul> <li>reason</li> <li>workflowId: use this to pull the details of the failed workflow.</li> <li>failureStatus</li> </ul> <p>You can also use the Workflow Status Listener: </p> <ul> <li>Set the workflowStatusListenerEnabled field in your workflow definition to true which enables notifications.</li> <li>Add a custom implementation of the Workflow Status Listener. Refer this.</li> <li>This notification can be implemented in such a way as to either send a notification to an external system or to send an event on the conductor queue to complete/fail another task in another workflow as described here.</li> </ul> <p>Refer to this documentation to extend conductor to send out events/notifications upon workflow completion/failure. </p>"},{"location":"devguide/faq.html#i-want-my-worker-to-stop-polling-and-executing-tasks-when-the-process-is-being-terminated-java-client","title":"I want my worker to stop polling and executing tasks when the process is being terminated. (Java client)","text":"<p>In a <code>PreDestroy</code> block within your application, call the <code>shutdown()</code> method on the <code>TaskRunnerConfigurer</code> instance that you have created to facilitate a graceful shutdown of your worker in case the process is being terminated.</p>"},{"location":"devguide/faq.html#can-i-exit-early-from-a-task-without-executing-the-configured-automatic-retries-in-the-task-definition","title":"Can I exit early from a task without executing the configured automatic retries in the task definition?","text":"<p>Set the status to <code>FAILED_WITH_TERMINAL_ERROR</code> in the TaskResult object within your worker. This would mark the task as FAILED and fail the workflow without retrying the task as a fail-fast mechanism.</p>"},{"location":"devguide/concepts/index.html","title":"Introduction","text":"<p>Conductor allows you to build a complex application using simple and granular tasks that do not need to be aware of or keep track of the state of your application's execution flow. Conductor keeps track of the state, calls tasks in the right order (sequentially or in parallel, as defined by you), retry calls if needed, handle failure scenarios gracefully, and outputs the final result.</p> <p></p> <p>Leveraging workflows in Conductor enables developers to truly focus on their core mission - building their application code in the languages of their choice. Conductor does the heavy lifting associated with ensuring high reliability, transactional consistency, and long durability of their workflows. Simply put, wherever your application's component lives and whichever languages they were written in, you can build a workflow in Conductor to orchestrate their execution in a reliable &amp; scalable manner.</p> <p>Workflows and Tasks are the two key concepts that underlie the Conductor system. </p>"},{"location":"devguide/concepts/tasks.html","title":"Tasks","text":"<p>Tasks are the building blocks of Conductor Workflows. There must be at least one task configured in each Workflow Definition. A typical Conductor workflow defines a lists of tasks that are executed until the completion or termination of the workflow.</p> <p>Tasks can be categorized into three types: </p>"},{"location":"devguide/concepts/tasks.html#types-of-tasks","title":"Types of Tasks","text":""},{"location":"devguide/concepts/tasks.html#system-tasks","title":"System Tasks","text":"<p>System Tasks are built-in tasks that are general purpose and re-usable. They are executed within the JVM of the Conductor server and managed by Conductor for execution and scalability. Such tasks allow you to get started without having to write custom workers. </p>"},{"location":"devguide/concepts/tasks.html#worker-tasks","title":"Worker Tasks","text":"<p>Worker Tasks or Simple Tasks are implemented by your application and run in a separate environment from Conductor. These tasks talk to the Conductor server via REST/gRPC to poll for tasks and update its status after execution.</p>"},{"location":"devguide/concepts/tasks.html#operators","title":"Operators","text":"<p>Operators are built-in primitives in Conductor that allow you control the flow of tasks in your workflow. Operators are similar to programming constructs such as <code>for</code> loops, <code>switch</code> blocks, etc.</p>"},{"location":"devguide/concepts/tasks.html#task-configuration","title":"Task Configuration","text":"<p>Task Configurations appear within the <code>tasks</code> array property of the Workflow Definition. This array is the blueprint that describes how a workflow will process an input payload by passing it through successive tasks.</p> <ul> <li>For all tasks, the configuration will specify what input parameters the task takes. </li> <li>For CUSTOM (worker based) tasks, the configuration will contain a reference to a registered worker <code>taskName</code>. </li> <li>For System Tasks and Operators, the task configuration will contain important parameters that control the behavior of the task. For example, the task configuration of an HTTP task will specify an endpoint URL and the template payload that it will be called with when the task executes.</li> </ul>"},{"location":"devguide/concepts/tasks.html#task-definition","title":"Task Definition","text":"<p>Not to be confused with Task Configurations, Task Definitions help define default task level parameters like inputs and outputs, timeouts, retries etc. for CUSTOM (i.e. worker implemented) tasks.</p> <ul> <li>All simple tasks need to be registered before they can be used by active workflows.</li> <li>Task definitions can be registered via the UI, or through the API.</li> <li>A registered task definition can be referenced from within different workflows.</li> </ul>"},{"location":"devguide/concepts/tasks.html#task-execution","title":"Task Execution","text":"<p>Each time a workload is passed into a configured task, a Task Execution object is created. This object has a unique ID and represents the result of the operation. This includes the status (i.e. whether the task was completed successfully), and any input, output and variables associated with the task. </p>"},{"location":"devguide/concepts/workers.html","title":"Workers","text":"<p>A worker is responsible for executing a custom task. Workers can be implemented in any language, and Swift Conductor comes with Client SDKs that provide features such as polling threads, metrics, and server communication that makes creating workers easy.</p> <p>Each worker adheres to the the Microservice design pattern and follows certain basic principles:</p> <ol> <li>Workers are stateless and do not implement a workflow specific logic.  </li> <li>Each worker executes a very specific task and produces well defined output given specific inputs.</li> <li>Workers are meant to be idempotent (or should handle cases where the task that partially executed gets rescheduled due to timeouts etc.)</li> <li>Workers do not implement the logic to handle retries etc, that is taken care by the Conductor server.</li> </ol> <p>Conductor maintains a registry of custom tasks.  A task MUST be registered before being used in a workflow. This can be done by creating and saving a Task Definition.</p>"},{"location":"devguide/concepts/workflows.html","title":"Workflows","text":"<p>We will talk about two distinct topics, defining a workflow and executing a workflow.</p>"},{"location":"devguide/concepts/workflows.html#workflow-definition","title":"Workflow Definition","text":"<p>The Workflow Definition is the Conductor primitive that encompasses the flow of your business logic. It contains all the information necessary to describe the behavior of a workflow.</p> <p>A Workflow Definition contains a collection of Task Configurations. This is the blueprint which specifies the order of execution of tasks within a workflow. This blueprint also specifies how data/state is passed from one task to another (using task input/output parameters).</p> <p>Additionally, the Workflow Definition contains metadata regulating the runtime behavior workflow, such what input and output parameters are expected for the entire workflow, and the workflow's the timeout and retry settings.</p>"},{"location":"devguide/concepts/workflows.html#workflow-execution","title":"Workflow Execution","text":"<p>If Workflow Definitions are like OOP classes, then Workflows Executions are like object instances. Each time a Workflow Definition is invoked with a given input, a new Workflow Execution with a unique ID is created. Definitions to Executions have a 1:N relationship.</p>"},{"location":"devguide/how-tos/Monitoring/Conductor-LogLevel.html","title":"Conductor Log Level","text":"<p>Conductor is based on Spring Boot, so the log levels are set via Spring Boot properties.</p> <p>From the Spring Boot Docs:</p> <p>All the supported logging systems can have the logger levels set in the Spring Environment (for example, in application.properties) by using <code>logging.level.&lt;logger-name&gt;=&lt;level&gt;</code> where level is one of TRACE, DEBUG, INFO, WARN, ERROR, FATAL, or OFF. The <code>root</code> logger can be configured by using logging.level.root.</p> <p>The following example shows potential logging settings in <code>application.properties</code>:</p> <pre><code>logging.level.root=warn\nlogging.level.org.springframework.web=debug\nlogging.level.org.hibernate=error\n</code></pre> <p>It\u2019s also possible to set logging levels using environment variables. For example, <code>LOGGING_LEVEL_ORG_SPRINGFRAMEWORK_WEB=DEBUG</code> will set <code>org.springframework.web</code> to <code>DEBUG</code>.</p>"},{"location":"devguide/how-tos/Tasks/creating-tasks.html","title":"Creating Task Definitions","text":"<p>Tasks can be created using the tasks metadata API</p> <p><code>POST /api/metadata/taskdefs</code></p> <p>This API takes an array of new task definitions.</p>"},{"location":"devguide/how-tos/Tasks/creating-tasks.html#examples","title":"Examples","text":""},{"location":"devguide/how-tos/Tasks/creating-tasks.html#example-using-curl","title":"Example using curl","text":"<pre><code>curl 'http://localhost:8080/api/metadata/taskdefs' \\\n  -H 'accept: */*' \\\n  -H 'content-type: application/json' \\\n  --data-raw '[{\"createdBy\":\"user\",\"name\":\"sample_task_name_1\",\"description\":\"This is a sample task for demo\",\"responseTimeoutSeconds\":10,\"timeoutSeconds\":30,\"inputKeys\":[],\"outputKeys\":[],\"timeoutPolicy\":\"TIME_OUT_WF\",\"retryCount\":3,\"retryLogic\":\"FIXED\",\"retryDelaySeconds\":5,\"inputTemplate\":{},\"rateLimitPerFrequency\":0,\"rateLimitFrequencyInSeconds\":1}]'\n</code></pre>"},{"location":"devguide/how-tos/Tasks/creating-tasks.html#example-using-node-fetch","title":"Example using node fetch","text":"<pre><code>fetch(\"http://localhost:8080/api/metadata/taskdefs\", {\n    \"headers\": {\n        \"accept\": \"*/*\",\n        \"content-type\": \"application/json\",\n    },\n    \"body\": \"[{\\\"createdBy\\\":\\\"user\\\",\\\"name\\\":\\\"sample_task_name_1\\\",\\\"description\\\":\\\"This is a sample task for demo\\\",\\\"responseTimeoutSeconds\\\":10,\\\"timeoutSeconds\\\":30,\\\"inputKeys\\\":[],\\\"outputKeys\\\":[],\\\"timeoutPolicy\\\":\\\"TIME_OUT_WF\\\",\\\"retryCount\\\":3,\\\"retryLogic\\\":\\\"FIXED\\\",\\\"retryDelaySeconds\\\":5,\\\"inputTemplate\\\":{},\\\"rateLimitPerFrequency\\\":0,\\\"rateLimitFrequencyInSeconds\\\":1}]\",\n    \"method\": \"POST\"\n});\n</code></pre>"},{"location":"devguide/how-tos/Tasks/creating-tasks.html#best-practices","title":"Best Practices","text":"<ol> <li>You can update a set of tasks together in this API</li> <li>Task configurations are important attributes that controls the behavior of this task in a Workflow. Refer to Task Configurations for all the options and details' </li> <li>You can also use the Conductor Swagger UI to update the tasks</li> </ol>"},{"location":"devguide/how-tos/Tasks/dynamic-vs-switch-tasks.html","title":"Dynamic vs Switch Tasks","text":"<p>Dynamic Tasks are useful in situations when need to run a task of which the task type is determined at runtime instead of during the configuration. It is similar to the <code>SWITCH</code> use case but with <code>DYNAMIC</code> we won't need to preconfigure all case options in the workflow definition itself. Instead, we can mark the task as <code>DYNAMIC</code> and determine which underlying task does it run during the workflow execution itself.</p> <ul> <li>Use DYNAMIC task as a replacement for SWITCH if you have too many case options</li> <li>DYNAMIC task is an option when you want to programmatically determine the next task to run instead of using expressions</li> <li>DYNAMIC task simplifies the workflow execution UI view which will now only show the selected task</li> <li>SWITCH task visualization is helpful as a documentation - showing you all options that the workflow could have    taken</li> <li>SWITCH task comes with a default task option which can be useful in some use cases</li> </ul> <p>Learn more about</p> <ul> <li>Dynamic Tasks</li> <li>Switch Tasks</li> </ul>"},{"location":"devguide/how-tos/Tasks/extending-system-tasks.html","title":"Extending System Tasks","text":"<p>System tasks allow Conductor to run simple tasks on the server - removing the need to build (and deploy) workers for basic tasks.  This allows for automating more mundane tasks without building specific microservices for them.</p> <p>However, sometimes it might be necessary to add additional parameters to a System Task to gain the behavior that is desired.</p>"},{"location":"devguide/how-tos/Tasks/extending-system-tasks.html#example-http-task","title":"Example HTTP Task","text":"<pre><code>{\n  \"name\": \"get_weather_90210\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"get_weather_90210\",\n      \"taskReferenceName\": \"get_weather_90210\",\n      \"inputParameters\": {\n        \"http_request\": {\n          \"uri\": \"https://weatherdbi.herokuapp.com/data/weather/90210\",\n          \"method\": \"GET\",\n          \"connectionTimeOut\": 1300,\n          \"readTimeOut\": 1300\n        }\n      },\n      \"type\": \"HTTP\",\n      \"decisionCases\": {},\n      \"defaultCase\": [],\n      \"forkTasks\": [],\n      \"startDelay\": 0,\n      \"joinOn\": [],\n      \"optional\": false,\n      \"defaultExclusiveJoinTask\": [],\n      \"asyncComplete\": false,\n      \"loopOver\": []\n    }\n  ],\n  \"inputParameters\": [],\n  \"outputParameters\": {\n    \"data\": \"${get_weather_ref.output.response.body.currentConditions.comment}\"\n  },\n  \"schemaVersion\": 2,\n  \"restartable\": true,\n  \"workflowStatusListenerEnabled\": false,\n  \"ownerEmail\": \"conductor@example.com\",\n  \"timeoutPolicy\": \"ALERT_ONLY\",\n  \"timeoutSeconds\": 0,\n  \"variables\": {},\n  \"inputTemplate\": {}\n}\n</code></pre> <p>This very simple workflow has a single HTTP Task inside.  No parameters need to be passed, and when run, the HTTP task will return the weather in Beverly Hills, CA (Zip code = 90210).</p> <p>This API has a very slow response time. In the HTTP task, the connection is set to time out after 1300ms, which is too short for this API, resulting in a timeout.  This API will work if we allowed for a longer timeout, but in order to demonstrate adding retries to the HTTP Task, we will artificially force the API call to fail.</p> <p>When this workflow is run - it fails, as expected.</p> <p>Now, sometimes an API call might fail due to an issue on the remote server, and retrying the call will result in a response.  With many Conductor tasks,  <code>retryCount</code>, <code>retryDelaySeconds</code> and <code>retryLogic</code> fields can be applied to retry the worker (with the desired parameters).</p> <p>By default, the HTTP Task does not have <code>retryCount</code>, <code>retryDelaySeconds</code> or <code>retryLogic</code> built in.  Attempting to add these parameters to a HTTP Task results in an error.</p>"},{"location":"devguide/how-tos/Tasks/extending-system-tasks.html#the-solution","title":"The Solution","text":"<p>We can create a task with the same name with the desired parameters.  Defining the following task (note that the <code>name</code> is identical to the one in the workflow):</p> <pre><code>{\n\n  \"createdBy\": \"\",\n  \"name\": \"get_weather_90210\",\n  \"description\": \"editing HTTP task\",\n  \"retryCount\": 3,\n  \"timeoutSeconds\": 5,\n  \"inputKeys\": [],\n  \"outputKeys\": [],\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 5,\n  \"responseTimeoutSeconds\": 5,\n  \"inputTemplate\": {},\n  \"rateLimitPerFrequency\": 0,\n  \"rateLimitFrequencyInSeconds\": 1\n}\n</code></pre> <p>We've added the three parameters: <code>retryCount: 3, retryDelaySeconds: 5, retryLogic: FIXED</code></p> <p>The <code>get_weather_90210</code> task will now run 4 times (it will fail once, and then retry 3 times), with a <code>FIXED</code> 5 second delay between attempts.</p> <p>Re-running the task (and looking at the timeline view) shows that this is what occurs.  There are 4 attempts, with a 5 second delay between them.</p> <p>If we change the <code>retryLogic</code> to EXPONENTIAL_BACKOFF, the delay between attempts grows exponentially:</p> <ol> <li>5*2^0 = 5 seconds</li> <li>5*2^1 = 10 seconds</li> <li>5*2^2 = 20 seconds</li> </ol>"},{"location":"devguide/how-tos/Tasks/monitoring-task-queues.html","title":"Monitoring Task Queues","text":"<p>Conductor offers an API and UI interface to monitor the task queues. This is useful to see details of the number of workers polling and monitoring the queue backlog.</p>"},{"location":"devguide/how-tos/Tasks/monitoring-task-queues.html#using-the-ui","title":"Using the UI","text":"<pre><code>&lt;your UI server URL&gt;/taskQueue\n</code></pre> <p>Access this screen via - Home &gt; Task Queues</p> <p>On this screen you can select and view the details of the task queue. The following information is shown:</p> <ol> <li>Queue Size - The number of tasks waiting to be executed</li> <li>Workers - The count and list of works and their instance reference who are polling for work for this task</li> </ol>"},{"location":"devguide/how-tos/Tasks/monitoring-task-queues.html#using-apis","title":"Using APIs","text":"<p>To see the size of the task queue via API:</p> <pre><code>curl 'http://localhost:8080/api/tasks/queue/sizes?taskType=&lt;TASK_NAME&gt;' \\\n  -H 'accept: */*' \n</code></pre> <p>To see the worker poll information of the task queue via API:</p> <pre><code>curl 'http://localhost:8080/api/tasks/queue/polldata?taskType=&lt;TASK_NAME&gt;' \\\n  -H 'accept: */*'\n</code></pre> <p>Note</p> <p>Replace <code>&lt;TASK_NAME&gt;</code> with your task name</p>"},{"location":"devguide/how-tos/Tasks/reusing-tasks.html","title":"Reusing Tasks","text":"<p>A powerful feature of Conductor is that it supports and enables re-usability out of the box. Task workers typically perform a unit of work and is usually a part of a larger workflow. Such workers are often re-usable in multiple workflows. Once a task is defined, you can use it across as any workflow.</p> <p>When re-using tasks, it's important to think of situations that a multi-tenant system faces. All the work assigned to this worker by default goes to the same task scheduling queue. This could result in your worker not being polled quickly if there is a noisy neighbour in the ecosystem. One way you can tackle this situation is by re-using the worker code, but having different task names registered for different use cases. And for each task name, you can run an appropriate number of workers based on expected load.</p>"},{"location":"devguide/how-tos/Tasks/task-configurations.html","title":"Task Configurations","text":"<p>Refer to Task Definitions for details on how to configure task definitions</p>"},{"location":"devguide/how-tos/Tasks/task-configurations.html#example","title":"Example","text":"<p>Here is a task template payload with commonly used fields:</p> <pre><code>{\n  \"createdBy\": \"user\",\n  \"name\": \"sample_task_name_1\",\n  \"description\": \"This is a sample task for demo\",\n  \"responseTimeoutSeconds\": 10,\n  \"timeoutSeconds\": 30,\n  \"inputKeys\": [],\n  \"outputKeys\": [],\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryCount\": 3,\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 5,\n  \"inputTemplate\": {},\n  \"rateLimitPerFrequency\": 0,\n  \"rateLimitFrequencyInSeconds\": 1\n}\n</code></pre>"},{"location":"devguide/how-tos/Tasks/task-configurations.html#best-practices","title":"Best Practices","text":"<ol> <li>Refer to Task Timeouts for additional information on how the various timeout settings work</li> <li>Refer to Monitoring Task Queues on how to monitor task queues</li> </ol>"},{"location":"devguide/how-tos/Tasks/task-inputs.html","title":"Task Inputs","text":"<p>Task inputs can be provided in multiple ways. This is configured in the workflow definition when a task is participating in the workflow.</p>"},{"location":"devguide/how-tos/Tasks/task-inputs.html#inputs-referred-from-workflow-inputs","title":"Inputs referred from Workflow inputs","text":"<p>When we start a workflow, we can provide inputs to the workflow in a json format. For example:</p> <pre><code>{\n  \"worfklowInputNumberExample\": 1,\n  \"worfklowInputTextExample\": \"SAMPLE\",\n  \"worfklowInputJsonExample\": {\n    \"nestedKey\": \"nestedValue\"\n  }\n}\n</code></pre> <p>These values can be referred as inputs into your task using the following expression:</p> <pre><code>{\n  \"taskInput1Key\": \"${workflow.input.worfklowInputNumberExample}\",\n  \"taskInput2Key\": \"${workflow.input.worfklowInputJsonExample.nestedKey}\"\n}\n</code></pre> <p>In this example, the tasks will receive the following inputs after they are evaluated: <pre><code>{\n  \"taskInput1Key\": 1,\n  \"taskInput2Key\": \"nestedValue\"\n}\n</code></pre></p>"},{"location":"devguide/how-tos/Tasks/task-inputs.html#inputs-referred-from-other-task-outputs","title":"Inputs referred from other Task outputs","text":"<p>Similar to how we can refer to workflow inputs, we can also refer to an output field that was generated by a task that executed before.</p> <p>Let's assume a task with the task reference name <code>previousTaskReference</code> executed and produced the following output:</p> <pre><code>{\n  \"taskOutputKey1\": \"outputValue\",\n  \"taskOutputKey2\": {\n    \"nestedKey1\": \"outputValue-1\"\n  }\n}\n</code></pre> <p>We can refer to these as the new task's input by using the following expression:</p> <pre><code>{\n  \"taskInput1Key\": \"${previousTaskReference.output.taskOutputKey1}\",\n  \"taskInput2Key\": \"${previousTaskReference.output.taskOutputKey2.nestedKey1}\"\n}\n</code></pre> <p>The expression format is based on Json Path and you can construct complex input params based on the syntax.</p>"},{"location":"devguide/how-tos/Tasks/task-inputs.html#hard-coded-inputs","title":"Hard coded inputs","text":"<p>Task inputs can also be hard coded in the workflow definitions. This is useful when you have a re-usable task which has configurable options that can be applied in different workflow contexts.</p> <pre><code>{\n  \"taskInput1\": \"OPTION_A\",\n  \"taskInput2\": 100\n}\n</code></pre>"},{"location":"devguide/how-tos/Tasks/task-timeouts.html","title":"Task Timeouts","text":"<p>Tasks can be configured to handle various scenarios of timeouts. Here are some scenarios and the relevance configuration fields.</p> Scenario Configuration A task worker picked up the task, but fails to respond back with an update <code>responseTimeoutSeconds</code> A task worker picked up the task and updates progress, but fails to complete within an expected timeframe <code>timeoutSeconds</code> A task is stuck in a retry loop with repeated failures beyond an expected timeframe <code>timeoutSeconds</code> Task doesn't get picked by any workers for a specific amount of time <code>pollTimeoutSeconds</code> Task isn't completed within a specified amount of time despite being picked up by task workers <code>timeoutSeconds</code> <p><code>timeoutSeconds</code> should always be greater than <code>responseTimeoutSeconds</code></p>"},{"location":"devguide/how-tos/Tasks/task-timeouts.html#timeout-seconds","title":"Timeout Seconds","text":"<pre><code>\"timeoutSeconds\" : 30\n</code></pre> <p>When configured with a value &gt; <code>0</code>, the system will wait for this task to complete successfully up until this number of seconds from when the task is first polled. We can use this to fail a workflow when a task breaches the overall SLA for completion.</p>"},{"location":"devguide/how-tos/Tasks/task-timeouts.html#response-timeout-seconds","title":"Response Timeout Seconds","text":"<pre><code>\"responseTimeoutSeconds\" : 10\n</code></pre> <p>When configured with a value &gt; <code>0</code>, the system will wait for this number of seconds from when the task is polled before the worker updates back with a status. The worker can keep the task in <code>IN_PROGRESS</code> state if it requires more time to complete.</p>"},{"location":"devguide/how-tos/Tasks/task-timeouts.html#poll-timeout-seconds","title":"Poll Timeout Seconds","text":"<pre><code>\"pollTimeoutSeconds\" : 10\n</code></pre> <p>When configured with a value &gt; <code>0</code>, the system will wait for this number of seconds for the task to be picked up by a task worker. Useful when you want to detect a backlogged task queue with not enough workers.</p>"},{"location":"devguide/how-tos/Tasks/updating-tasks.html","title":"Updating Task Definitions","text":"<p>Updates to the task definitions can be made using the following API</p> <pre><code>PUT /api/metadata/taskdefs\n</code></pre> <p>This API takes a single task definition and updates itself. </p>"},{"location":"devguide/how-tos/Tasks/updating-tasks.html#examples","title":"Examples","text":""},{"location":"devguide/how-tos/Tasks/updating-tasks.html#example-using-curl","title":"Example using curl","text":"<pre><code>curl 'http://localhost:8080/api/metadata/taskdefs' \\\n  -X 'PUT' \\\n  -H 'accept: */*' \\\n  -H 'content-type: application/json' \\\n  --data-raw '{\"createdBy\":\"user\",\"name\":\"sample_task_name_1\",\"description\":\"This is a sample task for demo\",\"responseTimeoutSeconds\":10,\"timeoutSeconds\":30,\"inputKeys\":[],\"outputKeys\":[],\"timeoutPolicy\":\"TIME_OUT_WF\",\"retryCount\":3,\"retryLogic\":\"FIXED\",\"retryDelaySeconds\":5,\"inputTemplate\":{},\"rateLimitPerFrequency\":0,\"rateLimitFrequencyInSeconds\":1}'\n</code></pre>"},{"location":"devguide/how-tos/Tasks/updating-tasks.html#example-using-node-fetch","title":"Example using node fetch","text":"<pre><code>fetch(\"http://localhost:8080/api/metadata/taskdefs\", {\n    \"headers\": {\n        \"accept\": \"*/*\",\n        \"content-type\": \"application/json\",\n    },\n    \"body\": \"{\\\"createdBy\\\":\\\"user\\\",\\\"name\\\":\\\"sample_task_name_1\\\",\\\"description\\\":\\\"This is a sample task for demo\\\",\\\"responseTimeoutSeconds\\\":10,\\\"timeoutSeconds\\\":30,\\\"inputKeys\\\":[],\\\"outputKeys\\\":[],\\\"timeoutPolicy\\\":\\\"TIME_OUT_WF\\\",\\\"retryCount\\\":3,\\\"retryLogic\\\":\\\"FIXED\\\",\\\"retryDelaySeconds\\\":5,\\\"inputTemplate\\\":{},\\\"rateLimitPerFrequency\\\":0,\\\"rateLimitFrequencyInSeconds\\\":1}\",\n    \"method\": \"PUT\"\n});\n</code></pre>"},{"location":"devguide/how-tos/Tasks/updating-tasks.html#best-practices","title":"Best Practices","text":"<ol> <li>You can also use the Conductor Swagger UI to update the tasks </li> <li>Task configurations are important attributes that controls the behavior of this task in a Workflow. Refer to Task Configurations for all the options and details'</li> </ol>"},{"location":"devguide/how-tos/Workers/build-a-golang-task-worker.html","title":"Build a Go Task Worker","text":"<p>Writing Workers with the Go SDK</p>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html","title":"Build a Java Task Worker","text":"<p>This guide provides introduction to building Task Workers in Java.</p>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#dependencies","title":"Dependencies","text":"<p>Conductor provides Java client libraries, which we will use to build a simple task worker.</p>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#maven-dependency","title":"Maven Dependency","text":"<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.swiftconductor.conductor&lt;/groupId&gt;\n    &lt;artifactId&gt;conductor-client&lt;/artifactId&gt;\n    &lt;version&gt;3.16-SNAPSHOT&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.swiftconductor.conductor&lt;/groupId&gt;\n    &lt;artifactId&gt;3.16-SNAPSHOT&lt;/artifactId&gt;\n    &lt;version&gt;3.16&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#gradle","title":"Gradle","text":"<pre><code>implementation group: 'com.swiftconductor.conductor', name: 'conductor-client', version: '3.16'\nimplementation group: 'com.swiftconductor.conductor', name: 'conductor-common', version: '3.16'\n</code></pre>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#implementing-a-task-worker","title":"Implementing a Task Worker","text":"<p>To create a worker, implement the <code>Worker</code> interface.</p> <pre><code>public class SampleWorker implements Worker {\n\n    private final String taskDefName;\n\n    public SampleWorker(String taskDefName) {\n        this.taskDefName = taskDefName;\n    }\n\n    @Override\n    public String getTaskDefName() {\n        return taskDefName;\n    }\n\n    @Override\n    public TaskResult execute(Task task) {\n        TaskResult result = new TaskResult(task);\n        result.setStatus(Status.COMPLETED);\n\n        //Register the output of the task\n        result.getOutputData().put(\"outputKey1\", \"value\");\n        result.getOutputData().put(\"oddEven\", 1);\n        result.getOutputData().put(\"mod\", 4);\n\n        return result;\n    }\n}\n</code></pre>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#implementing-workers-logic","title":"Implementing worker's logic","text":"<p>Worker's core implementation logic goes in the <code>execute</code> method. Upon completion, set the <code>TaskResult</code> with status as one of the following:</p> <ol> <li>COMPLETED: If the task has completed successfully.</li> <li>FAILED: If there are failures - business or system failures. Based on the task's configuration, when a task fails, it may be retried.</li> </ol> <p>The <code>getTaskDefName()</code> method returns the name of the task for which this worker provides the execution logic.</p> <p>See SampleWorker.java for the complete example.</p>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#configuring-polling-using-taskrunnerconfigurer","title":"Configuring polling using TaskRunnerConfigurer","text":"<p>The <code>TaskRunnerConfigurer</code> can be used to register the worker(s) and initialize the polling loop. It manages the task workers thread pool and server communication (poll and task update).</p> <p>Use the Builder to create an instance of the <code>TaskRunnerConfigurer</code>. The builder accepts the following parameters:</p> <pre><code> TaskClient taskClient = new TaskClient();\n taskClient.setRootURI(\"http://localhost:8080/api/\");        //Point this to the server API\n\n        int threadCount = 2;            //number of threads used to execute workers.  To avoid starvation, should be same or more than number of workers\n\n        Worker worker1 = new SampleWorker(\"task_1\");\n        Worker worker2 = new SampleWorker(\"task_5\");\n\n        // Create TaskRunnerConfigurer\n        TaskRunnerConfigurer configurer = new TaskRunnerConfigurer.Builder(taskClient, Arrays.asList(worker1, worker2))\n            .withThreadCount(threadCount)\n            .build();\n\n        // Start the polling and execution of tasks\n        configurer.init();\n</code></pre> <p>See Sample for full example.</p>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#configuration-details","title":"Configuration Details","text":"<p>Initialize the <code>Builder</code> with the following:</p> Parameter Description TaskClient TaskClient used to communicate with the Conductor server Workers Workers that will be used for polling work and task execution. Parameter Description Default withEurekaClient EurekaClient is used to identify if the server is in discovery or not. When the server goes out of discovery, the polling is stopped unless <code>pollOutOfDiscovery</code> is set to true. If passed null, discovery check is not done. provided by platform withThreadCount Number of threads assigned to the workers. Should be at-least the size of taskWorkers to avoid starvation in a busy system. Number of registered workers withSleepWhenRetry Time in milliseconds, for which the thread should sleep when task update call fails, before retrying the operation. 500 withUpdateRetryCount Number of attempts to be made when updating task status when update status call fails. 3 withWorkerNamePrefix String prefix that will be used for all the workers. workflow-worker- <p>Once an instance is created, call <code>init()</code> method to initialize the <code>TaskPollExecutor</code> and begin the polling and execution of tasks.</p> <p>Note</p> <p>To ensure that the <code>TaskRunnerConfigurer</code> stops polling for tasks when the instance becomes unhealthy, call the provided <code>shutdown()</code> hook in a <code>PreDestroy</code> block.</p>"},{"location":"devguide/how-tos/Workers/build-a-java-task-worker.html#properties","title":"Properties","text":"<p>The worker behavior can be further controlled by using these properties:</p> Property Type Description Default paused boolean If set to true, the worker stops polling. false pollInterval int Interval in milliseconds at which the server should be polled for tasks. 1000 pollOutOfDiscovery boolean If set to true, the instance will poll for tasks regardless of the discovery status. This is useful while running on a dev machine. false <p>Further, these properties can be set either by a <code>Worker</code> implementation or by setting the following system properties in the JVM:</p> Name Description <code>conductor.worker.&lt;property&gt;</code> Applies to ALL the workers in the JVM. <code>conductor.worker.&lt;taskDefName&gt;.&lt;property&gt;</code> Applies to the specified worker.  Overrides the global property."},{"location":"devguide/how-tos/Workers/build-a-python-task-worker.html","title":"Build a Python Task Worker","text":"<p>Create and run task workers</p>"},{"location":"devguide/how-tos/Workflows/debugging-workflows.html","title":"Debugging Workflows","text":"<p>Conductor UI is a tool that we can leverage for debugging issues. Refer to the following articles to search and view your workflow execution.</p> <ol> <li>Searching Workflows</li> <li>View Workflow Executions</li> </ol>"},{"location":"devguide/how-tos/Workflows/debugging-workflows.html#debugging-executions","title":"Debugging Executions","text":"<p>Open the Tasks &gt; Diagram tab to see the diagram of the overall workflow execution</p> <p>If there is a failure, you will them on the view marked as red. In most cases it should be clear what went wrong from the view itself. To see details of the failure, you can click on the failed task.</p> <p>The following fields are useful in debugging</p> Field Name Description Task Detail &gt; Summary &gt; Reason for Incompletion If an exception was thrown by the worker, it will be captured and displayed here Task Detail &gt; Summary &gt; Worker The worker instance id where this failure last occurred. Useful to dig for detailed logs if not already captured by Conductor Task Detail &gt; Input Verify if the task inputs were computed and provided correctly to the task Task Detail &gt; Output If output of a previous task is used as an input to your next task, refer here for what was produced Task Detail &gt; Logs If your task is supplying logs, we can look at that here Task Detail &gt; Retried Task - Select an instance If your task was retried, we can see all the attempts and correponding details here <p>Note: We can also access the task list from Tasks &gt; Task List tab.</p> <p>Here is a screen grab of the fields referred above.</p> <p></p>"},{"location":"devguide/how-tos/Workflows/debugging-workflows.html#recovering-from-failures","title":"Recovering From Failures","text":"<p>Once we have resolved the underlying issue of workflow execution failure, we might want to replay or retry failed workflows. The UI has functions that would allow us to do this:</p> <p>The Actions button provides the following options:</p> Action Name Description Restart with Current Definitions Restart this workflow from the beginning using the same version of the workflow definition that originally ran this workflow execution. This is useful if the workflow definition has changed and we want to retain this instance to the original version Restart with Latest Definitions Restart this workflow from the beginning using the latest definition of the workflow. If we made changes to definition, we can use this option to re-run this flow with the latest version Retry - From failed task Retry this workflow from the failed task <p></p> <p>Note: Conductor configurations allow your tasks to be retried automatically for transient failures. Refer to the task configuration options on how to leverage this.  </p>"},{"location":"devguide/how-tos/Workflows/handling-errors.html","title":"Handling Errors","text":"<p>When a workflow fails, there are 2 ways to handle the exception.</p>"},{"location":"devguide/how-tos/Workflows/handling-errors.html#set-failureworkflow-in-workflow-definition","title":"Set <code>failureWorkflow</code> in Workflow Definition","text":"<p>In your main workflow definition, you can configure a workflow to run upon failure, by adding the following parameter to the workflow:</p> <pre><code>\"failureWorkflow\": \"&lt;name of your failure workflow\",\n</code></pre> <p>When there is an issue with your workflow, Conductor will start the failure workflow.  By default, three parameters are passed:</p> <ul> <li>reason</li> <li>workflowId: use this to pull the details of the failed workflow.</li> <li>failureStatus</li> </ul>"},{"location":"devguide/how-tos/Workflows/handling-errors.html#example","title":"Example","text":"<p>Here is a sample failure workflow that sends a message to Slack when the workflow fails. It posts the reason and the workflowId into a slack message - to allow for quick debugging:</p> <pre><code>{\n  \"name\": \"shipping_failure\",\n  \"description\": \"workflow for failures with Bobs widget workflow\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"slack_message\",\n      \"taskReferenceName\": \"send_slack_message\",\n      \"inputParameters\": {\n        \"http_request\": {\n          \"headers\": {\n            \"Content-type\": \"application/json\"\n          },\n          \"uri\": \"https://hooks.slack.com/services/&lt;Unique Slack generated Key goes here&gt;\",\n          \"method\": \"POST\",\n          \"body\": {\n            \"text\": \"workflow: ${workflow.input.workflowId} failed. ${workflow.input.reason}\"\n          },\n          \"connectionTimeOut\": 5000,\n          \"readTimeOut\": 5000\n        }\n      },\n      \"type\": \"HTTP\",\n      \"retryCount\": 3\n    }\n  ],\n  \"restartable\": true,\n  \"workflowStatusListenerEnabled\": false,\n  \"ownerEmail\": \"conductor@example.com\",\n  \"timeoutPolicy\": \"ALERT_ONLY\",\n}\n</code></pre>"},{"location":"devguide/how-tos/Workflows/handling-errors.html#set-workfowstatuslistenerenabled","title":"Set <code>workfowStatusListenerEnabled</code>","text":"<p>When this is enabled, notifications are now possible, and by building a custom implementation of the Workflow Status Listener, a notification can be sent to an external service. More details.</p>"},{"location":"devguide/how-tos/Workflows/searching-workflows.html","title":"Searching Workflows","text":"<p>In this article we will learn how to search through workflow executions via the UI.</p>"},{"location":"devguide/how-tos/Workflows/searching-workflows.html#ui-workflows-view","title":"UI Workflows View","text":"<p>Open the home page of the UI installation. It will take you to the <code>Workflow Executions</code> view. This is where we can look at available workflow executions.</p>"},{"location":"devguide/how-tos/Workflows/searching-workflows.html#basic-search","title":"Basic Search","text":"<p>The following fields are available for searching for workflows.</p> Search Field Name Description Workflow Name Use this field to filter workflows by the configured name Workflow ID Use this field to filter to a specific workflow by its id Status Use this field to filter by status - available options are presented as a multi-select option Start Time - From Use this field to filter workflows that started on or after the time specified Start Time - To Use this field to filter workflows that started on or before the time specified Lookback (days) Use this field to filter workflows that ran in the last given number of days Free Text Query If you have indexing enabled, you can query by values that was part of your workflow inputs and outputs <p>The table listing has options to 1. Select columns for display 2. Sort by column value</p> <p>At the bottom of the table, there are options to 1. Select number of rows per page 2. Navigating through pages</p>"},{"location":"devguide/how-tos/Workflows/searching-workflows.html#find-by-tasks","title":"Find by Tasks","text":"<p>In addition to the options listed in Basic Search view, we have the following options in the Find by Tasks view.</p> Search Field Name Description Include Task ID Use this field to filter workflows that contains a task with this id Include Task Name Use this field to filter workflows that contains a task with name Free Text in Tasks If you have indexing enabled, you can query by values that was part of your workflow task inputs and outputs"},{"location":"devguide/how-tos/Workflows/starting-workflows.html","title":"Starting Workflows","text":"<p>Workflow executions can be started by using the following API:</p> <pre><code>POST /api/workflow/{name}\n</code></pre> <p><code>{name}</code> is the placeholder for workflow name. The POST API body is your workflow input parameters which can be empty if there are none.</p>"},{"location":"devguide/how-tos/Workflows/starting-workflows.html#using-client-sdks","title":"Using Client SDKs","text":"<p>Conductor offers client SDKs for popular languages which has library methods that can be used to make this API call.</p> <p>Refer to the SDK documentation to configure a client in your selected language to invoke workflow executions.</p>"},{"location":"devguide/how-tos/Workflows/starting-workflows.html#example-using-curl","title":"Example using curl","text":"<pre><code>curl 'http://localhost:8080/api/workflow/sample_workflow' \\\n  -H 'accept: text/plain' \\\n  -H 'content-type: application/json' \\\n  --data-raw '{\"service\":\"fedex\"}'\n</code></pre> <p>In this example we are specifying one input param called <code>service</code> with a value of <code>fedex</code> and the name of the workflow is <code>sample_workflow</code></p>"},{"location":"devguide/how-tos/Workflows/starting-workflows.html#example-using-node-fetch","title":"Example using node fetch","text":"<pre><code>fetch(\"http://localhost:8080/api/workflow/sample_workflow\", {\n    \"headers\": {\n        \"accept\": \"text/plain\",\n        \"content-type\": \"application/json\",\n    },\n    \"body\": \"{\\\"service\\\":\\\"fedex\\\"}\",\n    \"method\": \"POST\",\n});\n</code></pre>"},{"location":"devguide/how-tos/Workflows/updating-workflows.html","title":"Updating Workflows","text":"<p>Workflows can be created or updated using the workflow metadata API</p> <pre><code>PUT /api/metadata/workflow\n</code></pre>"},{"location":"devguide/how-tos/Workflows/updating-workflows.html#example-using-curl","title":"Example using curl","text":"<pre><code>curl 'http://localhost:8080/api/metadata/workflow' \\\n  -X 'PUT' \\\n  -H 'accept: */*' \\\n  -H 'content-type: application/json' \\\n  --data-raw '[{\"name\":\"sample_workflow\",\"version\":1,\"tasks\":[{\"name\":\"ship_via_fedex\",\"taskReferenceName\":\"ship_via_fedex\",\"type\":\"CUSTOM\"}],\"schemaVersion\":2}]'\n</code></pre>"},{"location":"devguide/how-tos/Workflows/updating-workflows.html#example-using-node-fetch","title":"Example using node fetch","text":"<pre><code>fetch(\"http://localhost:8080/api/metadata/workflow\", {\n  \"headers\": {\n    \"accept\": \"*/*\",\n    \"content-type\": \"application/json\"\n  },\n  \"body\": \"[{\\\"name\\\":\\\"sample_workflow\\\",\\\"version\\\":1,\\\"tasks\\\":[{\\\"name\\\":\\\"ship_via_fedex\\\",\\\"taskReferenceName\\\":\\\"ship_via_fedex\\\",\\\"type\\\":\\\"CUSTOM\\\"}],\\\"schemaVersion\\\":2}]\",\n  \"method\": \"PUT\"\n});\n</code></pre>"},{"location":"devguide/how-tos/Workflows/updating-workflows.html#best-practices","title":"Best Practices","text":"<ol> <li>If you are updating the workflow with new tasks, remember to register the task definitions first</li> <li>You can also use the Conductor Swagger UI to update the workflows </li> </ol>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html","title":"Versioning Workflows","text":"<p>Every workflow has a version number (this number must be an integer.)</p> <p>Versioning allows you to run different versions of the same workflow simultaneously.</p>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html#summary","title":"Summary","text":"<p>Use Case:  A new version of your core workflow will add a capability that is required for veryImportantCustomer.  However, otherVeryImportantCustomer will not be ready to implement this code for another 6 months.</p>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html#version-1","title":"Version 1","text":"<pre><code>{\n  \"name\": \"Core_workflow\",\n  \"description\": \"Very_important_business\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n        &lt;list of tasks&gt;\n    }\n  ],\n  \"outputParameters\": {\n  }\n}\n</code></pre>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html#version-2","title":"Version 2","text":"<pre><code>{\n  \"name\": \"Core_workflow\",\n  \"description\": \"Very_important_business\",\n  \"version\": 2,\n  \"tasks\": [\n    {\n        &lt;a different list of tasks&gt;\n    }\n  ],\n  \"outputParameters\": {\n  }\n}\n</code></pre>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html#version-2-launch","title":"Version 2 launch","text":"<p>Initially, both customers are on version 1 of the workflow.</p> <ul> <li>veryImportantCustomer may begin transitioning traffic onto version 2.  Any tasks that remain unfinished on version 1 stay* on version 1.  </li> <li>otherVeryImportantCustomer remains on version 1.</li> </ul>"},{"location":"devguide/how-tos/Workflows/versioning-workflows.html#6-months-later","title":"6 months later","text":"<ul> <li>All veryImportantCustomer workflows are on version 2.</li> <li>otherVeryImportantCustomer may begin transitioning traffic onto version 2.  Any tasks that remain unfinished on version 1 stay on version 1. </li> </ul>"},{"location":"devguide/how-tos/Workflows/view-workflow-executions.html","title":"View Workflow Executions","text":"<p>In this article we will learn how to view workflow executions via the UI.</p>"},{"location":"devguide/how-tos/Workflows/view-workflow-executions.html#viewing-a-workflow-execution","title":"Viewing a Workflow Execution","text":"<p>Refer to Searching Workflows to filter and find an execution you want to view. Click on the workflow id hyperlink to open the Workflow Execution Details page.</p> <p>The following tabs are available to view the details of the Workflow Execution</p> Tab Name Description Tasks Shows a view with the sub tabs Diagram, Task List and Timeline Tasks &gt; Diagram Visual view of the workflow and its tasks. Tasks &gt; Task List Tabular view of the task executions under this workflow. If there were failures, we will be able to see that here Tasks &gt; Timeline Shows the time each of the tasks took for execution in a timeline view Summary Summary view of the workflow execution Workflow Input/Output Shows the input and output payloads of the workflow. Enable copy mode to copy all or parts of the payload JSON Full JSON payload of the workflow including all tasks, inputs and outputs. Useful for detailed debugging."},{"location":"devguide/how-tos/Workflows/view-workflow-executions.html#viewing-a-workflow-task-detail","title":"Viewing a Workflow Task Detail","text":"<p>From both the Tasks &gt; Diagram and Tasks &gt; Task List views, we can click to see a task execution detail. This opens a flyout panel from the side and contains the following tabs.</p> Tab Name Description Summary Summary info of the task execution Input Task input payload - refer to this tab to see computed inputs passed into the task. Enable copy mode to copy all or parts of the payload Output Shows the output payload produced by the executed task. Enable copy mode to copy all or parts of the payload Log Any log messages logged by the task worked will show up here JSON Complete JSON payload for debugging issues Definition Task definition used when executing this task"},{"location":"devguide/how-tos/Workflows/view-workflow-executions.html#execution-path","title":"Execution Path","text":"<p>An exciting feature of conductor is the ability to see the exact execution path of a workflow. The executed paths are shown in green and is easy to follow like the example below. The alternative paths are greyed out for reference</p> <p></p> <p>Errors will be visible on the UI in ref such as the example below</p> <p></p>"},{"location":"devguide/labs/index.html","title":"Guided Tutorial","text":""},{"location":"devguide/labs/index.html#high-level-steps","title":"High Level Steps","text":"<p>Generally, these are the steps necessary in order to put Conductor to work for your business workflow:</p> <ol> <li>Create task worker(s) that poll for scheduled tasks at regular interval</li> <li>Create task definitions for these workers and register them.</li> <li>Create the workflow definition</li> </ol>"},{"location":"devguide/labs/index.html#before-we-begin","title":"Before We Begin","text":"<p>Ensure you have a Conductor instance up and running. This includes both the Server and the UI. We recommend following the Docker Instructions.</p>"},{"location":"devguide/labs/index.html#tools","title":"Tools","text":"<p>For the purpose of testing and issuing API calls, the following tools are useful</p> <ul> <li>Linux cURL command</li> <li>Postman or similar REST client</li> </ul>"},{"location":"devguide/labs/index.html#lets-go","title":"Let's Go","text":"<p>We will begin by defining a simple workflow that utilizes System Tasks. </p> <p>Next</p>"},{"location":"devguide/labs/beginner.html","title":"Beginner","text":""},{"location":"devguide/labs/beginner.html#hands-on-mode","title":"Hands on mode","text":"<p>Please feel free to follow along using any of these resources:</p> <ul> <li>Using cURL</li> <li>Postman or similar REST client</li> </ul>"},{"location":"devguide/labs/beginner.html#creating-a-workflow","title":"Creating a Workflow","text":"<p>Let's create a simple workflow that adds Netflix Idents to videos. We'll be mocking the adding Idents part and focusing on actually executing this process flow.</p> <p>What are Netflix Idents?</p> <p>Netflix Idents are those 4 second videos with Netflix logo, which appears at the beginning and end of shows.  You might have also noticed they're different for Animation and several other genres.</p> <p>Disclaimer</p> <p>Obviously, this is not how Netflix adds Idents. Those Workflows are indeed very complex. But, it should give you an idea about how Conductor can be used to implement similar features.</p> <p>The workflow in this lab will look like this:</p> <p></p> <p>This workflow contains the following:</p> <ul> <li> <p>Worker Task <code>verify_if_idents_are_added</code> to verify if Idents are already added.</p> </li> <li> <p>Switch Task that takes output from the previous task, and decides whether to schedule the <code>add_idents</code> task.</p> </li> <li> <p><code>add_idents</code> task which is another worker Task.</p> </li> </ul>"},{"location":"devguide/labs/beginner.html#creating-task-definitions","title":"Creating Task definitions","text":"<p>Let's create the task definition for <code>verify_if_idents_are_added</code> in JSON. This task will be a CUSTOM task which is supposed to be executed by an Idents microservice. We'll be mocking the Idents microservice part.</p> <p>Note that at this point, we don't have to specify whether it is a System task or Worker task. We are only specifying the required configurations for the task, like number of times it should be retried, timeouts etc. We shall start by using <code>name</code> parameter for task name. <pre><code>{\n  \"name\": \"verify_if_idents_are_added\"\n}\n</code></pre></p> <p>We'd like this task to be retried 3 times on failure.</p> <pre><code>{\n  \"name\": \"verify_if_idents_are_added\",\n  \"retryCount\": 3,\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 10\n}\n</code></pre> <p>And to timeout after 300 seconds. i.e. if the task doesn't finish execution within this time limit after transitioning to <code>IN_PROGRESS</code> state, the Conductor server cancels this task and schedules a new execution of this task in the queue.</p> <pre><code>{\n  \"name\": \"verify_if_idents_are_added\",\n  \"retryCount\": 3,\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 10,\n  \"timeoutSeconds\": 300,\n  \"timeoutPolicy\": \"TIME_OUT_WF\"\n}\n</code></pre> <p>And a responseTimeout of 180 seconds.</p> <pre><code>{\n  \"name\": \"verify_if_idents_are_added\",\n  \"retryCount\": 3,\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 10,\n  \"timeoutSeconds\": 300,\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"responseTimeoutSeconds\": 180\n}\n</code></pre> <p>We can define several other fields defined here, but this is a good place to start with.</p> <p>Similarly, create another task definition: <code>add_idents</code>.</p> <pre><code>{\n  \"name\": \"add_idents\",\n  \"retryCount\": 3,\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 10,\n  \"timeoutSeconds\": 300,\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"responseTimeoutSeconds\": 180\n}\n</code></pre> <p>Send a <code>POST</code> request to <code>/metadata/taskdefs</code> endpoint to register these tasks. You can use Swagger, Postman, CURL or similar tools.</p> <p>Why is the Switch Task not registered?</p> <p>System Tasks that are part of control flow do not need to be registered. However, some system tasks where the retries, rate limiting and other mechanisms are required, like <code>HTTP</code> Task, are to be registered though.</p> <p>Important</p> <p>Task and Workflow Definition names are unique. The names we use below might have already been registered. For this lab, add a prefix with your username, <code>{my_username}_verify_if_idents_are_added</code> for example. This is definitely not recommended for Production usage though.</p> <p>Example <pre><code>curl -X POST \\\n  http://localhost:8080/api/metadata/taskdefs \\\n  -H 'Content-Type: application/json' \\\n  -d '[\n    {\n      \"name\": \"verify_if_idents_are_added\",\n      \"retryCount\": 3,\n      \"retryLogic\": \"FIXED\",\n      \"retryDelaySeconds\": 10,\n      \"timeoutSeconds\": 300,\n      \"timeoutPolicy\": \"TIME_OUT_WF\",\n      \"responseTimeoutSeconds\": 180,\n      \"ownerEmail\": \"type your email here\"\n    },\n    {\n      \"name\": \"add_idents\",\n      \"retryCount\": 3,\n      \"retryLogic\": \"FIXED\",\n      \"retryDelaySeconds\": 10,\n      \"timeoutSeconds\": 300,\n      \"timeoutPolicy\": \"TIME_OUT_WF\",\n      \"responseTimeoutSeconds\": 180,\n      \"ownerEmail\": \"type your email here\"\n    }\n]'\n</code></pre></p>"},{"location":"devguide/labs/beginner.html#creating-workflow-definition","title":"Creating Workflow Definition","text":"<p>Creating Workflow definition is almost similar. We shall use the Task definitions created above. Note that same Task definitions can be used in multiple workflows, or for multiple times in same Workflow (that's where <code>taskReferenceName</code> is useful).</p> <p>A workflow without any tasks looks like this:</p> <pre><code>{\n    \"name\": \"add_netflix_identation\",\n    \"description\": \"Adds Netflix Identation to video files.\",\n    \"version\": 1,\n    \"schemaVersion\": 2,\n    \"tasks\": []\n}\n</code></pre> <p>Add the first task that this workflow has to execute. All the tasks must be added to the <code>tasks</code> array.</p> <pre><code>{\n    \"name\": \"add_netflix_identation\",\n    \"description\": \"Adds Netflix Identation to video files.\",\n    \"version\": 1,\n    \"schemaVersion\": 2,\n    \"tasks\": [\n        {\n            \"name\": \"verify_if_idents_are_added\",\n            \"taskReferenceName\": \"ident_verification\",\n            \"inputParameters\": {\n                \"contentId\": \"${workflow.input.contentId}\"\n            },\n            \"type\": \"CUSTOM\"\n        }\n    ]\n}\n</code></pre> <p>Wiring Input/Outputs</p> <p>Notice how we were using <code>${workflow.input.contentId}</code> to pass inputs to this task. Conductor can wire inputs between workflow and tasks, and between tasks. i.e The task <code>verify_if_idents_are_added</code> is wired to accept inputs from the workflow input using JSONPath expression <code>${workflow.input.param}</code>.</p> <p>Learn more about wiring inputs and outputs here.</p> <p>Let's define <code>decisionCases</code> now. </p> <p>Note: in earlier versions of this tutorial, the \"decision\" task was used. This has been deprecated.</p> <p>Checkout the Switch task structure here.</p> <p>A Switch task is specified by the <code>evaulatorType</code>, <code>expression</code> (the expression that defines the Switch) and <code>decisionCases</code> which lists all the branches of Switch task.  </p> <p>In this case, we'll use <code>\"evaluatorType\": \"value-param\"</code>, meaning that we'll just use the value inputted to make the decision.  Alternatively, there is a <code>\"evaluatorType\": \"JavaScript\"</code> that can be used for more complicated evaluations.</p> <p>Adding the switch task (without any decision cases): <pre><code>{\n    \"name\": \"add_netflix_identation\",\n    \"description\": \"Adds Netflix Identation to video files.\",\n    \"version\": 2,\n    \"schemaVersion\": 2,\n    \"tasks\": [\n        {\n            \"name\": \"verify_if_idents_are_added\",\n            \"taskReferenceName\": \"ident_verification\",\n            \"inputParameters\": {\n                \"contentId\": \"${workflow.input.contentId}\"\n            },\n            \"type\": \"CUSTOM\"\n        },\n        {\n            \"name\": \"switch_task\",\n            \"taskReferenceName\": \"is_idents_added\",\n            \"inputParameters\": {\n                \"case_value_param\": \"${ident_verification.output.is_idents_added}\"\n            },\n            \"type\": \"SWITCH\",\n            \"evaluatorType\": \"value-param\",\n            \"expression\": \"case_value_param\",\n            \"decisionCases\": {\n\n            }\n        }\n    ]\n}\n</code></pre></p> <p>Each switch task can have multiple tasks, so it has to be defined as an array.</p> <pre><code>{\n    \"name\": \"add_netflix_identation\",\n    \"description\": \"Adds Netflix Identation to video files.\",\n    \"version\": 2,\n    \"schemaVersion\": 2,\n    \"tasks\": [\n        {\n            \"name\": \"verify_if_idents_are_added\",\n            \"taskReferenceName\": \"ident_verification\",\n            \"inputParameters\": {\n                \"contentId\": \"${workflow.input.contentId}\"\n            },\n            \"type\": \"CUSTOM\"\n        },\n        {\n            \"name\": \"switch_task\",\n            \"taskReferenceName\": \"is_idents_added\",\n            \"inputParameters\": {\n                \"case_value_param\": \"${ident_verification.output.is_idents_added}\"\n            },\n            \"type\": \"SWITCH\",\n            \"evaluatorType\": \"value-param\",\n            \"expression\": \"case_value_param\",\n            \"decisionCases\": {\n                \"false\": [\n                    {\n                        \"name\": \"add_idents\",\n                        \"taskReferenceName\": \"add_idents_by_type\",\n                        \"inputParameters\": {\n                            \"identType\": \"${workflow.input.identType}\",\n                            \"contentId\": \"${workflow.input.contentId}\"\n                        },\n                        \"type\": \"CUSTOM\"\n                    }\n                ]\n            }\n        }\n    ]\n}\n</code></pre> <p>Just like the task definitions, register this workflow definition by sending a POST request to <code>/workflow</code> endpoint.</p> <p>Example <pre><code>curl -X POST \\\n  http://localhost:8080/api/metadata/workflow \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"name\": \"add_netflix_identation\",\n    \"description\": \"Adds Netflix Identation to video files.\",\n    \"version\": 2,\n    \"schemaVersion\": 2,\n    \"tasks\": [\n        {\n            \"name\": \"verify_if_idents_are_added\",\n            \"taskReferenceName\": \"ident_verification\",\n            \"inputParameters\": {\n                \"contentId\": \"${workflow.input.contentId}\"\n            },\n            \"type\": \"CUSTOM\"\n        },\n        {\n            \"name\": \"switch_task\",\n            \"taskReferenceName\": \"is_idents_added\",\n            \"inputParameters\": {\n                \"case_value_param\": \"${ident_verification.output.is_idents_added}\"\n            },\n            \"type\": \"SWITCH\",\n            \"evaluatorType\": \"value-param\",\n            \"expression\": \"case_value_param\",\n            \"decisionCases\": {\n                \"false\": [\n                    {\n                        \"name\": \"add_idents\",\n                        \"taskReferenceName\": \"add_idents_by_type\",\n                        \"inputParameters\": {\n                            \"identType\": \"${workflow.input.identType}\",\n                            \"contentId\": \"${workflow.input.contentId}\"\n                        },\n                        \"type\": \"CUSTOM\"\n                    }\n                ]\n            }\n        }\n    ]\n}'\n</code></pre></p>"},{"location":"devguide/labs/beginner.html#starting-the-workflow","title":"Starting the Workflow","text":"<p>Send a <code>POST</code> request to <code>/workflow</code> with: <pre><code>{\n    \"name\": \"add_netflix_identation\",\n    \"version\": 2,\n    \"correlationId\": \"my_netflix_identation_workflows\",\n    \"input\": {\n        \"identType\": \"animation\",\n        \"contentId\": \"my_unique_content_id\"\n    }\n}\n</code></pre></p> <p>Example:</p> <pre><code>curl -X POST \\\n  http://localhost:8080/api/workflow/add_netflix_identation \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"identType\": \"animation\",\n    \"contentId\": \"my_unique_content_id\"\n}'\n</code></pre> <p>Successful POST request should return a workflow Id, which you can use to find the execution in the UI.</p>"},{"location":"devguide/labs/beginner.html#conductor-user-interface","title":"Conductor User Interface","text":"<p>Open the UI and navigate to the RUNNING tab, the Workflow should be in the state as below:</p> <p></p> <p>Feel free to explore the various functionalities that the UI exposes. To elaborate on a few:</p> <ul> <li>Workflow Task modals (Opens on clicking any of the tasks in the workflow), which includes task I/O, logs and task JSON.</li> <li>Task Details tab, which shows the sequence of task execution, status, start/end time, and link to worker details which executed the task.</li> <li>Input/Output tab shows workflow input and output.</li> </ul>"},{"location":"devguide/labs/beginner.html#poll-for-worker-task","title":"Poll for Worker task","text":"<p>Now that <code>verify_if_idents_are_added</code> task is in <code>SCHEDULED</code> state, it is the worker's turn to fetch the task, execute it and update Conductor with final status of the task.</p> <p>Ideally, the workers implementing the Client interface would do this process, executing the tasks on real microservices. But, let's mock this part.</p> <p>Send a <code>GET</code> request to <code>/poll</code> endpoint with your task type.</p> <p>For example:</p> <pre><code>curl -X GET \\\n    http://localhost:8080/api/tasks/poll/verify_if_idents_are_added\n</code></pre>"},{"location":"devguide/labs/beginner.html#return-response-add-logs","title":"Return response, add logs","text":"<p>We can respond to Conductor with any of the following states:</p> <ul> <li>Task has COMPLETED.</li> <li>Task has FAILED.</li> <li>Call back after seconds [Process the task at a later time].</li> </ul> <p>Considering our Ident Service has verified that the Ident's are not yet added to given Content Id, let's return the task status by sending the below <code>POST</code> request to <code>/tasks</code> endpoint, with payload:</p> <pre><code>{\n  \"workflowInstanceId\": \"{workflowId}\",\n  \"taskId\": \"{taskId}\",\n  \"reasonForIncompletion\": \"\",\n  \"callbackAfterSeconds\": 0,\n  \"workerId\": \"localhost\",\n  \"status\": \"COMPLETED\",\n  \"outputData\": {\n    \"is_idents_added\": false\n  }\n}\n</code></pre> <p>Example:</p> <pre><code>curl -X POST \\\n  http://localhost:8080/api/tasks \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"workflowInstanceId\": \"cb7c5041-aa85-4940-acb4-3bdcfa9f5c5c\",\n    \"taskId\": \"741f362b-ee9a-47b6-81b5-9bbbd5c4c992\",\n    \"reasonForIncompletion\": \"\",\n    \"callbackAfterSeconds\": 0,\n    \"workerId\": \"string\",\n    \"status\": \"COMPLETED\",\n    \"outputData\": {\n        \"is_idents_added\": false\n    },\n    \"logs\": [\n        {\n            \"log\": \"Ident verification successful for title: {some_title_name}, with Id: {some_id}\",\n            \"createdTime\": 1550178825\n        }   \n    ]\n  }'\n</code></pre> <p>Check logs in UI</p> <p>You can find the logs we just sent by clicking the <code>verify_if_idents_are_added</code>, upon which a modal should open with <code>Logs</code> tab.</p>"},{"location":"devguide/labs/beginner.html#why-is-system-task-executed-but-worker-task-is-scheduled","title":"Why is System task executed, but Worker task is Scheduled.","text":"<p>You will notice that Workflow is in the state as below after sending the POST request:</p> <p></p> <p>Conductor has executed <code>is_idents_added</code> all through it's lifecycle, without us polling, or returning the status of Task. If it is still unclear, <code>is_idents_added</code> is a System task, and System tasks are executed by Conductor Server.</p> <p>But, <code>add_idents</code> is a CUSTOM task. So, the complete lifecycle of this task (Poll, Update) should be handled by a worker to continue with W\\workflow execution. When Conductor has finished executing all the tasks in given flow, the workflow will reach Terminal state (COMPLETED, FAILED, TIMED_OUT etc.)</p>"},{"location":"devguide/labs/beginner.html#next-steps","title":"Next steps","text":"<p>You can play around this workflow by failing one of the Tasks, restarting or retrying the Workflow, or by tuning the number of retries, timeoutSeconds etc.</p>"},{"location":"devguide/labs/eventhandlers.html","title":"Events and Event Handlers","text":"<p>In this exercise, we shall:</p> <ul> <li>Publish an Event to Conductor using <code>Event</code> task.</li> <li>Subscribe to Events, and perform actions:<ul> <li>Start a Workflow</li> <li>Complete Task</li> </ul> </li> </ul> <p>Conductor supports eventing with two Interfaces:</p> <ul> <li>Event Task</li> <li>Event Handlers</li> </ul> <p>We will create a cyclic workflow similar to this:</p> <p></p>"},{"location":"devguide/labs/eventhandlers.html#create-workflow-definitions","title":"Create Workflow Definitions","text":"<p>Let's create two workflows:</p> <ul> <li><code>test_workflow_for_eventHandler</code> which will have an <code>Event</code> task to start another workflow, and a <code>WAIT</code> System task that will be completed by an event.</li> <li><code>test_workflow_startedBy_eventHandler</code> which will have an <code>Event</code> task to generate an event to complete <code>WAIT</code> task in the above workflow.</li> </ul> <p>Send <code>POST</code> requests to <code>/metadata/workflow</code> endpoint with below payloads:</p> <pre><code>{\n  \"name\": \"test_workflow_for_eventHandler\",\n  \"description\": \"A test workflow to start another workflow with EventHandler\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"test_start_workflow_event\",\n      \"taskReferenceName\": \"start_workflow_with_event\",\n      \"type\": \"EVENT\",\n      \"sink\": \"conductor\"\n    },\n    {\n      \"name\": \"test_task_tobe_completed_by_eventHandler\",\n      \"taskReferenceName\": \"test_task_tobe_completed_by_eventHandler\",\n      \"type\": \"WAIT\"\n    }\n  ]\n}\n</code></pre> <pre><code>{\n  \"name\": \"test_workflow_startedBy_eventHandler\",\n  \"description\": \"A test workflow which is started by EventHandler, and then goes on to complete task in another workflow.\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"test_complete_task_event\",\n      \"taskReferenceName\": \"complete_task_with_event\",\n      \"inputParameters\": {\n        \"sourceWorkflowId\": \"${workflow.input.sourceWorkflowId}\"\n      },\n      \"type\": \"EVENT\",\n      \"sink\": \"conductor\"\n    }\n  ]\n}\n</code></pre>"},{"location":"devguide/labs/eventhandlers.html#event-tasks-in-workflow","title":"Event Tasks in Workflow","text":"<p><code>EVENT</code> task is a System task, and we shall define it just like other Tasks in Workflow, with <code>sink</code> parameter. Also, <code>EVENT</code> task doesn't have to be registered before using in Workflow. This is also true for the <code>WAIT</code> task. Hence, we will not be registering any tasks for these workflows.</p>"},{"location":"devguide/labs/eventhandlers.html#events-are-sent-but-theyre-not-handled-yet","title":"Events are sent, but they're not handled (yet)","text":"<p>Once you try to start <code>test_workflow_for_eventHandler</code> workflow, you would notice that the event is sent successfully, but the second worflow <code>test_workflow_startedBy_eventHandler</code> is not started. We have sent the Events, but we also need to define <code>Event Handlers</code> for Conductor to take any <code>actions</code> based on the Event. Let's create <code>Event Handlers</code>.</p>"},{"location":"devguide/labs/eventhandlers.html#create-event-handlers","title":"Create Event Handlers","text":"<p>Event Handler definitions are pretty much like Task or Workflow definitions. We start by name:</p> <pre><code>{\n  \"name\": \"test_start_workflow\"\n}\n</code></pre> <p>Event Handler should know the Queue it has to listen to. This should be defined in <code>event</code> parameter.</p> <p>When using Conductor queues, define <code>event</code> with format: </p> <p><code>conductor:{workflow_name}:{taskReferenceName}</code></p> <p>And when using SQS, define with format: </p> <p><code>sqs:{my_sqs_queue_name}</code></p> <pre><code>{\n  \"name\": \"test_start_workflow\",\n  \"event\": \"conductor:test_workflow_for_eventHandler:start_workflow_with_event\"\n}\n</code></pre> <p>Event Handler can perform a list of actions defined in <code>actions</code> array parameter, for this particular <code>event</code> queue.</p> <pre><code>{\n  \"name\": \"test_start_workflow\",\n  \"event\": \"conductor:test_workflow_for_eventHandler:start_workflow_with_event\",\n  \"actions\": [\n      \"&lt;insert-actions-here&gt;\"\n  ],\n  \"active\": true\n}\n</code></pre> <p>Let's define <code>start_workflow</code> action. We shall pass the name of workflow we would like to start. The <code>start_workflow</code> parameter can use any of the values from the general Start Workflow Request. Here we are passing in the workflowId, so that the Complete Task Event Handler can use it.</p> <pre><code>{\n    \"action\": \"start_workflow\",\n    \"start_workflow\": {\n        \"name\": \"test_workflow_startedBy_eventHandler\",\n        \"input\": {\n            \"sourceWorkflowId\": \"${workflowInstanceId}\"\n        }\n    }\n}\n</code></pre> <p>Send a <code>POST</code> request to <code>/event</code> endpoint:</p> <pre><code>{\n  \"name\": \"test_start_workflow\",\n  \"event\": \"conductor:test_workflow_for_eventHandler:start_workflow_with_event\",\n  \"actions\": [\n    {\n      \"action\": \"start_workflow\",\n      \"start_workflow\": {\n        \"name\": \"test_workflow_startedBy_eventHandler\",\n        \"input\": {\n          \"sourceWorkflowId\": \"${workflowInstanceId}\"\n        }\n      }\n    }\n  ],\n  \"active\": true\n}\n</code></pre> <p>Similarly, create another Event Handler to complete task.</p> <pre><code>{\n  \"name\": \"test_complete_task_event\",\n  \"event\": \"conductor:test_workflow_startedBy_eventHandler:complete_task_with_event\",\n  \"actions\": [\n    {\n        \"action\": \"complete_task\",\n        \"complete_task\": {\n            \"workflowId\": \"${sourceWorkflowId}\",\n            \"taskRefName\": \"test_task_tobe_completed_by_eventHandler\"\n         }\n    }\n  ],\n  \"active\": true\n}\n</code></pre>"},{"location":"devguide/labs/eventhandlers.html#summary","title":"Summary","text":"<p>After wiring all of the above, starting the <code>test_workflow_for_eventHandler</code> should:</p> <ol> <li>Start <code>test_workflow_startedBy_eventHandler</code> workflow.</li> <li>Sets <code>test_task_tobe_completed_by_eventHandler</code> WAIT task <code>IN_PROGRESS</code>.</li> <li><code>test_workflow_startedBy_eventHandler</code> event task would publish an Event to complete the WAIT task above.</li> <li>Both the workflows would move to <code>COMPLETED</code> state.</li> </ol>"},{"location":"devguide/labs/first-workflow.html","title":"A First Workflow","text":"<p>In this article we will explore how we can run a really simple workflow that runs without deploying any new microservice. </p> <p>Conductor can orchestrate HTTP services out of the box without implementing any code.  We will use that to create and run the first workflow.</p> <p>See System Task for the list of such built-in tasks. Using system tasks is a great way to run a lot of our code in production.</p> <p>To bring up a local instance of Conductor follow one of the recommended steps:</p> <ol> <li>Running Locally - From Code</li> <li>Running Locally - Docker Compose</li> </ol>"},{"location":"devguide/labs/first-workflow.html#configuring-our-first-workflow","title":"Configuring our First Workflow","text":"<p>This is a sample workflow that we can leverage for our test.</p> <pre><code>{\n  \"name\": \"first_sample_workflow\",\n  \"description\": \"First Sample Workflow\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"get_population_data\",\n      \"taskReferenceName\": \"get_population_data\",\n      \"inputParameters\": {\n        \"http_request\": {\n          \"uri\": \"https://datausa.io/api/data?drilldowns=Nation&amp;measures=Population\",\n          \"method\": \"GET\"\n        }\n      },\n      \"type\": \"HTTP\"\n    }\n  ],\n  \"inputParameters\": [],\n  \"outputParameters\": {\n    \"data\": \"${get_population_data.output.response.body.data}\",\n    \"source\": \"${get_population_data.output.response.body.source}\"\n  },\n  \"schemaVersion\": 2,\n  \"restartable\": true,\n  \"workflowStatusListenerEnabled\": false,\n  \"ownerEmail\": \"example@email.com\",\n  \"timeoutPolicy\": \"ALERT_ONLY\",\n  \"timeoutSeconds\": 0\n}\n</code></pre> <p>This is an example workflow that queries a publicly available JSON API to retrieve some data. This workflow doesn\u2019t require any worker implementation as the tasks in this workflow are managed by the system itself. This is an awesome feature of Conductor. For a lot of typical work, we won\u2019t have to write any code at all.</p> <p>Let's talk about this workflow a little more so that we can gain some context.</p> <pre><code>\"name\" : \"first_sample_workflow\"\n</code></pre> <p>This line here is how we name our workflow. In this case our workflow name is <code>first_sample_workflow</code></p> <p>This workflow contains just one worker. The workers are defined under the key <code>tasks</code>. Here is the worker definition with the most important values:</p> <pre><code>{\n  \"name\": \"get_population_data\",\n  \"taskReferenceName\": \"get_population_data\",\n  \"inputParameters\": {\n    \"http_request\": {\n      \"uri\": \"https://datausa.io/api/data?drilldowns=Nation&amp;measures=Population\",\n      \"method\": \"GET\"\n    }\n  },\n  \"type\": \"HTTP\"\n}\n</code></pre> <p>Here is a list of fields and what it does:</p> <ol> <li><code>\"name\"</code> : Name of our worker</li> <li><code>\"taskReferenceName\"</code> : This is a reference to this worker in this specific workflow implementation. We can have multiple    workers of the same name in our workflow, but we will need a unique task reference name for each of them. Task    reference name should be unique across our entire workflow.</li> <li><code>\"inputParameters\"</code> : These are the inputs into our worker. We can hard code inputs as we have done here. We can    also provide dynamic inputs such as from the workflow input or based on the output of another worker. We can find    examples of this in our documentation.</li> <li><code>\"type\"</code> : This is what defines what the type of worker is. In our example - this is <code>HTTP</code>. There are more task    types which we can find in the Conductor documentation.</li> <li><code>\"http_request\"</code> : This is an input that is required for tasks of type <code>HTTP</code>. In our example we have provided a well    known internet JSON API url and the type of HTTP method to invoke - <code>GET</code></li> </ol> <p>We haven't talked about the other fields that we can use in our definitions as these are either just metadata or more advanced concepts which we can learn more in the detailed documentation.</p> <p>Ok, now that we have walked through our workflow details, let's run this and see how it works.</p> <p>To configure the workflow, head over to the swagger API of conductor server and access the metadata workflow create API:</p> <p>http://http://localhost:8080/swagger-ui/index.html?configUrl=/api-docs/swagger-config#/metadata-resource/create</p> <p>If the link doesn\u2019t open the right Swagger section, we can navigate to Metadata-Resource \u2192 <code>POST /api/metadata/workflow</code></p> <p></p> <p>Paste the workflow payload into the Swagger API and hit Execute.</p> <p>Now if we head over to the UI, we can see this workflow definition created:</p> <p></p> <p>If we click through we can see a visual representation of the workflow:</p> <p></p>"},{"location":"devguide/labs/first-workflow.html#running-our-first-workflow","title":"Running our First Workflow","text":"<p>Let\u2019s run this workflow. To do that we can use the swagger API under the workflow-resources</p> <p>http://http://localhost:8080/swagger-ui/index.html?configUrl=/api-docs/swagger-config#/workflow-resource/startWorkflow_1</p> <p></p> <p>Hit Execute!</p> <p>Conductor will return a workflow id. We will need to use this id to load this up on the UI. If our UI installation has search enabled we wouldn't need to copy this. If we don't have search enabled (using Elasticsearch) copy it from the Swagger UI.</p> <p></p> <p>Ok, we should see this running and get completed soon. Let\u2019s go to the UI to see what happened.</p> <p>To load the workflow directly, use this URL format:</p> <pre><code>http://localhost:5000/execution/&lt;WORKFLOW_ID&gt;\n</code></pre> <p>Replace <code>&lt;WORKFLOW_ID&gt;</code> with our workflow id from the previous step. We should see a screen like below. Click on the different tabs to see all inputs and outputs and task list etc. Explore away!</p> <p></p>"},{"location":"devguide/labs/first-workflow.html#summary","title":"Summary","text":"<p>In this article \u2014 we learned how to run a sample workflow in our Conductor installation. Concepts we touched on:</p> <ol> <li>Workflow creation</li> <li>System tasks such as HTTP</li> <li>Running a workflow via API</li> </ol>"},{"location":"devguide/labs/kitchensink.html","title":"Kitchen Sink","text":"<p>An example kitchensink workflow that demonstrates the usage of all the schema constructs.</p>"},{"location":"devguide/labs/kitchensink.html#definition","title":"Definition","text":"<pre><code>{\n  \"name\": \"kitchensink\",\n  \"description\": \"kitchensink workflow\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"task_1\",\n      \"taskReferenceName\": \"task_1\",\n      \"inputParameters\": {\n        \"mod\": \"${workflow.input.mod}\",\n        \"oddEven\": \"${workflow.input.oddEven}\"\n      },\n      \"type\": \"CUSTOM\"\n    },\n    {\n      \"name\": \"event_task\",\n      \"taskReferenceName\": \"event_0\",\n      \"inputParameters\": {\n        \"mod\": \"${workflow.input.mod}\",\n        \"oddEven\": \"${workflow.input.oddEven}\"\n      },\n      \"type\": \"EVENT\",\n      \"sink\": \"conductor\"\n    },\n    {\n      \"name\": \"dyntask\",\n      \"taskReferenceName\": \"task_2\",\n      \"inputParameters\": {\n        \"taskToExecute\": \"${workflow.input.task2Name}\"\n      },\n      \"type\": \"DYNAMIC\",\n      \"dynamicTaskNameParam\": \"taskToExecute\"\n    },\n    {\n      \"name\": \"oddEvenDecision\",\n      \"taskReferenceName\": \"oddEvenDecision\",\n      \"inputParameters\": {\n        \"oddEven\": \"${task_2.output.oddEven}\"\n      },\n      \"type\": \"DECISION\",\n      \"caseValueParam\": \"oddEven\",\n      \"decisionCases\": {\n        \"0\": [\n          {\n            \"name\": \"task_4\",\n            \"taskReferenceName\": \"task_4\",\n            \"inputParameters\": {\n              \"mod\": \"${task_2.output.mod}\",\n              \"oddEven\": \"${task_2.output.oddEven}\"\n            },\n            \"type\": \"CUSTOM\"\n          },\n          {\n            \"name\": \"dynamic_fanout\",\n            \"taskReferenceName\": \"fanout1\",\n            \"inputParameters\": {\n              \"dynamicTasks\": \"${task_4.output.dynamicTasks}\",\n              \"input\": \"${task_4.output.inputs}\"\n            },\n            \"type\": \"FORK_JOIN_DYNAMIC\",\n            \"dynamicForkTasksParam\": \"dynamicTasks\",\n            \"dynamicForkTasksInputParamName\": \"input\"\n          },\n          {\n            \"name\": \"dynamic_join\",\n            \"taskReferenceName\": \"join1\",\n            \"type\": \"JOIN\"\n          }\n        ],\n        \"1\": [\n          {\n            \"name\": \"fork_join\",\n            \"taskReferenceName\": \"forkx\",\n            \"type\": \"FORK_JOIN\",\n            \"forkTasks\": [\n              [\n                {\n                  \"name\": \"task_10\",\n                  \"taskReferenceName\": \"task_10\",\n                  \"type\": \"CUSTOM\"\n                },\n                {\n                  \"name\": \"sub_workflow_x\",\n                  \"taskReferenceName\": \"wf3\",\n                  \"inputParameters\": {\n                    \"mod\": \"${task_1.output.mod}\",\n                    \"oddEven\": \"${task_1.output.oddEven}\"\n                  },\n                  \"type\": \"SUB_WORKFLOW\",\n                  \"subWorkflowParam\": {\n                    \"name\": \"sub_flow_1\",\n                    \"version\": 1\n                  }\n                }\n              ],\n              [\n                {\n                  \"name\": \"task_11\",\n                  \"taskReferenceName\": \"task_11\",\n                  \"type\": \"CUSTOM\"\n                },\n                {\n                  \"name\": \"sub_workflow_x\",\n                  \"taskReferenceName\": \"wf4\",\n                  \"inputParameters\": {\n                    \"mod\": \"${task_1.output.mod}\",\n                    \"oddEven\": \"${task_1.output.oddEven}\"\n                  },\n                  \"type\": \"SUB_WORKFLOW\",\n                  \"subWorkflowParam\": {\n                    \"name\": \"sub_flow_1\",\n                    \"version\": 1\n                  }\n                }\n              ]\n            ]\n          },\n          {\n            \"name\": \"join\",\n            \"taskReferenceName\": \"join2\",\n            \"type\": \"JOIN\",\n            \"joinOn\": [\n              \"wf3\",\n              \"wf4\"\n            ]\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"search_elasticsearch\",\n      \"taskReferenceName\": \"get_es_1\",\n      \"inputParameters\": {\n        \"http_request\": {\n          \"uri\": \"http://localhost:9200/conductor/_search?size=10\",\n          \"method\": \"GET\"\n        }\n      },\n      \"type\": \"HTTP\"\n    },\n    {\n      \"name\": \"task_30\",\n      \"taskReferenceName\": \"task_30\",\n      \"inputParameters\": {\n        \"statuses\": \"${get_es_1.output..status}\",\n        \"workflowIds\": \"${get_es_1.output..workflowId}\"\n      },\n      \"type\": \"CUSTOM\"\n    }\n  ],\n  \"outputParameters\": {\n    \"statues\": \"${get_es_1.output..status}\",\n    \"workflowIds\": \"${get_es_1.output..workflowId}\"\n  },\n  \"ownerEmail\": \"example@email.com\",\n  \"schemaVersion\": 2\n}\n</code></pre>"},{"location":"devguide/labs/kitchensink.html#visual-flow","title":"Visual Flow","text":""},{"location":"devguide/labs/kitchensink.html#running-kitchensink-workflow","title":"Running Kitchensink Workflow","text":"<ol> <li>If you are running Conductor locally, use the <code>-DloadSample=true</code> Java system property when launching the server.  This will create a kitchensink workflow, related task definitions and kick off an instance of kitchensink workflow. Otherwise, you can create a new Workflow Definition in the UI by copying the sample above.</li> <li>Once the workflow has started, the first task remains in the <code>SCHEDULED</code> state.  This is because no workers are currently polling for the task.</li> <li>We will use the REST endpoints directly to poll for tasks and updating the status.</li> </ol>"},{"location":"devguide/labs/kitchensink.html#start-workflow-execution","title":"Start workflow execution","text":"<p>Start the execution of the kitchensink workflow by posting the following:</p> <p><pre><code>curl -X POST --header 'Content-Type: application/json' --header 'Accept: text/plain' 'http://localhost:8080/api/workflow/kitchensink' -d '\n{\n    \"task2Name\": \"task_5\" \n}\n'\n</code></pre> The response is a text string identifying the workflow instance id.</p>"},{"location":"devguide/labs/kitchensink.html#poll-for-the-first-task","title":"Poll for the first task:","text":"<pre><code>curl http://localhost:8080/api/tasks/poll/task_1\n</code></pre> <p>The response should look something like:</p> <pre><code>{\n    \"taskType\": \"task_1\",\n    \"status\": \"IN_PROGRESS\",\n    \"inputData\": {\n        \"mod\": null,\n        \"oddEven\": null\n    },\n    \"referenceTaskName\": \"task_1\",\n    \"retryCount\": 0,\n    \"seq\": 1,\n    \"pollCount\": 1,\n    \"taskDefName\": \"task_1\",\n    \"scheduledTime\": 1486580932471,\n    \"startTime\": 1486580933869,\n    \"endTime\": 0,\n    \"updateTime\": 1486580933902,\n    \"startDelayInSeconds\": 0,\n    \"retried\": false,\n    \"callbackFromWorker\": true,\n    \"responseTimeoutSeconds\": 3600,\n    \"workflowInstanceId\": \"b0d1a935-3d74-46fd-92b2-0ca1e388659f\",\n    \"taskId\": \"b9eea7dd-3fbd-46b9-a9ff-b00279459476\",\n    \"callbackAfterSeconds\": 0,\n    \"polledTime\": 1486580933902,\n    \"queueWaitTime\": 1398\n}\n</code></pre>"},{"location":"devguide/labs/kitchensink.html#update-the-task-status","title":"Update the task status","text":"<ul> <li>Note the values for <code>taskId</code> and <code>workflowInstanceId</code> fields from the poll response</li> <li>Update the status of the task as <code>COMPLETED</code> as below:</li> </ul> <pre><code>curl -H 'Content-Type:application/json' -H 'Accept:application/json' -X POST http://localhost:8080/api/tasks/ -d '\n{\n    \"taskId\": \"b9eea7dd-3fbd-46b9-a9ff-b00279459476\",\n    \"workflowInstanceId\": \"b0d1a935-3d74-46fd-92b2-0ca1e388659f\",\n    \"status\": \"COMPLETED\",\n    \"outputData\": {\n        \"mod\": 5,\n        \"taskToExecute\": \"task_1\",\n        \"oddEven\": 0,\n        \"dynamicTasks\": [\n            {\n                \"name\": \"task_1\",\n                \"taskReferenceName\": \"task_1_1\",\n                \"type\": \"CUSTOM\"\n            },\n            {\n                \"name\": \"sub_workflow_4\",\n                \"taskReferenceName\": \"wf_dyn\",\n                \"type\": \"SUB_WORKFLOW\",\n                \"subWorkflowParam\": {\n                    \"name\": \"sub_flow_1\"\n                }\n            }\n        ],\n        \"inputs\": {\n            \"task_1_1\": {},\n            \"wf_dyn\": {}\n        }\n    }\n}'\n</code></pre> <p>This will mark the task_1 as completed and schedule <code>task_5</code> as the next task. Repeat the same process for the subsequently scheduled tasks until the completion.</p>"},{"location":"documentation/advanced/annotation-processor.html","title":"Annotation Processor","text":"<p>This module is strictly for code generation tasks during builds based on annotations. Currently supports <code>protogen</code></p>"},{"location":"documentation/advanced/annotation-processor.html#usage","title":"Usage","text":"<p>This is an actual example of this module which is implemented in common/build.gradle</p> <pre><code>task protogen(dependsOn: jar, type: JavaExec) {\n    classpath configurations.annotationsProcessorCodegen\n    main = 'com.swiftconductor.conductor.annotationsprocessor.protogen.ProtoGenTask'\n    args(\n            \"conductor.proto\",\n            \"com.swiftconductor.conductor.proto\",\n            \"github.com/swift-conductor/conductor/client/gogrpc/conductor/model\",\n            \"${rootDir}/grpc/src/main/proto\",\n            \"${rootDir}/grpc/src/main/java/com/swiftconductor/conductor/grpc\",\n            \"com.swiftconductor.conductor.grpc\",\n            jar.archivePath,\n            \"com.swiftconductor.conductor.common\",\n    )\n}\n</code></pre>"},{"location":"documentation/advanced/archival-of-workflows.html","title":"Archiving Workflows","text":"<p>Conductor has support for archiving workflow upon termination or completion. Enabling this will delete the workflow from the configured database, but leave the associated data in Elasticsearch so it is still searchable. </p> <p>To enable, set the <code>conductor.workflow-status-listener.type</code> property to <code>archive</code>.</p> <p>A number of additional properties are available to control archival.</p> Property Default Value Description conductor.workflow-status-listener.archival.ttlDuration 0s The time to live in seconds for workflow archiving module. Currently, only RedisExecutionDAO supports this conductor.workflow-status-listener.archival.delayQueueWorkerThreadCount 5 The number of threads to process the delay queue in workflow archival conductor.workflow-status-listener.archival.delaySeconds 60 The time to delay the archival of workflow"},{"location":"documentation/advanced/azureblob-storage.html","title":"Azure Blob Storage","text":"<p>The AzureBlob storage module uses azure blob to store and retrieve workflows/tasks input/output payload that went over the thresholds defined in properties named <code>conductor.[workflow|task].[input|output].payload.threshold.kb</code>.</p> <p>Warning Azure Java SDK use libs already present inside <code>conductor</code> like <code>jackson</code> and <code>netty</code>. You may encounter deprecated issues, or conflicts and need to adapt the code if the module is not maintained along with <code>conductor</code>. It has only been tested with v12.2.0.</p>"},{"location":"documentation/advanced/azureblob-storage.html#configuration","title":"Configuration","text":""},{"location":"documentation/advanced/azureblob-storage.html#usage","title":"Usage","text":"<p>Cf. Documentation External Payload Storage</p>"},{"location":"documentation/advanced/azureblob-storage.html#example","title":"Example","text":"<pre><code>conductor.additional.modules=com.swiftconductor.conductor.azureblob.AzureBlobModule\nes.set.netty.runtime.available.processors=false\n\nworkflow.external.payload.storage=AZURE_BLOB\nworkflow.external.payload.storage.azure_blob.connection_string=DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://127.0.0.1:10000/devstoreaccount1;EndpointSuffix=localhost\nworkflow.external.payload.storage.azure_blob.signedurlexpirationseconds=360\n</code></pre>"},{"location":"documentation/advanced/azureblob-storage.html#testing","title":"Testing","text":"<p>You can use Azurite to simulate an Azure Storage.</p>"},{"location":"documentation/advanced/azureblob-storage.html#troubleshoots","title":"Troubleshoots","text":"<ul> <li>When using es5 persistence you will receive an <code>java.lang.IllegalStateException</code> because the Netty lib will call <code>setAvailableProcessors</code> two times. To resolve this issue you need to set the following system property</li> </ul> <pre><code>es.set.netty.runtime.available.processors=false\n</code></pre> <p>If you want to change the default HTTP client of azure sdk, you can use <code>okhttp</code> instead of <code>netty</code>. For that you need to add the following dependency.</p> <pre><code>com.azure:azure-core-http-okhttp:${compatible version}\n</code></pre>"},{"location":"documentation/advanced/extend.html","title":"Extending Conductor","text":""},{"location":"documentation/advanced/extend.html#backend","title":"Backend","text":"<p>Conductor provides a pluggable backend.  The current implementation uses Dynomite.</p> <p>There are 4 interfaces that need to be implemented for each backend:</p> <pre><code>//Store for workflow and task definitions\ncom.swiftconductor.conductor.dao.MetadataDAO\n</code></pre> <pre><code>//Store for workflow executions\ncom.swiftconductor.conductor.dao.ExecutionDAO\n</code></pre> <pre><code>//Index for workflow executions\ncom.swiftconductor.conductor.dao.IndexDAO\n</code></pre> <pre><code>//Queue provider for tasks\ncom.swiftconductor.conductor.dao.QueueDAO\n</code></pre> <p>It is possible to mix and match different implementations for each of these. For example, SQS for queueing and a relational store for others.</p>"},{"location":"documentation/advanced/extend.html#system-tasks","title":"System Tasks","text":"<p>To create system tasks follow the steps below:</p> <ul> <li>Extend <code>com.swiftconductor.conductor.core.execution.tasks.WorkflowSystemTask</code></li> <li>Instantiate the new class as part of the startup (eager singleton)</li> <li>Implement the <code>TaskMapper</code> interface</li> <li>Add this implementation to the map identified by TaskMappers</li> </ul>"},{"location":"documentation/advanced/extend.html#external-payload-storage","title":"External Payload Storage","text":"<p>To configure conductor to externalize the storage of large payloads:</p> <ul> <li>Implement the <code>ExternalPayloadStorage</code> interface.</li> <li>Add the storage option to the enum here.</li> <li>Set this JVM system property <code>workflow.external.payload.storage</code> to the value of the enum element added above.</li> <li>Add a binding similar to this.</li> </ul>"},{"location":"documentation/advanced/extend.html#workflow-status-listener","title":"Workflow Status Listener","text":"<p>To provide a notification mechanism upon completion/termination of workflows:</p> <ul> <li>Implement the <code>WorkflowStatusListener</code> interface</li> <li>This can be configured to plugin custom notification/eventing upon workflows reaching a terminal state.</li> </ul>"},{"location":"documentation/advanced/extend.html#locking-service","title":"Locking Service","text":"<p>By default, Conductor Server module loads Zookeeper lock module. If you'd like to provide your own locking implementation module,  for eg., with Dynomite and Redlock:</p> <ul> <li>Implement <code>Lock</code> interface.</li> <li>Add a binding similar to this</li> <li>Enable locking service: <code>conductor.app.workflowExecutionLockEnabled: true</code></li> </ul>"},{"location":"documentation/advanced/extend.html#event-handling","title":"Event Handling","text":"<p>Provide the implementation of EventQueueProvider.</p> <p>E.g. SQS Queue Provider:  SQSEventQueueProvider.java </p>"},{"location":"documentation/advanced/externalpayloadstorage.html","title":"External Payload Storage","text":"<p>Warning</p> <p>The external payload storage is currently only implemented to be used to by the Java client. Client libraries in other languages need to be modified to enable this. Contributions are welcomed.</p>"},{"location":"documentation/advanced/externalpayloadstorage.html#context","title":"Context","text":"<p>Conductor can be configured to enforce barriers on the size of workflow and task payloads for both input and output. These barriers can be used as safeguards to prevent the usage of conductor as a data persistence system and to reduce the pressure on its datastore.</p>"},{"location":"documentation/advanced/externalpayloadstorage.html#barriers","title":"Barriers","text":"<p>Conductor typically applies two kinds of barriers:</p> <ul> <li>Soft Barrier</li> <li>Hard Barrier</li> </ul>"},{"location":"documentation/advanced/externalpayloadstorage.html#soft-barrier","title":"Soft Barrier","text":"<p>The soft barrier is used to alleviate pressure on the conductor datastore. In some special workflow use-cases, the size of the payload is warranted enough to be stored as part of the workflow execution. In such cases, conductor externalizes the storage of such payloads to S3 and uploads/downloads to/from S3 as needed during the execution. This process is completely transparent to the user/worker process.  </p>"},{"location":"documentation/advanced/externalpayloadstorage.html#hard-barrier","title":"Hard Barrier","text":"<p>The hard barriers are enforced to safeguard the conductor backend from the pressure of having to persist and deal with voluminous data which is not essential for workflow execution. In such cases, conductor will reject such payloads and will terminate/fail the workflow execution with the reasonForIncompletion set to an appropriate error message detailing the payload size.</p>"},{"location":"documentation/advanced/externalpayloadstorage.html#usage","title":"Usage","text":""},{"location":"documentation/advanced/externalpayloadstorage.html#barriers-setup","title":"Barriers setup","text":"<p>Set the following properties to the desired values in the JVM system properties:</p> Property Description default value conductor.app.workflowInputPayloadSizeThreshold Soft barrier for workflow input payload in KB 5120 conductor.app.maxWorkflowInputPayloadSizeThreshold Hard barrier for workflow input payload in KB 10240 conductor.app.workflowOutputPayloadSizeThreshold Soft barrier for workflow output payload in KB 5120 conductor.app.maxWorkflowOutputPayloadSizeThreshold Hard barrier for workflow output payload in KB 10240 conductor.app.taskInputPayloadSizeThreshold Soft barrier for task input payload in KB 3072 conductor.app.maxTaskInputPayloadSizeThreshold Hard barrier for task input payload in KB 10240 conductor.app.taskOutputPayloadSizeThreshold Soft barrier for task output payload in KB 3072 conductor.app.maxTaskOutputPayloadSizeThreshold Hard barrier for task output payload in KB 10240"},{"location":"documentation/advanced/externalpayloadstorage.html#amazon-s3","title":"Amazon S3","text":"<p>Conductor provides an implementation of Amazon S3 used to externalize large payload storage. Set the following property in the JVM system properties: <pre><code>conductor.external-payload-storage.type=S3\n</code></pre></p> <p>Note</p> <p>This implementation assumes that S3 access is configured on the instance.</p> <p>Set the following properties to the desired values in the JVM system properties:</p> Property Description default value conductor.external-payload-storage.s3.bucketName S3 bucket where the payloads will be stored conductor.external-payload-storage.s3.signedUrlExpirationDuration The expiration time in seconds of the signed url for the payload 5 <p>The payloads will be stored in the bucket configured above in a <code>UUID.json</code> file at locations determined by the type of the payload. See here for information about how the object key is determined.</p>"},{"location":"documentation/advanced/externalpayloadstorage.html#azure-blob-storage","title":"Azure Blob Storage","text":"<p>ProductLive provides an implementation of Azure Blob Storage used to externalize large payload storage.  </p> <p>To build conductor with azure blob feature read the README.md in <code>azureblob-storage</code> module </p> <p>Note</p> <p>This implementation assumes that you have an Azure Blob Storage account's connection string or SAS Token. If you want signed url to expired you must specify a Connection String. </p> <p>Set the following properties to the desired values in the JVM system properties:</p> Property Description default value workflow.external.payload.storage.azure_blob.connection_string Azure Blob Storage connection string. Required to sign Url. workflow.external.payload.storage.azure_blob.endpoint Azure Blob Storage endpoint. Optional if connection_string is set. workflow.external.payload.storage.azure_blob.sas_token Azure Blob Storage SAS Token. Must have permissions <code>Read</code> and <code>Write</code> on Resource <code>Object</code> on Service <code>Blob</code>. Optional if connection_string is set. workflow.external.payload.storage.azure_blob.container_name Azure Blob Storage container where the payloads will be stored <code>conductor-payloads</code> workflow.external.payload.storage.azure_blob.signedurlexpirationseconds The expiration time in seconds of the signed url for the payload 5 workflow.external.payload.storage.azure_blob.workflow_input_path Path prefix where workflows input will be stored with an random UUID filename workflow/input/ workflow.external.payload.storage.azure_blob.workflow_output_path Path prefix where workflows output will be stored with an random UUID filename workflow/output/ workflow.external.payload.storage.azure_blob.task_input_path Path prefix where tasks input will be stored with an random UUID filename task/input/ workflow.external.payload.storage.azure_blob.task_output_path Path prefix where tasks output will be stored with an random UUID filename task/output/ <p>The payloads will be stored as done in Amazon S3.</p>"},{"location":"documentation/advanced/externalpayloadstorage.html#postgresql-storage","title":"PostgreSQL Storage","text":"<p>Frinx provides an implementation of PostgreSQL Storage used to externalize large payload storage.</p> <p>Note</p> <p>This implementation assumes that you have an PostgreSQL database server with all required credentials.</p> <p>Set the following properties to your application.properties:</p> Property Description default value conductor.external-payload-storage.postgres.conductor-url URL, that can be used to pull the json configurations, that will be downloaded from PostgreSQL to the conductor server. For example: for local development it is <code>http://localhost:8080</code> <code>\"\"</code> conductor.external-payload-storage.postgres.url PostgreSQL database connection URL. Required to connect to database. conductor.external-payload-storage.postgres.username Username for connecting to PostgreSQL database. Required to connect to database. conductor.external-payload-storage.postgres.password Password for connecting to PostgreSQL database. Required to connect to database. conductor.external-payload-storage.postgres.table-name The PostgreSQL schema and table name where the payloads will be stored <code>external.external_payload</code> conductor.external-payload-storage.postgres.max-data-rows Maximum count of data rows in PostgreSQL database. After overcoming this limit, the oldest data will be deleted. Long.MAX_VALUE (9223372036854775807L) conductor.external-payload-storage.postgres.max-data-days Maximum count of days of data age in PostgreSQL database. After overcoming limit, the oldest data will be deleted. 0 conductor.external-payload-storage.postgres.max-data-months Maximum count of months of data age in PostgreSQL database. After overcoming limit, the oldest data will be deleted. 0 conductor.external-payload-storage.postgres.max-data-years Maximum count of years of data age in PostgreSQL database. After overcoming limit, the oldest data will be deleted. 1 <p>The maximum date age for fields in the database will be: <code>years + months + days</code> The payloads will be stored in PostgreSQL database with key (externalPayloadPath) <code>UUID.json</code> and you can generate URI for this data using <code>external-postgres-payload-resource</code> rest controller.  To make this URI work correctly, you must correctly set the conductor-url property.</p>"},{"location":"documentation/advanced/redis.html","title":"Redis","text":"<p>By default conductor runs with an in-memory Redis mock. However, you can change the configuration by setting the properties <code>conductor.db.type</code> and <code>conductor.redis.hosts</code>.</p>"},{"location":"documentation/advanced/redis.html#conductordbtype","title":"<code>conductor.db.type</code>","text":"Value Description dynomite Dynomite Cluster. Dynomite is a proxy layer that provides sharding and replication. memory Uses an in-memory Redis mock. Should be used only for development and testing purposes. redis_cluster Redis Cluster configuration. redis_sentinel Redis Sentinel configuration. redis_standalone Redis Standalone configuration."},{"location":"documentation/advanced/redis.html#conductorredishosts","title":"<code>conductor.redis.hosts</code>","text":"<p>Expected format is <code>host:port:rack</code> separated by semicolon, e.g.: </p> <pre><code>conductor.redis.hosts=host0:6379:us-east-1c;host1:6379:us-east-1c;host2:6379:us-east-1c\n</code></pre>"},{"location":"documentation/advanced/redis.html#auth-support","title":"Auth Support","text":"<p>Password authentication is supported. The password should be set as the 4th param of the first host <code>host:port:rack:password</code>, e.g.:</p> <pre><code>conductor.redis.hosts=host0:6379:us-east-1c:my_str0ng_pazz;host1:6379:us-east-1c;host2:6379:us-east-1c\n</code></pre> <p>Notes</p> <ul> <li>In a cluster, all nodes use the same password.</li> <li>In a sentinel configuration, sentinels and redis nodes use the same password.</li> </ul>"},{"location":"documentation/architecture/directed-acyclic-graph.html","title":"Directed Acyclic Graph (DAG)","text":""},{"location":"documentation/architecture/directed-acyclic-graph.html#what-is-a-directed-acyclic-graph-dag","title":"What is a Directed Acyclic Graph (DAG)?","text":"<p>Conductor workflows are directed acyclic graphs (DAGs). But, what exactly is a DAG?</p> <p>To understand a DAG, we'll walk through each term (but not in order):</p>"},{"location":"documentation/architecture/directed-acyclic-graph.html#graph","title":"Graph","text":"<p>A graph is \"a collection of vertices (or point) and edges (or lines) that indicate connections between the vertices.\"  </p> <p>By this definition, this is a graph - just not exactly correct in the context of DAGs:</p> <p></p> <p>But in the context of workflows, we're thinking of a graph more like this:</p> <p></p> <p>Imagine each vertex as a microservice, and the lines are how the microservices are connected together. However, this graph is not a directed graph - as there is no direction given to each connection.</p>"},{"location":"documentation/architecture/directed-acyclic-graph.html#directed","title":"Directed","text":"<p>A directed graph means that there is a direction to each connection. For example, this graph is directed:</p> <p></p> <p>Each arrow has a direction, Point \"N\" can proceed directly to \"B\", but \"B\" cannot proceed to \"N\" in the opposite direction.  </p>"},{"location":"documentation/architecture/directed-acyclic-graph.html#acyclic","title":"Acyclic","text":"<p>Acyclic means without circular or cyclic paths.  In the directed example above,  A -&gt; B -&gt; D -&gt; A is a cyclic loop.  </p> <p>So a Directed Acyclic Graph is a set of vertices where the connections are directed without any looping.  DAG charts can only \"move forward\" and cannot redo a step (or series of steps.)</p> <p>Since a Conductor workflow is a series of vertices that can connect in only a specific direction and cannot loop, a Conductor workflow is thus a directed acyclic graph:</p> <p></p>"},{"location":"documentation/architecture/directed-acyclic-graph.html#can-a-workflow-have-loops-and-still-be-a-dag","title":"Can a workflow have loops and still be a DAG?","text":"<p>Yes. For example, Conductor workflows have Do-While loops:</p> <p></p> <p>This is still a DAG, because the loop is just shorthand for running the tasks inside the loop over and over again.  For example, if the 2nd loop in the above image is run 3 times, the workflow path will be:</p> <ol> <li>zero_offset_fix_1</li> <li>post_to_orbit_ref_1</li> <li>zero_offset_fix_2</li> <li>post_to_orbit_ref_2</li> <li>zero_offset_fix_3</li> <li>post_to_orbit_ref_3</li> </ol> <p>The path is directed forward, and the loop just makes it easier to define the workflow.</p>"},{"location":"documentation/architecture/overview.html","title":"Overview","text":"<p>The API and storage layers are pluggable and provide ability to work with different backends and queue service providers.</p>"},{"location":"documentation/architecture/overview.html#runtime-model","title":"Runtime Model","text":"<p>Conductor follows RPC based communication model where workers are running on a separate machine from the server. Workers communicate with server over HTTP based endpoints and employs polling model for managing work queues.</p> <p></p>"},{"location":"documentation/architecture/overview.html#notes","title":"Notes","text":"<ul> <li>Workers are remote systems that communicate over HTTP with the conductor servers.</li> <li>Task Queues are used to schedule tasks for workers.  We use dyno-queues internally but it can easily be swapped with SQS or similar pub-sub mechanism.</li> <li>conductor-redis-persistence module uses Dynomite for storing the state and metadata along with Elasticsearch for indexing backend.</li> <li>See section under extending backend for implementing support for different databases for storage and indexing.</li> </ul>"},{"location":"documentation/architecture/tasklifecycle.html","title":"Task Lifecycle","text":""},{"location":"documentation/architecture/tasklifecycle.html#task-state-transitions","title":"Task state transitions","text":"<p>The figure below depicts the state transitions that a task can go through within a workflow execution.</p> <p></p>"},{"location":"documentation/architecture/tasklifecycle.html#retries-and-failure-scenarios","title":"Retries and Failure Scenarios","text":""},{"location":"documentation/architecture/tasklifecycle.html#task-failure-and-retries","title":"Task failure and retries","text":"<p>Retries for failed task executions of each task can be configured independently. <code>retryCount</code>, <code>retryDelaySeconds</code> and <code>retryLogic</code> can be used to configure the retry mechanism.</p> <p></p> <ol> <li>Worker (W1) polls for task T1 from the Conductor server and receives the task.</li> <li>Upon processing this task, the worker determines that the task execution is a failure and reports this to the server with FAILED status after 10 seconds.</li> <li>The server will persist this FAILED execution of T1. A new execution of task T1 will be created and scheduled to be polled. This task will be available to be polled after 5 (retryDelaySeconds) seconds.</li> </ol>"},{"location":"documentation/architecture/tasklifecycle.html#poll-timeout-seconds","title":"Poll Timeout Seconds","text":"<p>Poll timeout is the maximum amount of time by which a worker needs to poll a task, else the task will be marked as <code>TIMED_OUT</code>.</p> <p></p> <p>In the figure above, task T1 does not get polled by the worker within 60 seconds, so Conductor marks it as <code>TIMED_OUT</code>.</p>"},{"location":"documentation/architecture/tasklifecycle.html#timeout-seconds","title":"Timeout seconds","text":"<p>Timeout is the maximum amount of time that the task must reach a terminal state in, else it will be marked as <code>TIMED_OUT</code>.</p> <p></p> <p>0 seconds -&gt; Worker polls for task T1 from the Conductor server and receives the task. T1 is put into <code>IN_PROGRESS</code> status by the server. Worker starts processing the task but is unable to process the task at this time. Worker updates the server with T1 set to <code>IN_PROGRESS</code> status and a callback of 9 seconds. Server puts T1 back in the queue but makes it invisible and the worker continues to poll for the task but does not receive T1 for 9 seconds.  </p> <p>9,18 seconds -&gt; Worker receives T1 from the server and is still unable to process the task and updates the server with a callback of 9 seconds.</p> <p>27 seconds -&gt; Worker polls and receives task T1 from the server and is now able to process this task.</p> <p>30 seconds (T1 timeout) -&gt; Server marks T1 as <code>TIMED_OUT</code> because it is not in a terminal state after first being moved to <code>IN_PROGRESS</code> status. Server schedules a new task based on the retry count.</p> <p>32 seconds -&gt; Worker completes processing of T1 and updates the server with <code>COMPLETED</code> status. Server will ignore this update since T1 has already been moved to a terminal status (<code>TIMED_OUT</code>).</p>"},{"location":"documentation/architecture/tasklifecycle.html#response-timeout-seconds","title":"Response timeout seconds","text":"<p>Response timeout is the time within which the worker must respond to the server with an update for the task, else the task will be marked as TIMED_OUT.</p> <p></p> <p>0 seconds -&gt; Worker polls for the task T1 from the Conductor server and receives the task. T1 is put into <code>IN_PROGRESS</code> status by the server.</p> <p>Worker starts processing the task but the worker instance dies during this execution.</p> <p>20 seconds (T1 responseTimeout) -&gt; Server marks T1 as <code>TIMED_OUT</code> since the task has not been updated by the worker within the configured responseTimeoutSeconds (20). A new instance of task T1 is scheduled as per the retry configuration.</p> <p>25 seconds -&gt; The retried instance of T1 is available to be polled by the worker, after the retryDelaySeconds (5) has elapsed.</p>"},{"location":"documentation/architecture/technicaldetails.html","title":"Technical Details","text":""},{"location":"documentation/architecture/technicaldetails.html#dynamic-workflow-executions","title":"Dynamic Workflow Executions","text":"<p>In the earlier version (v1.x), Conductor allowed the execution of workflows referencing the workflow and task definitions stored as metadata in the system. This meant that a workflow execution with 10 custom tasks to run entailed:</p> <ul> <li>Registration of the 10 task definitions if they don't exist (assuming workflow task type CUSTOM for simplicity)</li> <li>Registration of the workflow definition</li> <li>Each time a definition needs to be retrieved, a call to the metadata store needed to be performed</li> <li>In addition to that, the system allowed current metadata that is in use to be altered, leading to possible inconsistencies/race conditions</li> </ul> <p>To eliminate these pain points, the execution was changed such that the workflow definition is embedded within the workflow execution and the task definitions are themselves embedded within this workflow definition. </p> <p>This enables the concept of ephemeral/dynamic workflows and tasks. Instead of fetching metadata definitions throughout the execution, the definitions are fetched and embedded into the execution at the start of the workflow execution. </p> <p>This also enabled the StartWorkflowRequest to be extended to provide the complete workflow definition that will be used during execution, thus removing the need for pre-registration. The MetadataMapperService prefetches the workflow and task definitions and embeds these within the workflow data, if not provided in the StartWorkflowRequest.</p> <p>Following benefits are seen as a result of these changes:</p> <ul> <li>Grants immutability of the definition stored within the execution data against modifications to the metadata store</li> <li>Better testability of workflows with faster experimental changes to definitions</li> <li>Reduced stress on the datastore due to prefetching the metadata only once at the start</li> </ul>"},{"location":"documentation/architecture/technicaldetails.html#maintaining-workflow-consistency","title":"Maintaining workflow consistency","text":""},{"location":"documentation/architecture/technicaldetails.html#problem","title":"Problem","text":"<p>Conductor\u2019s Workflow decide is the core logic which recursively evaluates the state of the workflow, schedules tasks, persists workflow and task(s) state at several checkpoints, and progresses the workflow.</p> <p>In a multi-node Conductor server deployment, the decide on a workflow can be triggered concurrently. For example, the worker can update Conductor server with latest task state, which calls decide, while the sweeper service (which periodically evaluates the workflow state to progress from task timeouts) would also call the decide on a different instance. The decide can be run concurrently in two different jvm nodes with two different workflow states, and based on the workflow configuration and current state, the result could be inconsistent.</p>"},{"location":"documentation/architecture/technicaldetails.html#a-two-part-solution-to-maintain-workflow-consistency","title":"A two-part solution to maintain Workflow Consistency","text":""},{"location":"documentation/architecture/technicaldetails.html#preventing-concurrent-decides-with-distributed-locking","title":"Preventing concurrent decides with distributed locking","text":"<p>The goal is to allow only one decide to run on a workflow at any given time across the whole Conductor Server cluster. This can be achieved by plugging in distributed locking implementations like Zookeeper, Redlock etc. A Zookeeper module implementing Conductor\u2019s Locking service is provided.</p>"},{"location":"documentation/architecture/technicaldetails.html#preventing-stale-data-updates-with-fencing-tokens","title":"Preventing stale data updates with fencing tokens","text":"<p>While the locking service helps to run one decide at a time, it might still be possible for nodes with timed out locks to reactivate and continue execution from where it left off (usually with stale data). This can be avoided with fencing tokens, which basically is an incrementing counter on workflow state with read-before-write support in a transaction or similar construct. </p> <p>We\u2019ve decided to first only rollout distributed locking with Zookeeper.</p>"},{"location":"documentation/architecture/technicaldetails.html#setting-up-desired-level-of-consistency","title":"Setting up desired level of consistency","text":"<p>Based on your requirements, it is possible to use none, one or both of the distributed locking and fencing tokens implementations.</p>"},{"location":"documentation/architecture/technicaldetails.html#alternative-solution-to-distributed-decide-evaluation","title":"Alternative solution to distributed \"decide\" evaluation","text":"<p>As mentioned in the previous section, the \"decide\" logic is triggered from multiple places in a conductor instance. Either a direct trigger such as user starting a workflow or a timed trigger from the Sweeper service.</p> <p>Sweeper service is responsible for continually checking state of all workflows executions and trigger the \"decide\" logic which in turn can time the workflow out.</p> <p>In a single node deployment (single Dynomite / Redis cluster and single Conductor server) this shouldn't be a problem. But when running multiple replicated Dynomite / Redis clusters and a Conductor server on top of each cluster, this might trigger the race condition described in previous section.</p> <p>Dynomite / Redis cluster is a single or multiple instance Dynomite / Redis setup that holds all the data.</p> <p>More on Dynomite / Redis HA setup: (https://netflixtechblog.com/introducing-dynomite-making-non-distributed-databases-distributed-c7bce3d89404)</p> <p>In a cluster deployment, the default behavior for Dyno Queues is such, that it distributes the workload (round-robin style) to all the conductor servers. This can create a situation where the first task to be executed is queued for conductor server #1 but the sweeper service is queued for conductor server #2.</p>"},{"location":"documentation/architecture/technicaldetails.html#more-on-task-queues","title":"More on Task Queues","text":"<p>Dyno Queues are the default queuing mechanism of Conductor.</p> <p>Queues are allocated and used for: * Task execution - each task type gets a queue * Workflow execution - single queue with all currently executing workflows (deciderQueue). This queue is used by SweeperService.</p> <p>Each conductor server instance gets its own set of queues. Or more precisely a queue shard of its own. This means that if you have 2 task types, you end up with 6 queues altogether e.g.</p> <pre><code>conductor_queues.test.QUEUE._deciderQueue.c\nconductor_queues.test.QUEUE._deciderQueue.d\nconductor_queues.test.QUEUE.HTTP.c\nconductor_queues.test.QUEUE.HTTP.d\nconductor_queues.test.QUEUE.LAMBDA.c\nconductor_queues.test.QUEUE.LAMBDA.d\n</code></pre> <p>The \"c\" and \"d\" suffixes are the shards identifying conductor server instace #1 and instance #2 respectively.</p> <p>The shard names are extracted from the Dynomite rack name such as <code>us-east-1c</code>. That is set to either <code>\"LOCAL_RACK\"</code> or <code>{EC2_AVAILABILTY_ZONE}</code>.</p> <p>Considering an execution of a simple workflow with just 2 tasks: [HTTP, LAMBDA], you should end up with queues being filled as follows:</p> <pre><code>Workflow execution    -&gt; conductor_queues.test.QUEUE._deciderQueue.c\nHTTP task execution   -&gt; conductor_queues.test.QUEUE.HTTP.d\nLAMBDA task execution -&gt; conductor_queues.test.QUEUE.LAMBDA.c\n</code></pre> <p>Which means that SweeperService in conductor instance #1 is responsible for sweeping the workflow, conductor #2 is responsible for executing HTTP task and conductor #1 again responsible for executing LAMBDA task.</p> <p>This illustrates the race condition: If the HTTP task completion in instance #2 happens at the same time as sweep in instance #1 ... you can end up with 2 different updates to a workflow execution: one update timing workflow out while the other completing the task and scheduling next.</p> <p>The round-robin strategy responsible for work distribution is defined here</p>"},{"location":"documentation/architecture/technicaldetails.html#back-to-alternative-solution","title":"Back to alternative solution","text":"<p>The alternative solution here is switching <code>round-robin</code> queue allocation for a <code>local-only</code> strategy. The <code>local-only</code> strategy meaning that a workflow and its task executions are queued only for the Conductor instance which started the workflow. </p> <p>Since all tasks and the sweeper service read/write only from/to \"local\" queues, it is impossible to run into a race condition between conductor instances.</p> <p>This completely avoids the race condition for the price of removing task execution distribution.</p> <p>The downside here is that the workload is not distributed across all Conductor servers. Which might be an advantage in active-standby deployments.</p>"},{"location":"documentation/architecture/technicaldetails.html#considering-other-downsides","title":"Considering other downsides","text":"<p>For example in a situation where a Conductor instance goes down:</p> <p>With <code>local-only</code> strategy, the workflow executions from failed conductor instance will not progress until:   * The conductor instance is restarted or   * The executions are manually terminated and restarted from a different node</p> <p>With <code>round-robin</code> strategy, there is a chance the tasks will be rescheduled on a different Conductor node. However this is nondeterministic.</p> <p>Enabling local only queue allocation strategy for Dynomite / Redis queues:</p> <p>Just enable following setting the config.properties:</p> <pre><code>workflow.dyno.queue.sharding.strategy=localOnly\n</code></pre> <p>The default is roundRobin</p>"},{"location":"documentation/architecture/technicaldetails.html#grpc","title":"gRPC","text":"<p>As part of this addition, all of the modules and bootstrap code within them were refactored to leverage providers, which facilitated moving  the Jetty server into a separate module and the conformance to Guice guidelines and best practices. </p> <p>This feature constitutes a server-side gRPC implementation along with protobuf RPC schemas for the workflow, metadata and task APIs that can be run concurrently with the Jersey-based HTTP/REST server. The protobuf models for all the types are exposed through the API. gRPC java clients for the workflow, metadata and task APIs are also available for use. Another valuable addition is an idiomatic Go gRPC client implementation for the worker API.</p> <p>The proto models are auto-generated at compile time using this ProtoGen library. This custom library adds messageInput and messageOutput fields to all proto tasks and task definitions. </p> <p>The goal of these fields is providing a type-safe way to pass input and input metadata through tasks that use the gRPC API. These fields use the Any protobuf type which can store any arbitrary message type in a type-safe way, without the server needing to know the exact serialization format of the message. </p> <p>In order to expose these Any objects in the REST API, a custom encoding is used that contains the raw data of the serialized message by converting it into a dictionary with '@type' and '@value' keys, where '@type' is identical to the canonical representation and '@value' contains a base64 encoded string with the binary data of the serialized message. </p> <p>The JsonMapperProvider provides the object mapper initialized with this module to enable serialization/deserialization of these JSON objects.</p>"},{"location":"documentation/architecture/technicaldetails.html#elasticsearch-support","title":"ElasticSearch Support","text":"<p>Indexing workflow execution is one of the primary features of Conductor. This enables archival of terminal state workflows from the primary data store, along with providing a clean search capability from the UI. </p> <p>In Conductor 1.x, we supported both versions 2 and 5 of Elasticsearch by shadowing version 5 and all its dependencies. This proved to be rather tedious increasing build times by over 10 minutes. </p> <p>In Conductor 2.x, we have removed active support for ES 2.x, because of valuable community contributions for elasticsearch 5 and elasticsearch 6 modules. </p> <p>Unlike Conductor 1.x, Conductor 2.x supports elasticsearch 5 by default, which can easily be replaced with version 6 if needed.</p>"},{"location":"documentation/architecture/technicaldetails.html#decoupling-elasticsearch-from-persistence","title":"Decoupling ElasticSearch from Persistence","text":"<p>In the earlier version (1.x), the indexing logic was imbibed within the persistence layer, thus creating a tight coupling between the primary datastore and the indexing engine. </p> <p>This meant that the primary datastore determines how we orchestrate between the storage (redis, mysql, etc) and the indexer(elastic search). </p> <p>The main disadvantage of this approach is the lack of flexibility, that is, we cannot run an in-memory database and external elastic search or vice-versa.</p> <p>We plan to improve this further by removing the indexing from the critical path of workflow execution, thus reducing possible points of failure during execution.</p>"},{"location":"documentation/architecture/technicaldetails.html#cassandra-persistence","title":"Cassandra Persistence","text":"<p>The Cassandra persistence layer currently provides a partial implementation of the ExecutionDAO that supports all the CRUD operations for tasks and workflow execution. The data modelling is done in a denormalized manner and stored in two tables. The \"workflows\" table houses all the information for a workflow execution including all its tasks and is the source of truth for all the information regarding a workflow and its tasks. The \"task_lookup\" table, as the name suggests stores a lookup of taskIds to workflowId. This table facilitates the fast retrieval of task data given a taskId. </p> <p>All the datastore operations that are used during the critical execution path of a workflow have been implemented currently. Few of the operational abilities of the ExecutionDAO are yet to be implemented. This module also does not provide implementations for QueueDAO, PollDataDAO and RateLimitingDAO. We envision using the Cassandra DAO with an external queue implementation, since implementing a queuing recipe on top of Cassandra is an anti-pattern that we want to stay away from.</p>"},{"location":"documentation/architecture/technicaldetails.html#external-payload-storage","title":"External Payload Storage","text":"<p>The implementation of this feature is such that the externalization of payloads is fully transparent and automated to the user. Conductor operators can configure the usage of this feature and is completely abstracted and hidden from the user, thereby allowing the operators full control over the barrier limits. </p> <p>Currently, only AWS S3 is supported as a storage system, however, as with all other Conductor components, this is pluggable and can be extended to enable any other object store to be used as an external payload storage system.</p> <p>The externalization of payloads is enforced using two kinds of barriers. Soft barriers are used when the  payload size is warranted enough to be stored as part of workflow execution. These payloads will be stored in external storage and used during execution. Hard barriers are enforced to safeguard against voluminous data, and such payloads are rejected and the workflow execution is failed.</p> <p>The payload size is evaluated in the client before being sent over the wire to the server. If the payload size exceeds the configured soft limit, the client makes a request to the server for the location at which the payload is to be stored. </p> <p>In this case where S3 is being used, the server returns a signed url for the location and the client uploads the payload using this signed url. </p> <p>The relative path to the payload object is then stored in the workflow/task metadata. The server can then download this payload from this path and use as needed during execution. </p> <p>This allows the server to control access to the S3 bucket, thereby making the user applications where the worker processes are run completely agnostic of the permissions needed to access this location.</p>"},{"location":"documentation/clientsdks/index.html","title":"Client SDKs","text":"<p>Conductor tasks that are executed by remote workers communicate over HTTP endpoints/gRPC to poll for the task and update the status of the execution. The follow SDKs are provided for implementing Conductor workers.</p> <ul> <li>Python</li> <li>Java</li> <li>.NET</li> <li>Go</li> </ul>"},{"location":"documentation/clientsdks/csharp-sdk.html","title":".NET Client SDK","text":"<p>The .NET Client SDK <code>conductor-client-dotnet</code> provides a way to build Task Workers and Clients in C#.</p> <p>The code for the .NET Client SDK is available on Github. Please feel free to file PRs, issues, etc. there. </p>"},{"location":"documentation/clientsdks/csharp-sdk.html#quick-start","title":"Quick Start","text":"<p>Install the latest version of <code>swift-conductor-client</code> from NuGet:</p> <pre><code>dotnet add package swift-conductor-client\n</code></pre> <p>See .NET Client SDK docs for details.</p>"},{"location":"documentation/clientsdks/go-sdk.html","title":"Go Client SDK","text":"<p>The Go Client SDK <code>conductor-client-golang</code> provides a way to build Task Workers and Clients in Go.</p> <p>The code for the Go Client SDK is available on Github. Please feel free to file PRs, issues, etc. there.</p>"},{"location":"documentation/clientsdks/go-sdk.html#quick-start","title":"Quick Start","text":"<p>Get the latest Go Client SDK:</p> <pre><code>go get github.com/swift-conductor/conductor-client-golang\n</code></pre> <p>See Go Client SDK docs for details.</p>"},{"location":"documentation/clientsdks/java-sdk.html","title":"Java Client SDK","text":"<p>Conductor provides the following java clients to interact with the various APIs</p> Client Usage Metadata Client Register / Update workflow and task definitions Workflow Client Start a new workflow / Get execution status of a workflow Task Client Poll for task / Update task result after execution / Get status of a task"},{"location":"documentation/clientsdks/java-sdk.html#worker","title":"Worker","text":"<p>Conductor provides an automated framework to poll for tasks, manage the execution thread and update the status of the execution back to the server.</p> <p>Implement the Worker interface to execute the task.</p>"},{"location":"documentation/clientsdks/java-sdk.html#taskrunnerconfigurer","title":"TaskRunnerConfigurer","text":"<p>The TaskRunnerConfigurer can be used to register the worker(s) and initialize the polling loop.  </p> <p>Manages the task workers thread pool and server communication (poll and task update).  </p> <p>Use the Builder to create an instance of the TaskRunnerConfigurer. The Builder constructor takes the following parameters.</p> Parameter Description TaskClient TaskClient used to communicate to the Conductor server Workers Workers that will be used for polling work and task execution. <p>The builder accepts the following parameters:</p> Parameter Description Default withEurekaClient EurekaClient is used to identify if the server is in discovery or not.  When the server goes out of discovery, the polling is stopped unless <code>pollOutOfDiscovery</code> is set to true. If passed null, discovery check is not done. provided by platform withThreadCount Number of threads assigned to the workers. Should be at-least the size of taskWorkers to avoid starvation in a busy system. Number of registered workers withSleepWhenRetry Time in milliseconds, for which the thread should sleep when task update call fails, before retrying the operation. 500 withUpdateRetryCount Number of attempts to be made when updating task status when update status call fails. 3 withWorkerNamePrefix String prefix that will be used for all the workers. workflow-worker- withShutdownGracePeriodSeconds Waiting seconds before forcing shutdown of your worker 10 <p>Once an instance is created, call <code>init()</code> method to initialize the TaskPollExecutor and begin the polling and execution of tasks.</p> <p>Note</p> <p>To ensure that the TaskRunnerConfigurer stops polling for tasks when the instance becomes unhealthy, call the provided <code>shutdown()</code> hook in a <code>PreDestroy</code> block.</p>"},{"location":"documentation/clientsdks/java-sdk.html#properties","title":"Properties","text":"<p>The worker behavior can be further controlled by using these properties:</p> Property Type Description Default paused boolean If set to true, the worker stops polling. false pollInterval int Interval in milliseconds at which the server should be polled for tasks. 1000 pollOutOfDiscovery boolean If set to true, the instance will poll for tasks regardless of the discovery   status. This is useful while running on a dev machine. false <p>Further, these properties can be set either by Worker implementation or by setting the following system properties in the JVM:</p> Name Description <code>conductor.worker.&lt;property&gt;</code> Applies to ALL the workers in the JVM. <code>conductor.worker.&lt;taskDefName&gt;.&lt;property&gt;</code> Applies to the specified worker.  Overrides the global property."},{"location":"documentation/clientsdks/java-sdk.html#examples","title":"Examples","text":"<ul> <li> <p>Sample Worker Implementation</p> </li> <li> <p>Example</p> </li> </ul>"},{"location":"documentation/clientsdks/python-sdk.html","title":"Python Client SDK","text":"<p>The code for the Python Client SDK is available on GitHub. Please feel free to file PRs, issues, etc. there.</p>"},{"location":"documentation/clientsdks/python-sdk.html#quick-start","title":"Quick Start","text":"<p>Install the latest version of <code>swift-conductor-client</code> from PyPi:</p> <pre><code>pip install swift-conductor-client\n</code></pre> <p>See Python Client SDK docs for details.</p>"},{"location":"documentation/clientsdks/typescript-sdk.html","title":"TypeScript Client SDK","text":"<p>The code for the TypeScript Client SDK is available on GitHub. Please feel free to file PRs, issues, etc. there.</p>"},{"location":"documentation/clientsdks/typescript-sdk.html#quick-start","title":"Quick Start","text":"<p>Install the latest version of <code>swift-conductor-client</code> from NPM:</p> <pre><code>npm install @swift-conductor/conductor-client\n</code></pre> <p>See TypeScript Client SDK docs for details.</p>"},{"location":"documentation/configuration/eventhandlers.html","title":"Event Handlers","text":"<p>Eventing in Conductor provides for loose coupling between workflows and support for producing and consuming events from external systems.</p> <p>This includes:</p> <ol> <li>Being able to produce an event (message) in an external system like SQS or internal to Conductor. </li> <li>Start a workflow when a specific event occurs that matches the provided criteria.</li> </ol> <p>Conductor provides SUB_WORKFLOW task that can be used to embed a workflow inside parent workflow.  Eventing supports provides similar capability without explicitly adding dependencies and provides fire-and-forget style integrations.</p>"},{"location":"documentation/configuration/eventhandlers.html#event-task","title":"Event Task","text":"<p>Event task provides ability to publish an event (message) to either Conductor or an external eventing system like SQS. Event tasks are useful for creating event based dependencies for workflows and tasks.</p> <p>See Event Task for documentation.</p>"},{"location":"documentation/configuration/eventhandlers.html#event-handler","title":"Event Handler","text":"<p>Event handlers are listeners registered that executes an action when a matching event occurs.  The supported actions are:</p> <ol> <li>Start a Workflow</li> <li>Fail a Task</li> <li>Complete a Task</li> </ol> <p>Event Handlers can be configured to listen to Conductor Events or an external event like SQS.</p>"},{"location":"documentation/configuration/eventhandlers.html#configuration","title":"Configuration","text":"<p>Event Handlers are configured via <code>/event/</code> APIs.</p>"},{"location":"documentation/configuration/eventhandlers.html#structure","title":"Structure","text":"<p><pre><code>{\n  \"name\" : \"descriptive unique name\",\n  \"event\": \"event_type:event_location\",\n  \"condition\": \"boolean condition\",\n  \"actions\": [\"see examples below\"]\n}\n</code></pre> <code>condition</code> is an expression that MUST evaluate to a boolean value.  A Javascript like syntax is supported that can be used to evaluate condition based on the payload. Actions are executed only when the condition evaluates to <code>true</code>.</p>"},{"location":"documentation/configuration/eventhandlers.html#examples","title":"Examples","text":""},{"location":"documentation/configuration/eventhandlers.html#condition","title":"Condition","text":"<p>Given the following payload in the message:</p> <pre><code>{\n    \"fileType\": \"AUDIO\",\n    \"version\": 3,\n    \"metadata\": {\n       \"length\": 300,\n       \"codec\": \"aac\"\n    }\n}\n</code></pre> <p>The following expressions can be used in <code>condition</code> with the indicated results:</p> Expression Result <code>$.version &gt; 1</code> true <code>$.version &gt; 10</code> false <code>$.metadata.length == 300</code> true"},{"location":"documentation/configuration/eventhandlers.html#actions","title":"Actions","text":"<p>Examples of actions that can be configured in the <code>actions</code> array:</p> <p>To start a workflow</p> <pre><code>{\n    \"action\": \"start_workflow\",\n    \"start_workflow\": {\n        \"name\": \"WORKFLOW_NAME\",\n        \"version\": \"&lt;optional_param&gt;\",\n        \"input\": {\n            \"param1\": \"${param1}\" \n        }\n    }\n}\n</code></pre> <p>To complete a task</p> <pre><code>{\n    \"action\": \"complete_task\",\n    \"complete_task\": {\n      \"workflowId\": \"${workflowId}\",\n      \"taskRefName\": \"task_1\",\n      \"output\": {\n        \"response\": \"${result}\"\n      }\n    },\n    \"expandInlineJSON\": true\n}\n</code></pre> <p>To fail a task*</p> <p><pre><code>{\n    \"action\": \"fail_task\",\n    \"fail_task\": {\n      \"workflowId\": \"${workflowId}\",\n      \"taskRefName\": \"task_1\",\n      \"output\": {\n        \"response\": \"${result}\"\n      }\n    },\n    \"expandInlineJSON\": true\n}\n</code></pre> Input for starting a workflow and output when completing / failing task follows the same expressions used for wiring task inputs.</p> <p>Expanding stringified JSON elements in payload</p> <p><code>expandInlineJSON</code> property, when set to true will expand the inlined stringified JSON elements in the payload to JSON documents and replace the string value with JSON document. This feature allows such elements to be used with JSON path expressions. </p>"},{"location":"documentation/configuration/isolation-groups.html","title":"Isolation Groups","text":"<p>Consider an HTTP task where the latency of an API is high, task queue piles up effecting execution of other HTTP tasks which have low latency.</p> <p>We can isolate the execution of such tasks to have predictable performance using <code>isolationgroupId</code>, a property of task definition.</p> <p>When we set isolationGroupId,  the executor <code>SystemTaskWorkerCoordinator</code> will allocate an isolated queue and an isolated thread pool for execution of those tasks.</p> <p>If no <code>isolationgroupId</code> is specified in task definition, then fallback is default behaviour where the executor executes the task in shared thread-pool for all tasks. </p>"},{"location":"documentation/configuration/isolation-groups.html#example","title":"Example","text":"<p>** Task Definition ** <pre><code>{\n  \"name\": \"encode_task\",\n  \"retryCount\": 3,\n\n  \"timeoutSeconds\": 1200,\n  \"inputKeys\": [\n    \"sourceRequestId\",\n    \"qcElementType\"\n  ],\n  \"outputKeys\": [\n    \"state\",\n    \"skipped\",\n    \"result\"\n  ],\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 600,\n  \"responseTimeoutSeconds\": 3600,\n  \"concurrentExecLimit\": 100,\n  \"rateLimitFrequencyInSeconds\": 60,\n  \"rateLimitPerFrequency\": 50,\n  \"isolationgroupId\": \"myIsolationGroupId\"\n}\n</code></pre> ** Workflow Definition ** <pre><code>{\n  \"name\": \"encode_and_deploy\",\n  \"description\": \"Encodes a file and deploys to CDN\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"encode\",\n      \"taskReferenceName\": \"encode\",\n      \"type\": \"HTTP\", \n      \"inputParameters\": {\n        \"http_request\": {\n          \"uri\": \"http://localhost:9200/conductor/_search?size=10\",\n          \"method\": \"GET\"\n        }\n      }\n    }\n  ],\n  \"outputParameters\": {\n    \"cdn_url\": \"${d1.output.location}\"\n  },\n  \"failureWorkflow\": \"cleanup_encode_resources\",\n  \"restartable\": true,\n  \"workflowStatusListenerEnabled\": true,\n  \"schemaVersion\": 2\n}\n</code></pre></p> <ul> <li>puts <code>encode</code> in <code>HTTP-myIsolationGroupId</code> queue, and allocates a new thread pool for this for execution.</li> </ul> <p>Note:   To enable this feature, the <code>workflow.isolated.system.task.enable</code> property needs to be made <code>true</code>,its default value is <code>false</code></p> <p>The property <code>workflow.isolated.system.task.worker.thread.count</code>  sets the thread pool size for isolated tasks; default is <code>1</code>.</p> <p>isolationGroupId is currently supported only in HTTP and kafka Task. </p>"},{"location":"documentation/configuration/isolation-groups.html#execution-name-space","title":"Execution Name Space","text":"<p><code>executionNameSpace</code> A property of taskdef can be used to provide JVM isolation to task execution and scale executor deployments horizontally.</p> <p>Limitation of using isolationGroupId is that we need to scale executors vertically as the executor allocates a new thread pool per <code>isolationGroupId</code>.  Also, since the executor runs the tasks in the same JVM, task execution is not isolated completely. </p> <p>To support JVM isolation, and also allow the executors to scale horizontally, we can use <code>executionNameSpace</code> property in taskdef.</p> <p>Executor consumes tasks whose executionNameSpace matches with the configuration property <code>workflow.system.task.worker.executionNameSpace</code></p> <p>If the property is not set, the executor executes tasks without any executionNameSpace set. </p> <pre><code>{\n  \"name\": \"encode_task\",\n  \"retryCount\": 3,\n\n  \"timeoutSeconds\": 1200,\n  \"inputKeys\": [\n    \"sourceRequestId\",\n    \"qcElementType\"\n  ],\n  \"outputKeys\": [\n    \"state\",\n    \"skipped\",\n    \"result\"\n  ],\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 600,\n  \"responseTimeoutSeconds\": 3600,\n  \"concurrentExecLimit\": 100,\n  \"rateLimitFrequencyInSeconds\": 60,\n  \"rateLimitPerFrequency\": 50,\n  \"executionNameSpace\": \"myExecutionNameSpace\"\n}\n</code></pre>"},{"location":"documentation/configuration/isolation-groups.html#example-workflow-task","title":"Example Workflow task","text":"<pre><code>{ \n  \"name\": \"encode_and_deploy\",\n  \"description\": \"Encodes a file and deploys to CDN\",\n  \"version\": 1,\n  \"tasks\": [\n    { \n      \"name\": \"encode\",\n      \"taskReferenceName\": \"encode\",\n      \"type\": \"HTTP\", \n      \"inputParameters\": {\n        \"http_request\": {\n          \"uri\": \"http://localhost:9200/conductor/_search?size=10\",\n          \"method\": \"GET\"\n        }\n      }\n    }\n  ],\n  \"outputParameters\": {\n    \"cdn_url\": \"${d1.output.location}\"\n  },\n  \"failureWorkflow\": \"cleanup_encode_resources\",\n  \"restartable\": true,\n  \"workflowStatusListenerEnabled\": true,\n  \"schemaVersion\": 2\n}\n</code></pre> <ul> <li><code>encode</code> task is executed by the executor deployment whose <code>workflow.system.task.worker.executionNameSpace</code> property is <code>myExecutionNameSpace</code> </li> </ul> <p><code>executionNameSpace</code> can be used along with <code>isolationGroupId</code></p> <p>If the above task contains a isolationGroupId <code>myIsolationGroupId</code>, the tasks will be scheduled in a queue HTTP@myExecutionNameSpace-myIsolationGroupId, and have a new threadpool for execution in the deployment group with myExecutionNameSpace</p>"},{"location":"documentation/configuration/taskdef.html","title":"Task Definition","text":"<p>Task Definitions are used to register CUSTOM tasks (workers). Conductor maintains a registry of user task types. A task type MUST be registered before being used in a workflow.</p> <p>This should not be confused with Task Configurations which are part of the Workflow Definition, and are iterated in the <code>tasks</code> property in the definition.</p>"},{"location":"documentation/configuration/taskdef.html#schema","title":"Schema","text":"Field Type Description Notes name string Task Name. Unique name of the Task that resonates with its function. Must be unique description string Description of the task Optional retryCount number Number of retries to attempt when a Task is marked as failure Defaults to 3 with maximum allowed capped at 10 retryLogic string (enum) Mechanism for the retries See Retry Logic retryDelaySeconds number Time to wait before retries Defaults to 60 seconds timeoutPolicy string (enum) Task's timeout policy Defaults to <code>TIME_OUT_WF</code>; See Timeout Policy timeoutSeconds number Time in seconds, after which the task is marked as <code>TIMED_OUT</code> if not completed after transitioning to <code>IN_PROGRESS</code> status for the first time No timeouts if set to 0 responseTimeoutSeconds number If greater than 0, the task is rescheduled if not updated with a status after this time (heartbeat mechanism). Useful when the worker polls for the task but fails to complete due to errors/network failure. Defaults to 3600 pollTimeoutSeconds number Time in seconds, after which the task is marked as <code>TIMED_OUT</code> if not polled by a worker No timeouts if set to 0 inputKeys array of string(s) Array of keys of task's expected input. Used for documenting task's input. Optional. See Using inputKeys and outputKeys. outputKeys array of string(s) Array of keys of task's expected output. Used for documenting task's output. Optional. See Using inputKeys and outputKeys. inputTemplate object Define default input values. Optional. See Using inputTemplate concurrentExecLimit number Number of tasks that can be executed at any given time Optional rateLimitFrequencyInSeconds number Sets the rate limit frequency window. Optional. See Task Rate limits rateLimitPerFrequency number Sets the max number of tasks that can be given to workers within window. Optional. See Task Rate limits below ownerEmail string Email address of the team that owns the task Required"},{"location":"documentation/configuration/taskdef.html#retry-logic","title":"Retry Logic","text":"<ul> <li>FIXED: Reschedule the task after <code>retryDelaySeconds</code></li> <li>EXPONENTIAL_BACKOFF: Reschedule the task after <code>retryDelaySeconds * (2 ^ attemptNumber)</code></li> <li>LINEAR_BACKOFF: Reschedule after <code>retryDelaySeconds * backoffRate * attemptNumber</code></li> </ul>"},{"location":"documentation/configuration/taskdef.html#timeout-policy","title":"Timeout Policy","text":"<ul> <li>RETRY: Retries the task again</li> <li>TIME_OUT_WF: Workflow is marked as TIMED_OUT and terminated. This is the default value.</li> <li>ALERT_ONLY: Registers a counter (task_timeout)</li> </ul>"},{"location":"documentation/configuration/taskdef.html#task-concurrent-execution-limits","title":"Task Concurrent Execution Limits","text":"<p><code>concurrentExecLimit</code> limits the number of simultaneous Task executions at any point.</p> <p>Example  You have 1000 task executions waiting in the queue, and 1000 workers polling this queue for tasks, but if you have set <code>concurrentExecLimit</code> to 10, only 10 tasks would be given to workers (which would lead to starvation). If any of the workers finishes execution, a new task(s) will be removed from the queue, while still keeping the current execution count to 10.</p>"},{"location":"documentation/configuration/taskdef.html#task-rate-limits","title":"Task Rate Limits","text":"<p>Rate Limiting</p> <p>Rate limiting is only supported for the Redis-persistence module and is not available with other persistence layers.</p> <ul> <li><code>rateLimitFrequencyInSeconds</code> and <code>rateLimitPerFrequency</code> should be used together.</li> <li><code>rateLimitFrequencyInSeconds</code> sets the \"frequency window\", i.e the <code>duration</code> to be used in <code>events per duration</code>. Eg: 1s, 5s, 60s, 300s etc.</li> <li><code>rateLimitPerFrequency</code>defines the number of Tasks that can be given to Workers per given \"frequency window\". No rate limit if set to 0.</li> </ul> <p>Example Let's set <code>rateLimitFrequencyInSeconds = 5</code>, and <code>rateLimitPerFrequency = 12</code>. This means our frequency window is of 5 seconds duration, and for each frequency window, Conductor would only give 12 tasks to workers. So, in a given minute, Conductor would only give 12*(60/5) = 144 tasks to workers irrespective of the number of workers that are polling for the task.  </p> <p>Note that unlike <code>concurrentExecLimit</code>, rate limiting doesn't take into account tasks already in progress/completed. Even if all the previous tasks are executed within 1 sec, or would take a few days, the new tasks are still given to workers at configured frequency, 144 tasks per minute in above example.   </p>"},{"location":"documentation/configuration/taskdef.html#using-inputkeys-and-outputkeys","title":"Using <code>inputKeys</code> and <code>outputKeys</code>","text":"<ul> <li><code>inputKeys</code> and <code>outputKeys</code> can be considered as parameters and return values for the Task.</li> <li>Consider the task Definition as being represented by an interface: <code>(value1, value2 .. valueN) someTaskDefinition(key1, key2 .. keyN);</code></li> <li>However, these parameters are not strictly enforced at the moment. Both <code>inputKeys</code> and <code>outputKeys</code> act as a documentation for task re-use. The tasks in workflow need not define all of the keys in the task definition.</li> <li>In the future, this can be extended to be a strict template that all task implementations must adhere to, just like interfaces in programming languages.</li> </ul>"},{"location":"documentation/configuration/taskdef.html#using-inputtemplate","title":"Using <code>inputTemplate</code>","text":"<ul> <li><code>inputTemplate</code> allows to define default values, which can be overridden by values provided in Workflow.</li> <li>Eg: In your Task Definition, you can define your inputTemplate as:</li> </ul> <pre><code>\"inputTemplate\": {\n    \"url\": \"https://some_url:7004\"\n}\n</code></pre> <ul> <li>Now, in your workflow Definition, when using above task, you can use the default <code>url</code> or override with something else in the task's <code>inputParameters</code>.</li> </ul> <pre><code>\"inputParameters\": {\n    \"url\": \"${workflow.input.some_new_url}\"\n}\n</code></pre>"},{"location":"documentation/configuration/taskdef.html#complete-example","title":"Complete Example","text":"<p>This is an example of a Task Definition for a worker implementation named <code>encode_task</code>.</p> <pre><code>{\n  \"name\": \"encode_task\",\n  \"retryCount\": 3,\n  \"timeoutSeconds\": 1200,\n  \"inputKeys\": [\n    \"sourceRequestId\",\n    \"qcElementType\"\n  ],\n  \"outputKeys\": [\n    \"state\",\n    \"skipped\",\n    \"result\"\n  ],\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 600,\n  \"responseTimeoutSeconds\": 3600,\n  \"pollTimeoutSeconds\": 3600,\n  \"concurrentExecLimit\": 100,\n  \"rateLimitFrequencyInSeconds\": 60,\n  \"rateLimitPerFrequency\": 50,\n  \"ownerEmail\": \"foo@bar.com\",\n  \"description\": \"Sample Encoding task\"\n}\n</code></pre>"},{"location":"documentation/configuration/taskdomains.html","title":"Task Domains","text":"<p>Task domains helps support task development. The idea is same \"task definition\" can be implemented in different \"domains\". A domain is some arbitrary name that the developer controls. So when the workflow is started, the caller can specify, out of all the tasks in the workflow, which tasks need to run in a specific domain, this domain is then used to poll for task on the client side to execute it.  </p> <p>As an example if a workflow (WF1) has 3 tasks T1, T2, T3. The workflow is deployed and working fine, which means there are T2 workers polling and executing. If you modify T2 and run it locally there is no guarantee that your modified T2 worker will get the task that you are looking for as it coming from the general T2 queue. \"Task Domain\" feature solves this problem by splitting the T2 queue by domains, so when the app polls for task T2 in a specific domain, it get the correct task.</p> <p>When starting a workflow multiple domains can be specified as a fall backs, for example \"domain1,domain2\". Conductor keeps track of last polling time for each task, so in this case it checks if the there are any active workers (workers polled at least once in a 10 second window) for \"domain1\" then the task is put in \"domain1\", if not then the same check is done for the next domain in sequence \"domain2\" and so on.</p> <p>If no workers are active for the domains provided:</p> <ul> <li>If <code>NO_DOMAIN</code> is provided as last token in list of domains, then no domain is set.</li> <li>Else, task will be added to last inactive domain in list of domains, hoping that workers would soon be available for that domain.</li> </ul> <p>Also, a <code>*</code> token can be used to apply domains for all tasks. This can be overridden by providing task specific mappings along with <code>*</code>. </p> <p>For example, the below configuration:</p> <pre><code>\"taskToDomain\": {\n  \"*\": \"mydomain\",\n  \"some_task_x\":\"NO_DOMAIN\",\n  \"some_task_y\": \"someDomain, NO_DOMAIN\",\n  \"some_task_z\": \"someInactiveDomain1, someInactiveDomain2\"\n}\n</code></pre> <ul> <li>puts <code>some_task_x</code> in default queue (no domain).</li> <li>puts <code>some_task_y</code> in <code>someDomain</code> domain, if available or in default otherwise.</li> <li>puts <code>some_task_z</code> in <code>someInactiveDomain2</code>, even though workers are not available yet.</li> <li>and puts all other tasks in <code>mydomain</code> (even if workers are not available).</li> </ul> <p>Note that this \"fall back\" type domain strings can only be used when starting the workflow, when polling from the client only one domain is used. Also, <code>NO_DOMAIN</code> token should be used last.</p>"},{"location":"documentation/configuration/taskdomains.html#how-to-use-task-domains","title":"How to use Task Domains","text":""},{"location":"documentation/configuration/taskdomains.html#change-the-poll-call","title":"Change the poll call","text":"<p>The poll call must now specify the domain. </p>"},{"location":"documentation/configuration/taskdomains.html#java-client","title":"Java Client","text":"<p>If you are using the java client then a simple property change will force  TaskRunnerConfigurer to pass the domain to the poller. <pre><code>    conductor.worker.T2.domain=mydomain //Task T2 needs to poll for domain \"mydomain\"\n</code></pre></p>"},{"location":"documentation/configuration/taskdomains.html#rest-call","title":"REST call","text":"<p><code>GET /api/tasks/poll/batch/T2?workerid=myworker&amp;domain=mydomain</code> <code>GET /api/tasks/poll/T2?workerid=myworker&amp;domain=mydomain</code></p>"},{"location":"documentation/configuration/taskdomains.html#change-the-start-workflow-call","title":"Change the start workflow call","text":"<p>When starting the workflow, make sure the task to domain mapping is passes</p>"},{"location":"documentation/configuration/taskdomains.html#java-client_1","title":"Java Client","text":"<pre><code>{Map&lt;String, Object&gt; input = new HashMap&lt;&gt;();\ninput.put(\"wf_input1\", \"one\");\n\nMap&lt;String, String&gt; taskToDomain = new HashMap&lt;&gt;();\ntaskToDomain.put(\"T2\", \"mydomain\");\n\n// Other options ...\n// taskToDomain.put(\"*\", \"mydomain, NO_DOMAIN\")\n// taskToDomain.put(\"T2\", \"mydomain, fallbackDomain1, fallbackDomain2\")\n\nStartWorkflowRequest swr = new StartWorkflowRequest();\nswr.withName(\"myWorkflow\")\n    .withCorrelationId(\"corr1\")\n    .withVersion(1)\n    .withInput(input)\n    .withTaskToDomain(taskToDomain);\n\nwfclient.startWorkflow(swr);\n</code></pre>"},{"location":"documentation/configuration/taskdomains.html#rest-call_1","title":"REST call","text":"<p><code>POST /api/workflow</code></p> <pre><code>{\n  \"name\": \"myWorkflow\",\n  \"version\": 1,\n  \"correlatonId\": \"corr1\"\n  \"input\": {\n    \"wf_input1\": \"one\"\n  },\n  \"taskToDomain\": {\n    \"*\": \"mydomain\",\n    \"some_task_x\":\"NO_DOMAIN\",\n    \"some_task_y\": \"someDomain, NO_DOMAIN\"\n  }\n}\n</code></pre>"},{"location":"documentation/configuration/workerdef.html","title":"Custom Tasks","text":"<p>A worker is responsible for executing a worker task (<code>type=CUSTOM</code>). Operator and System Tasks are handled by the Conductor server, while user defined tasks (<code>type=CUSTOM</code>) need to have a worker created that awaits the work to be scheduled by the server for it to be executed.</p> <p>Worker implementation should follow these basic design principles:</p> <ol> <li>Workers are stateless and do not implement a workflow specific logic.  </li> <li>Each worker executes a very specific task and produces well defined output given specific inputs.</li> <li>Workers are meant to be idempotent (or should handle cases where the task that partially executed gets rescheduled due to timeouts etc.)</li> <li>Workers do not implement the logic to handle retries etc, that is taken care by the Conductor server.</li> </ol> <p>Workers can be implemented in any language, and Conductor comes with Python, Java, and Golang clients that make creating workers easy by providing features such as polling threads, metrics and server communication.</p>"},{"location":"documentation/configuration/workflowdef/index.html","title":"Workflow Definition","text":"<p>The Workflow Definition contains all the information necessary to define the behavior of a workflow. The most important part of this definition is the <code>tasks</code> property, which is an array of Workflow Tasks.</p>"},{"location":"documentation/configuration/workflowdef/index.html#workflow-properties","title":"Workflow Properties","text":"Field Type Description Notes name string Name of the workflow description string Description of the workflow Optional version number Numeric field used to identify the version of the schema. Use incrementing numbers. When starting a workflow execution, if not specified, the definition with highest version is used tasks array of object(s) An array of task configurations. Details inputParameters array of string(s) List of input parameters. Used for documenting the required inputs to workflow Optional. outputParameters object JSON template used to generate the output of the workflow If not specified, the output is defined as the output of the last executed task inputTemplate object Default input values. See Using inputTemplate Optional. failureWorkflow string Workflow to be run on current Workflow failure. Useful for cleanup or post actions on failure. Explanation Optional. schemaVersion number Current Conductor Schema version. schemaVersion 1 is discontinued. Must be 2 restartable boolean Flag to allow Workflow restarts Defaults to true workflowStatusListenerEnabled boolean Enable status callback. Explanation Defaults to false ownerEmail string Email address of the team that owns the workflow Required timeoutSeconds number The timeout in seconds after which the workflow will be marked as <code>TIMED_OUT</code> if it hasn't been moved to a terminal state No timeouts if set to 0 timeoutPolicy string (enum) Workflow's timeout policy Defaults to <code>TIME_OUT_WF</code>"},{"location":"documentation/configuration/workflowdef/index.html#workflow-tasks","title":"Workflow Tasks","text":"<p>The <code>tasks</code> property in a Workflow Definition defines an array of Workflow Tasks. This is the blueprint for the workflow. Workflow Tasks can reference different types of Tasks.</p> <ul> <li>System Tasks</li> <li>System Operators</li> <li>Custom Tasks</li> </ul> <p>Note: Task Configuration should not be confused with Task Definitions, which are used to register CUSTOM (worker based) tasks.</p> Field Type Description Notes name string Name of the task. MUST be registered as a Task Type with Conductor before starting workflow taskReferenceName string Alias used to refer the task within the workflow. MUST be unique within workflow. type string Type of task. CUSTOM for tasks executed by remote workers, or one of the system task types description string Description of the task optional optional boolean true or false. When set to true - workflow continues even if the task fails. The status of the task is reflected as <code>COMPLETED_WITH_ERRORS</code> Defaults to <code>false</code> inputParameters object JSON template that defines the input given to the task. Only one of <code>inputParameters</code> or <code>inputExpression</code> can be used in a task. See Using Expressions for details inputExpression object JSONPath expression that defines the input given to the task. Only one of <code>inputParameters</code> or <code>inputExpression</code> can be used in a task. See Using Expressions for details asyncComplete boolean <code>false</code> to mark status COMPLETED upon execution; <code>true</code> to keep the task IN_PROGRESS and wait for an external event to complete it. Defaults to <code>false</code> startDelay number Time in seconds to wait before making the task available to be polled by a worker. Defaults to 0. <p>In addition to these parameters, System Tasks have their own parameters. Check out System Tasks for more information.</p>"},{"location":"documentation/configuration/workflowdef/index.html#failure-workflow","title":"Failure Workflow","text":"<p>The failure workflow gets the original failed workflow\u2019s input along with 3 additional items,</p> <ul> <li><code>workflowId</code> - The id of the failed workflow which triggered the failure workflow.</li> <li><code>reason</code> - A string containing the reason for workflow failure.</li> <li><code>failureStatus</code> - A string status representation of the failed workflow.</li> <li><code>failureTaskId</code> - The id of the failed task of the workflow that triggered the failure workflow.</li> </ul>"},{"location":"documentation/configuration/workflowdef/index.html#timeout-policy","title":"Timeout Policy","text":"<ul> <li>TIME_OUT_WF: Workflow is marked as TIMED_OUT and terminated</li> <li>ALERT_ONLY: Registers a counter (workflow_failure with status tag set to <code>TIMED_OUT</code>)</li> </ul>"},{"location":"documentation/configuration/workflowdef/index.html#workflow-status-listener","title":"Workflow Status Listener","text":"<p>Setting the <code>workflowStatusListenerEnabled</code> field in your Workflow Definition to <code>true</code> enables notifications.</p> <p>To add a custom implementation of the Workflow Status Listener. Refer to this .</p> <p>The listener can be implemented in such a way as to either send a notification to an external system or to send an event on the conductor queue to complete/fail another task in another workflow as described here.</p>"},{"location":"documentation/configuration/workflowdef/index.html#default-input-with-inputtemplate","title":"Default Input with <code>inputTemplate</code>","text":"<ul> <li><code>inputTemplate</code> allows you to define default input values, which can optionally be overridden at runtime (when the workflow is invoked).</li> <li>Example: In your Workflow Definition, you can define your inputTemplate as:</li> </ul> <pre><code>\"inputTemplate\": {\n    \"url\": \"https://some_url:7004\"\n}\n</code></pre> <p>And the default <code>url</code> would be <code>https://some_url:7004</code> if no <code>url</code> was provided as input to your workflow.</p>"},{"location":"documentation/configuration/workflowdef/index.html#using-expressions","title":"Using Expressions","text":"<p>Each executed task is given an input based on the <code>inputParameters</code> template or the <code>inputExpression</code> configured in the task configuration. Only one of <code>inputParameters</code> or <code>inputExpression</code> can be used in a task.</p>"},{"location":"documentation/configuration/workflowdef/index.html#inputparameters","title":"inputParameters","text":"<p><code>inputParameters</code> can use JSONPath expressions to extract values out of the workflow input and other tasks in the workflow.</p> <p>For example, workflows are supplied an <code>input</code> by the client/caller when a new execution is triggered. The workflow <code>input</code> is available via an expression of the form <code>${workflow.input...}</code>. Likewise, the <code>input</code> and <code>output</code> data of a previously executed task can also be extracted using an expression for use in the <code>inputParameters</code> of a subsequent task.</p> <p>Generally, <code>inputParameters</code> can use expressions of the following syntax:</p> <p><code>${SOURCE.input/output.JSONPath}</code></p> Field Description SOURCE Can be either <code>\"workflow\"</code> or the reference name of any task input/output Refers to either the input or output of the source JSONPath JSON path expression to extract JSON fragment from source's input/output <p>JSON Path Support</p> <p>Conductor supports JSONPath specification and uses Java implementation from here.</p> <p>Escaping expressions</p> <p>To escape an expression, prefix it with an extra $ character (ex.: <code>$${workflow.input...}</code>).</p>"},{"location":"documentation/configuration/workflowdef/index.html#inputexpression","title":"inputExpression","text":"<p><code>inputExpression</code> can be used to select an entire object from the workflow input, or the output of another task. The field supports all definite JSONPath expressions.</p> <p>The syntax for mapping values in <code>inputExpression</code> follows the pattern,</p> <p><code>SOURCE.input/output.JSONPath</code></p> <p>NOTE: The <code>inputExpression</code> field does not require the expression to be wrapped in <code>${}</code>.</p> <p>See example below.</p>"},{"location":"documentation/configuration/workflowdef/index.html#examples","title":"Examples","text":""},{"location":"documentation/configuration/workflowdef/index.html#example-1-a-basic-workflow-definition","title":"Example 1 - A Basic Workflow Definition","text":"<p>Assume your business logic is to simply to get some shipping information and then do the shipping. You start by logically partitioning them into two tasks:</p> <ol> <li>shipping_info - The first task takes the provided account number, and outputs an address.</li> <li>shipping_task - The 2nd task takes the address info and generates a shipping label.</li> </ol> <p>We can configure these two tasks in the <code>tasks</code> array of our Workflow Definition. Let's assume that <code>shipping info</code> takes an account number, and returns a name and address.</p> <pre><code>{\n  \"name\": \"mail_a_box\",\n  \"description\": \"shipping Workflow\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"shipping_info\",\n      \"taskReferenceName\": \"shipping_info_ref\",\n      \"inputParameters\": {\n        \"account\": \"${workflow.input.accountNumber}\"\n      },\n      \"type\": \"CUSTOM\"\n    },\n    {\n      \"name\": \"shipping_task\",\n      \"taskReferenceName\": \"shipping_task_ref\",\n      \"inputParameters\": {\n        \"name\": \"${shipping_info_ref.output.name}\",\n        \"streetAddress\": \"${shipping_info_ref.output.streetAddress}\",\n        \"city\": \"${shipping_info_ref.output.city}\",\n        \"state\": \"${shipping_info_ref.output.state}\",\n        \"zipcode\": \"${shipping_info_ref.output.zipcode}\"\n      },\n      \"type\": \"CUSTOM\"\n    }\n  ],\n  \"outputParameters\": {\n    \"trackingNumber\": \"${shipping_task_ref.output.trackinNumber}\"\n  },\n  \"failureWorkflow\": \"shipping_issues\",\n  \"restartable\": true,\n  \"workflowStatusListenerEnabled\": true,\n  \"ownerEmail\": \"conductor@example.com\",\n  \"timeoutPolicy\": \"ALERT_ONLY\",\n  \"timeoutSeconds\": 0,\n  \"variables\": {},\n  \"inputTemplate\": {}\n}\n</code></pre> <p>Upon completion of the 2 tasks, the workflow outputs the tracking number generated in the 2nd task. If the workflow fails, a second workflow named <code>shipping_issues</code> is run.</p>"},{"location":"documentation/configuration/workflowdef/index.html#example-2-task-configuration","title":"Example 2 - Task Configuration","text":"<p>Consider a task <code>http_task</code> with input configured to use input/output parameters from workflow and a task named <code>loc_task</code>.</p> <pre><code>{\n  \"name\": \"encode_workflow\",\n  \"description\": \"Encode movie.\",\n  \"version\": 1,\n  \"inputParameters\": [\n    \"movieId\", \"fileLocation\", \"recipe\"\n  ],\n  \"tasks\": [\n    {\n      \"name\": \"loc_task\",\n      \"taskReferenceName\": \"loc_task_ref\",\n      \"taskType\": \"CUSTOM\",\n      ...\n    },\n    {\n      \"name\": \"http_task\",\n      \"taskReferenceName\": \"http_task_ref\",\n      \"taskType\": \"HTTP\",\n      \"inputParameters\": {\n        \"movieId\": \"${workflow.input.movieId}\",\n        \"url\": \"${workflow.input.fileLocation}\",\n        \"lang\": \"${loc_task.output.languages[0]}\",\n        \"http_request\": {\n          \"method\": \"POST\",\n          \"url\": \"http://example.com/${loc_task.output.fileId}/encode\",\n          \"body\": {\n            \"recipe\": \"${workflow.input.recipe}\",\n            \"params\": {\n              \"width\": 100,\n              \"height\": 100\n            }\n          },\n          \"headers\": {\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/json\"\n          }\n        }\n      }\n    }\n  ],\n  \"ownerEmail\": \"conductor@example.com\",\n  \"variables\": {},\n  \"inputTemplate\": {}\n}\n</code></pre> <p>Consider the following as the workflow input</p> <pre><code>{\n  \"movieId\": \"movie_123\",\n  \"fileLocation\": \"s3://moviebucket/file123\",\n  \"recipe\": \"png\"\n}\n</code></pre> <p>And the output of the loc_task as the following;</p> <pre><code>{\n  \"fileId\": \"file_xxx_yyy_zzz\",\n  \"languages\": [\"en\", \"ja\", \"es\"]\n}\n</code></pre> <p>When scheduling the task, Conductor will merge the values from workflow input and <code>loc_task</code>'s output and create the input to the <code>http_task</code> as follows:</p> <pre><code>{\n  \"movieId\": \"movie_123\",\n  \"url\": \"s3://moviebucket/file123\",\n  \"lang\": \"en\",\n  \"http_request\": {\n    \"method\": \"POST\",\n    \"url\": \"http://example.com/file_xxx_yyy_zzz/encode\",\n    \"body\": {\n      \"recipe\": \"png\",\n      \"params\": {\n        \"width\": 100,\n        \"height\": 100\n      }\n    },\n    \"headers\": {\n      \"Accept\": \"application/json\",\n      \"Content-Type\": \"application/json\"\n    }\n  }\n}\n</code></pre>"},{"location":"documentation/configuration/workflowdef/index.html#example-3-inputexpression","title":"Example 3 - inputExpression","text":"<p>Given the following task configuration:</p> <pre><code>{\n  \"name\": \"loc_task\",\n  \"taskReferenceName\": \"loc_task_ref\",\n  \"taskType\": \"CUSTOM\",\n  \"inputExpression\": {\n    \"expression\": \"workflow.input\",\n    \"type\": \"JSON_PATH\"\n  }\n}\n</code></pre> <p>When the workflow is invoked with the following workflow input</p> <pre><code>{\n  \"movieId\": \"movie_123\",\n  \"fileLocation\": \"s3://moviebucket/file123\",\n  \"recipe\": \"png\"\n}\n</code></pre> <p>When the task <code>loc_task</code> is scheduled, the entire workflow input object will be passed in as the task input:</p> <pre><code>{\n  \"movieId\": \"movie_123\",\n  \"fileLocation\": \"s3://moviebucket/file123\",\n  \"recipe\": \"png\"\n}\n</code></pre>"},{"location":"documentation/metrics/client.html","title":"Client Metrics","text":"<p>When using the Java client, the following metrics are published:</p> Name Purpose Tags task_execution_queue_full Counter to record execution queue has saturated taskType task_poll_error Client error when polling for a task queue taskType, includeRetries, status task_paused Counter for number of times the task has been polled, when the worker has been paused taskType task_execute_error Execution error taskType task_ack_failed Task ack failed taskType task_ack_error Task ack has encountered an exception taskType task_update_error Task status cannot be updated back to server taskType task_poll_counter Incremented each time polling is done taskType task_poll_time Time to poll for a batch of tasks taskType task_execute_time Time to execute a task taskType task_result_size Records output payload size of a task taskType workflow_input_size Records input payload size of a workflow workflowType, workflowVersion external_payload_used Incremented each time external payload storage is used name, operation, payloadType <p>Metrics on client side supplements the one collected from server in identifying the network as well as client side issues.</p>"},{"location":"documentation/metrics/server.html","title":"Server Metrics","text":"<p>Conductor uses spectator to collect the metrics.</p> <ul> <li>To enable conductor serve to publish metrics, add this dependency to your build.gradle.</li> <li>Conductor Server enables you to load additional modules dynamically, this feature can be controlled using this configuration.</li> <li>Create your own AbstractModule that overides configure function and registers the Spectator metrics registry.</li> <li>Initialize the Registry and add it to the global registry via <code>((CompositeRegistry)Spectator.globalRegistry()).add(...)</code>.</li> </ul> <p>The following metrics are published by the server. You can use these metrics to configure alerts for your workflows and tasks.</p> Name Purpose Tags workflow_server_error Rate at which server side error is happening methodName workflow_failure Counter for failing workflows workflowName, status workflow_start_error Counter for failing to start a workflow workflowName workflow_running Counter for no. of running workflows workflowName, version workflow_execution Timer for Workflow completion workflowName, ownerApp task_queue_wait Time spent by a task in queue taskType task_execution Time taken to execute a task taskType, includeRetries, status task_poll Time taken to poll for a task taskType task_poll_count Counter for number of times the task is being polled taskType, domain task_queue_depth Pending tasks queue depth taskType, ownerApp task_rate_limited Current number of tasks being rate limited taskType task_concurrent_execution_limited Current number of tasks being limited by concurrent execution limit taskType task_timeout Counter for timed out tasks taskType task_response_timeout Counter for tasks timedout due to responseTimeout taskType task_update_conflict Counter for task update conflicts. Eg: when the workflow is in terminal state workflowName, taskType, taskStatus, workflowStatus event_queue_messages_processed Counter for number of messages fetched from an event queue queueType, queueName observable_queue_error Counter for number of errors encountered when fetching messages from an event queue queueType event_queue_messages_handled Counter for number of messages executed from an event queue queueType, queueName external_payload_storage_usage Counter for number of times external payload storage was used name, operation, payloadType"},{"location":"documentation/metrics/server.html#collecting-metrics-with-log4j","title":"Collecting metrics with Log4j","text":"<p>One way of collecting metrics is to push them into the logging framework (log4j). Log4j supports various appenders that can print metrics into a console/file or even send them to remote metrics collectors over e.g. syslog channel.</p> <p>Conductor provides optional modules that connect metrics registry with the logging framework. To enable these modules, configure following additional modules property in config.properties:</p> <pre><code>conductor.metrics-logger.enabled = true\nconductor.metrics-logger.reportPeriodSeconds = 15\n</code></pre> <p>This will push all available metrics into log4j every 15 seconds.</p> <p>By default, the metrics will be handled as a regular log message (just printed to console with default log4j.properties). In order to change that, you can use following log4j configuration that prints metrics into a dedicated file:</p> <pre><code>log4j.rootLogger=INFO,console,file\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n\n\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=/app/logs/conductor.log\nlog4j.appender.file.MaxFileSize=10MB\nlog4j.appender.file.MaxBackupIndex=10\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n\n\n# Dedicated file appender for metrics\nlog4j.appender.fileMetrics=org.apache.log4j.RollingFileAppender\nlog4j.appender.fileMetrics.File=/app/logs/metrics.log\nlog4j.appender.fileMetrics.MaxFileSize=10MB\nlog4j.appender.fileMetrics.MaxBackupIndex=10\nlog4j.appender.fileMetrics.layout=org.apache.log4j.PatternLayout\nlog4j.appender.fileMetrics.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n\n\nlog4j.logger.ConductorMetrics=INFO,console,fileMetrics\nlog4j.additivity.ConductorMetrics=false\n</code></pre> <p>This configuration is bundled with conductor-server in file: log4j-file-appender.properties and can be utilized by setting env var:</p> <pre><code>LOG4J_PROP=log4j-file-appender.properties\n</code></pre> <p>This variable is used by startup.sh script.</p>"},{"location":"documentation/metrics/server.html#integration-with-logstash-using-a-log-file","title":"Integration with logstash using a log file","text":"<p>The metrics collected by log4j can be further processed and pushed into a central collector such as ElasticSearch. One way of achieving this is to use: log4j file appender -&gt; logstash -&gt; ElasticSearch.</p> <p>Considering the above setup, you can deploy logstash to consume the contents of /app/logs/metrics.log file, process it and send further to elasticsearch.</p> <p>Following configuration needs to be used in logstash to achieve it:</p> <p>pipeline.yml:</p> <pre><code>- pipeline.id: conductor_metrics\n  path.config: \"/usr/share/logstash/pipeline/logstash_metrics.conf\"\n  pipeline.workers: 2\n</code></pre> <p>logstash_metrics.conf</p> <pre><code>input {\n\n file {\n  path =&gt; [\"/conductor-server-logs/metrics.log\"]\n  codec =&gt; multiline {\n      pattern =&gt; \"^%{TIMESTAMP_ISO8601} \"\n      negate =&gt; true\n      what =&gt; previous\n  }\n }\n}\n\nfilter {\n    kv {\n        field_split =&gt; \", \"\n        include_keys =&gt; [ \"name\", \"type\", \"count\", \"value\" ]\n    }\n    mutate {\n        convert =&gt; {\n          \"count\" =&gt; \"integer\"\n          \"value\" =&gt; \"float\"\n        }\n      }\n}\n\noutput {\n elasticsearch {\n  hosts =&gt; [\"elasticsearch:9200\"]\n }\n}\n</code></pre> <p>Note: In addition to forwarding the metrics into ElasticSearch, logstash will extract following fields from each metric: name, type, count, value and set proper types</p>"},{"location":"documentation/metrics/server.html#integration-with-fluentd-using-a-syslog-channel","title":"Integration with fluentd using a syslog channel","text":"<p>Another example of metrics collection uses: log4j syslog appender -&gt; fluentd -&gt; prometheus.</p> <p>In this case, a specific log4j properties file needs to be used so that metrics are pushed into a syslog channel:</p> <pre><code>    log4j.rootLogger=INFO,console,file\n\n    log4j.appender.console=org.apache.log4j.ConsoleAppender\n    log4j.appender.console.layout=org.apache.log4j.PatternLayout\n    log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n\n\n    log4j.appender.file=org.apache.log4j.RollingFileAppender\n    log4j.appender.file.File=/app/logs/conductor.log\n    log4j.appender.file.MaxFileSize=10MB\n    log4j.appender.file.MaxBackupIndex=10\n    log4j.appender.file.layout=org.apache.log4j.PatternLayout\n    log4j.appender.file.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n\n\n    # Syslog based appender streaming metrics into fluentd\n    log4j.appender.server=org.apache.log4j.net.SyslogAppender\n    log4j.appender.server.syslogHost=fluentd:5170\n    log4j.appender.server.facility=LOCAL1\n    log4j.appender.server.layout=org.apache.log4j.PatternLayout\n    log4j.appender.server.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n\n\n    log4j.logger.ConductorMetrics=INFO,console,server\n    log4j.additivity.ConductorMetrics=false\n</code></pre> <p>And on the fluentd side you need following configuration:</p> <pre><code>    &lt;source&gt;\n      @type prometheus\n    &lt;/source&gt;\n\n    &lt;source&gt;\n      @type syslog\n      port 5170\n      bind 0.0.0.0\n      tag conductor\n        &lt;parse&gt;\n         ; only allow TIMER metrics of workflow execution and extract tenant ID\n          @type regexp\n          expression /^.*type=TIMER, name=workflow_execution.class-WorkflowMonitor.+workflowName-(?&lt;tenant&gt;.*)_(?&lt;workflow&gt;.+), count=(?&lt;count&gt;\\d+), min=(?&lt;min&gt;[\\d.]+), max=(?&lt;max&gt;[\\d.]+), mean=(?&lt;mean&gt;[\\d.]+).*$/\n          types count:integer,min:float,max:float,mean:float\n        &lt;/parse&gt;\n    &lt;/source&gt;\n\n    &lt;filter conductor.local1.info&gt;\n        @type prometheus\n        &lt;metric&gt;\n          name conductor_workflow_count\n          type gauge\n          desc The total number of executed workflows\n          key count\n          &lt;labels&gt;\n            workflow ${workflow}\n            tenant ${tenant}\n            user ${email}\n          &lt;/labels&gt;\n        &lt;/metric&gt;\n        &lt;metric&gt;\n          name conductor_workflow_max_duration\n          type gauge\n          desc Max duration in millis for a workflow\n          key max\n          &lt;labels&gt;\n            workflow ${workflow}\n            tenant ${tenant}\n            user ${email}\n          &lt;/labels&gt;\n        &lt;/metric&gt;\n        &lt;metric&gt;\n          name conductor_workflow_mean_duration\n          type gauge\n          desc Mean duration in millis for a workflow\n          key mean\n          &lt;labels&gt;\n            workflow ${workflow}\n            tenant ${tenant}\n            user ${email}\n          &lt;/labels&gt;\n        &lt;/metric&gt;\n    &lt;/filter&gt;\n\n    &lt;match **&gt;\n      @type stdout\n    &lt;/match&gt;\n</code></pre> <p>With above configuration, fluentd will: - Listen to raw metrics on 0.0.0.0:5170 - Collect only workflow_execution TIMER metrics - Process the raw metrics and expose 3 prometheus specific metrics - Expose prometheus metrics on http://fluentd:24231/metrics </p>"},{"location":"documentation/metrics/server.html#collecting-metrics-with-prometheus","title":"Collecting metrics with Prometheus","text":"<p>Another way to collect metrics is using Prometheus client to push them to Prometheus server.</p> <p>Conductor provides optional modules that connect metrics registry with Prometheus. To enable these modules, configure following additional module property in config.properties:</p> <pre><code>conductor.metrics-prometheus.enabled = true\n</code></pre> <p>This will simply push these metrics via Prometheus collector. However, you need to configure your own Prometheus collector and expose the metrics via an endpoint.</p>"},{"location":"getting-started/basic-concepts.html","title":"Basic Concepts","text":""},{"location":"getting-started/basic-concepts.html#definitions-aka-metadata-or-blueprints","title":"Definitions (aka Metadata or Blueprints)","text":"<p>Conductor definitions are like class definitions in OOP paradigm, or templates. You define this once, and use for each workflow execution. Definitions to Executions have 1:N relationship.</p>"},{"location":"getting-started/basic-concepts.html#workflow-definition","title":"Workflow Definition","text":"<p>A Workflow Definition is the container that describes your process. It contains tasks, sub-workflows, with inputs and outputs connected to each other in order to achieve the desired action. The tasks are either System Operators (e.g. fork, join, wait, switch, conditional, etc), System Tasks (e.g. HTTP, Inline, Human) or Custom Tasks (e.g. encode a file). The system operators and tasks are executed by the Swift Conductor server. The custom tasks are executed by a dedicated Worker on a remote machine.</p> <p>Detailed description</p>"},{"location":"getting-started/basic-concepts.html#task-definition","title":"Task Definition","text":"<p>Task definitions help define task level parameters like inputs and outputs, timeouts, retries etc.</p> <ul> <li>All tasks need to be registered before they can be used by active workflows.</li> <li>A task can be re-used within multiple workflows.</li> </ul> <p>Detailed description</p>"},{"location":"getting-started/basic-concepts.html#task-types","title":"Task Types","text":"<p>Tasks are the building blocks of Workflow. There must be at least one task in a Workflow. Tasks can be categorized into two types: </p> <ul> <li>System tasks - executed by Conductor server.</li> <li>Custom tasks - executed by your own workers.</li> </ul>"},{"location":"getting-started/basic-concepts.html#system-tasks","title":"System Tasks","text":"<p>System tasks are executed within the JVM of the Conductor server and managed by Conductor for its execution and scalability.</p> <p>See Systems tasks for list of available Task types, and instructions for using them.</p> <p>Note</p> <p>Conductor provides an API to create user defined tasks that are executed in the same JVM as the engine. See WorkflowSystemTask interface for details.</p>"},{"location":"getting-started/basic-concepts.html#custom-tasks","title":"Custom Tasks","text":"<p>Custom tasks are implemented by your application(s) and run in a separate environment from Swift Conductor. The custom tasks can be implemented in any language.  These tasks talk to Conductor server via REST/gRPC to poll for tasks and update its status after execution.</p> <p>Custom tasks are identified by task type CUSTOM in the blueprint.</p>"},{"location":"getting-started/client.html","title":"Using the Client","text":"<p>CUSTOM Conductor tasks that are executed by remote workers which communicate over HTTP endpoints or gRPC to poll for the tasks, perform the work, and update the status of the execution.</p>"},{"location":"getting-started/client.html#client-apis","title":"Client APIs","text":"<ul> <li>Python</li> <li>TypeScript</li> <li>.NET</li> <li>Java</li> <li>Go</li> </ul>"},{"location":"getting-started/start-workflow.html","title":"Starting a Workflow","text":""},{"location":"getting-started/start-workflow.html#start-workflow-endpoint","title":"Start Workflow Endpoint","text":"<p>When starting a Workflow execution with a registered definition, <code>/workflow</code> accepts following parameters:</p> Field Description Notes name Name of the Workflow. MUST be registered with Conductor before starting workflow version Workflow version defaults to latest available version input JSON object with key value params, that can be used by downstream tasks See Wiring Inputs and Outputs for details correlationId Unique Id that correlates multiple Workflow executions optional taskToDomain See Task Domains for more information. optional workflowDef An adhoc Workflow Definition to run, without registering. See Dynamic Workflows. optional externalInputPayloadStoragePath This is taken care of by Java client. See External Payload Storage for more info. optional priority Priority level for the tasks within this workflow execution. Possible values are between 0 - 99. optional <p>Example:</p> <p>Send a <code>POST</code> request to <code>/workflow</code> with payload like: <pre><code>{\n    \"name\": \"encode_and_deploy\",\n    \"version\": 1,\n    \"correlationId\": \"my_unique_correlation_id\",\n    \"input\": {\n        \"param1\": \"value1\",\n        \"param2\": \"value2\"\n    }\n}\n</code></pre></p>"},{"location":"getting-started/start-workflow.html#dynamic-workflows","title":"Dynamic Workflows","text":"<p>If the need arises to run a one-time workflow, and it doesn't make sense to register Task and Workflow definitions in Conductor Server, as it could change dynamically for each execution, dynamic workflow executions can be used.</p> <p>This enables you to provide a workflow definition embedded with the required task definitions to the Start Workflow Request in the <code>workflowDef</code> parameter, avoiding the need to register the blueprints before execution.</p> <p>Example:</p> <p>Send a <code>POST</code> request to <code>/workflow</code> with payload like: <pre><code>{\n  \"name\": \"my_adhoc_unregistered_workflow\",\n  \"workflowDef\": {\n    \"ownerApp\": \"my_owner_app\",\n    \"ownerEmail\": \"my_owner_email@test.com\",\n    \"createdBy\": \"my_username\",\n    \"name\": \"my_adhoc_unregistered_workflow\",\n    \"description\": \"Test Workflow setup\",\n    \"version\": 1,\n    \"tasks\": [\n        {\n            \"name\": \"fetch_data\",\n            \"type\": \"HTTP\",\n            \"taskReferenceName\": \"fetch_data\",\n            \"inputParameters\": {\n              \"http_request\": {\n                \"connectionTimeOut\": \"3600\",\n                \"readTimeOut\": \"3600\",\n                \"uri\": \"${workflow.input.uri}\",\n                \"method\": \"GET\",\n                \"accept\": \"application/json\",\n                \"content-Type\": \"application/json\",\n                \"headers\": {\n                }\n              }\n            },\n            \"taskDefinition\": {\n                \"name\": \"fetch_data\",\n                \"retryCount\": 0,\n                \"timeoutSeconds\": 3600,\n                \"timeoutPolicy\": \"TIME_OUT_WF\",\n                \"retryLogic\": \"FIXED\",\n                \"retryDelaySeconds\": 0,\n                \"responseTimeoutSeconds\": 3000\n            }\n        }\n    ],\n    \"outputParameters\": {\n    }\n  },\n  \"input\": {\n    \"uri\": \"http://www.google.com\"\n  }\n}\n</code></pre></p> <p>Note</p> <p>If the <code>taskDefinition</code> is defined with Metadata API, it doesn't have to be added in above dynamic workflow definition.</p>"},{"location":"getting-started/steps.html","title":"High Level Steps","text":"<p>Steps required for a new workflow to be registered and get executed</p> <ol> <li>Define task definitions used by the workflow. </li> <li>Create the workflow definition</li> <li>Create task worker(s) that polls for scheduled tasks at regular interval</li> </ol>"},{"location":"getting-started/steps.html#trigger-workflow-execution","title":"Trigger Workflow Execution","text":"<pre><code>POST /workflow/{name}\n{\n    ... //json payload as workflow input\n}\n</code></pre>"},{"location":"getting-started/steps.html#polling-for-a-task","title":"Polling for a task","text":"<pre><code>GET /tasks/poll/batch/{taskType}\n</code></pre>"},{"location":"getting-started/steps.html#update-task-status","title":"Update task status","text":"<pre><code>POST /tasks\n{\n    \"outputData\": {\n        \"encodeResult\":\"success\",\n        \"location\": \"http://cdn.example.com/file/location.png\"\n        //any task specific output\n     },\n     \"status\": \"COMPLETED\"\n}\n</code></pre>"},{"location":"getting-started/why-conductor.html","title":"Why Conductor?","text":""},{"location":"getting-started/why-conductor.html#conductor-was-built-to-help-teams-orchestrate-microservices-based-process-flows","title":"Conductor was built to help teams orchestrate microservices based process flows.","text":""},{"location":"getting-started/why-conductor.html#features","title":"Features","text":"<ul> <li>A distributed server ecosystem, which stores workflow state information efficiently.</li> <li>Allow creation of process / business flows in which each individual task can be implemented by the same / different microservices.</li> <li>A DAG (Directed Acyclic Graph) based workflow definition.</li> <li>Workflow definitions are decoupled from the service implementations.</li> <li>Provide visibility and traceability into these process flows.</li> <li>Simple interface to connect workers, which execute the tasks in workflows.</li> <li>Workers are language agnostic, allowing each microservice to be written in the language most suited for the service.</li> <li>Full operational control over workflows with the ability to pause, resume, restart, retry and terminate.</li> <li>Allow greater reuse of existing microservices providing an easier path for onboarding.</li> <li>User interface to visualize, replay and search the process flows.</li> <li>Ability to scale to millions of concurrently running process flows.</li> <li>Backed by a queuing service abstracted from the clients.</li> <li>Be able to operate on HTTP or other transports e.g. gRPC.</li> <li>Event handlers to control workflows via external actions.</li> <li>Client implementations in Python, Java, C#, Go and other languages.</li> <li>Various configurable properties with sensible defaults to fine tune workflow and task executions like rate limiting, concurrent execution limits etc.</li> </ul>"},{"location":"getting-started/why-conductor.html#why-not-peer-to-peer-choreography","title":"Why not peer to peer choreography?","text":"<p>With peer to peer task choreography, we found it was harder to scale with growing business needs and complexities. Pub/sub model worked for simplest of the flows, but quickly highlighted some of the issues associated with the approach:</p> <ul> <li>Process flows are \u201cembedded\u201d within the code of multiple application.</li> <li>Often, there is tight coupling and assumptions around input/output, SLAs etc, making it harder to adapt to changing needs.</li> <li>Almost no way to systematically answer \u201cHow much are we done with process X\u201d?</li> </ul>"},{"location":"getting-started/running/docker.html","title":"Running Conductor using Docker","text":"<p>In this article we will explore how you can set up Conductor on your local machine using Docker compose.</p>"},{"location":"getting-started/running/docker.html#prerequisites","title":"Prerequisites","text":"<ol> <li>Docker: https://docs.docker.com/get-docker/</li> <li>Recommended host with CPU and RAM to be able to run multiple docker containers (at-least 16GB RAM)</li> </ol>"},{"location":"getting-started/running/docker.html#swift-conductor-core","title":"Swift Conductor Core","text":""},{"location":"getting-started/running/docker.html#clone","title":"Clone","text":"<pre><code>git clone https://github.com/swift-conductor/conductor.git\n</code></pre>"},{"location":"getting-started/running/docker.html#build","title":"Build","text":"<pre><code>cd conductor/docker\n\ndocker-compose build\n</code></pre>"},{"location":"getting-started/running/docker.html#run","title":"Run","text":""},{"location":"getting-started/running/docker.html#use-docker-compose-to-bring-up-the-local-server","title":"Use <code>docker-compose</code> to bring up the local server:","text":"<pre><code>docker-compose up --detach\n</code></pre> <p>The docker compose will bring up the following containers:</p> <ol> <li>Swift Conductor Core Server</li> <li>Redis for database and task queue</li> <li>Elasticsearch v6 for searching workflows</li> </ol> <p>You can open the server URL in your browser to verify that they are running correctly:</p> <ul> <li>Server: http://localhost:8080</li> </ul>"},{"location":"getting-started/running/docker.html#swift-conductor-ce","title":"Swift Conductor CE","text":"<p>CE == Community Edition</p>"},{"location":"getting-started/running/docker.html#clone_1","title":"Clone","text":"<pre><code>git clone https://github.com/swift-conductor/conductor-community.git\n</code></pre>"},{"location":"getting-started/running/docker.html#build-docker-image","title":"Build Docker image","text":"<pre><code>cd conductor-community/docker\n\ndocker-compose --file docker-compose-redis.yaml build\n</code></pre>"},{"location":"getting-started/running/docker.html#run_1","title":"Run","text":""},{"location":"getting-started/running/docker.html#use-docker-compose-to-bring-up-the-local-server_1","title":"Use <code>docker-compose</code> to bring up the local server:","text":"Docker Compose Description docker-compose-redis.yaml Redis + Elasticsearch 7, Redis database, Redis queue, ElasticSearch index docker-compose-postgres.yaml Postgres + Elasticsearch 7, Postgress database, Redis queue, ElasticSearch index docker-compose-mysql.yaml Mysql + Elasticsearch 7, MySql database, Redis queue, ElasticSearch index <p>For example this will start the server instance backed by a Redis database, Redis queue, and ElasticSearch index.</p> <pre><code>docker-compose --file docker-compose-redis.yaml up --detach\n</code></pre> <p>The docker compose will bring up the following containers:</p> <ol> <li>Swift Conductor CE Server </li> <li>Swift Conductor UI</li> <li>Redis for database and task queue</li> <li>Elasticsearch v7 for searching workflows</li> </ol> <p>You can open the Server and UI URLs in your browser to verify that they are running correctly:</p> <ul> <li>Server: http://localhost:8080</li> <li>UI: http://localhost:5000</li> </ul>"},{"location":"getting-started/running/docker.html#monitoring-with-prometheus","title":"Monitoring with Prometheus","text":"<p>Start Prometheus only:</p> <pre><code>docker-compose --file docker-compose-prometheus.yaml up --detach\n</code></pre> <p>or</p> <p>Start Prometheus and Grafana (optional):</p> <pre><code>docker-compose --file docker-compose-prometheus-grafana.yaml up --detach\n</code></pre> <p>Open Prometheus - http://localhost:9090 and Grafana - http://localhost:3000 in your browser (use admin / admin to login to Grafana).</p>"},{"location":"getting-started/running/elastic_search_6.html","title":"Elastic Search 6","text":"<p>Install Podman:</p> <pre><code>brew install podman\n</code></pre> <p>Start Podman machine:</p> <pre><code>podman machine init --cpus 2 --disk-size 100 --memory 4096\npodman machine set --rootful\n\npodman machine start\n</code></pre> <p>Start Elastic Search 6 container on port 9200:</p> <pre><code>podman pull docker.io/elasticsearch:6.8.23\npodman run -d --name es6-conductor -p 9200:9200 -p 9300:9300 -e \"ES_JAVA_OPTS=-Xms512m -Xmx1024m\" -e \"xpack.security.enabled=false\" -e \"discovery.type=single-node\" elasticsearch:6.8.23\n</code></pre>"},{"location":"getting-started/running/elastic_search_7.html","title":"Elastic Search 7","text":"<p>Install Podman:</p> <pre><code>brew install podman\n</code></pre> <p>Start Podman machine:</p> <pre><code>podman machine init --cpus 2 --disk-size 100 --memory 4096\npodman machine set --rootful\n\npodman machine start\n</code></pre> <p>Start Elastic Search 7 container on port 9300:</p> <pre><code>podman pull docker.io/elasticsearch:7.17.16\npodman run -d --name es7-conductor -p 9200:9200 -p 9300:9300 -e \"ES_JAVA_OPTS=-Xms512m -Xmx1024m\" -e \"xpack.security.enabled=false\" -e \"discovery.type=single-node\" elasticsearch:7.17.16\n</code></pre>"},{"location":"getting-started/running/redis.html","title":"Redis","text":"<p>Install Podman:</p> <pre><code>brew install podman\n</code></pre> <p>Start Podman machine:</p> <pre><code>podman machine init --cpus 2 --disk-size 100 --memory 4096\npodman machine set --rootful\n\npodman machine start\n</code></pre> <p>Start Redis container:</p> <pre><code>podman pull docker.io/redis\npodman run -d --name redis-conductor -p 6379:6379 redis\n</code></pre>"},{"location":"getting-started/running/source.html","title":"Building Conductor From Source","text":""},{"location":"getting-started/running/source.html#prerequisites","title":"Prerequisites","text":"<ol> <li>JDK 17 or greater</li> <li>Podman to in order to run Redis and ElasticSearch.</li> <li>Node for building and running UI. Instructions at https://nodejs.org.</li> <li>Npm for building and running UI. Instructions at https://www.npmjs.com/.</li> <li>Redis instance</li> <li>Elastic Search 6 instance</li> <li>Docker if you want to run server tests (Optional). </li> </ol>"},{"location":"getting-started/running/source.html#build-the-server","title":"Build the server","text":"<p>Clone the Swift Conductor code from the conductor repo.</p> <pre><code>git clone https://github.com/swift-conductor/conductor.git\n</code></pre>"},{"location":"getting-started/running/source.html#run","title":"Run","text":"<pre><code>./gradlew server\n</code></pre> <p>Navigate to the Swagger API docs:</p> <p>http://localhost:8080/swagger-ui/index.html</p> <p></p> <p>NOTE for Mac with an Apple Silicon users: If you are using a new Mac with an Apple Silicon Chip, you must make a small change to <code>conductor/grpc/build.gradle</code> - adding \"osx-x86_64\" to two lines:</p> <pre><code>protobuf {\n    protoc {\n        artifact = \"com.google.protobuf:protoc:${revProtoBuf}:osx-x86_64\"\n    }\n    plugins {\n        grpc {\n            artifact = \"io.grpc:protoc-gen-grpc-java:${revGrpc}:osx-x86_64\"\n        }\n    }\n...\n} \n</code></pre> <p>You also need to install Rosetta:  </p> <pre><code>softwareupdate --install-rosetta\n</code></pre>"},{"location":"getting-started/running/source.html#build-and-run-the-ui","title":"Build and run the UI","text":"<p>Clone Swift Conductor UI code from the conductor-ui repo.</p> <pre><code>git clone https://github.com/swift-conductor/conductor-ui\n</code></pre>"},{"location":"getting-started/running/source.html#install-packages","title":"Install packages","text":"<p>You need Node 14 and <code>npm</code> installed. After that run <code>npm install</code> to install all packages:</p> <pre><code>npm install\n</code></pre>"},{"location":"getting-started/running/source.html#run_1","title":"Run","text":"<pre><code>npm run start\n</code></pre> <p>Open http://localhost:5000 in your browser. </p> <p></p> <p>If you require compiled assets to host on a production web server, the project can be built with the command <code>npm run build</code>.</p>"},{"location":"reference/api/index.html","title":"API Specification","text":"<p>See the following sections for API endpoint documentation. </p> <ul> <li>Metadata API</li> <li>Start Workflow API</li> <li>Workflow API</li> <li>Task API</li> </ul> <p>Task Domains can be used to address tasks to specific pools of workers at runtime.</p>"},{"location":"reference/api/index.html#swagger-ui","title":"Swagger UI","text":"<p>As an alternative resource, the Swagger UI will always have the definitive interface description.</p>"},{"location":"reference/api/metadata.html","title":"Metadata API","text":""},{"location":"reference/api/metadata.html#workflow-metadata","title":"Workflow Metadata","text":"Endpoint Description Input <code>GET /api/metadata/workflow</code> Get all the workflow definitions n/a <code>POST /api/metadata/workflow</code> Register new workflow Workflow Definition <code>PUT /api/metadata/workflow</code> Register/Update new workflows List of Workflow Definition <code>GET /api/metadata/workflow/{name}?version=</code> Get the workflow definitions workflow name, version (optional)"},{"location":"reference/api/metadata.html#task-metadata","title":"Task Metadata","text":"Endpoint Description Input <code>GET /api/metadata/taskdefs</code> Get all the task definitions n/a <code>GET /api/metadata/taskdefs/{taskType}</code> Retrieve task definition Task Name <code>POST /api/metadata/taskdefs</code> Register new task definitions List of Task Definitions <code>PUT /api/metadata/taskdefs</code> Update a task definition A Task Definition <code>DELETE /api/metadata/taskdefs/{taskType}</code> Delete a task definition Task Name"},{"location":"reference/api/startworkflow.html","title":"Start Workflow API","text":""},{"location":"reference/api/startworkflow.html#api-parameters","title":"API Parameters","text":"<p>When starting a Workflow execution with a registered definition, <code>/api/workflow</code> accepts following parameters in the <code>POST</code> payload:</p> Field Description Notes name Name of the Workflow. MUST be registered with Conductor before starting workflow version Workflow version defaults to latest available version input JSON object with key value params, that can be used by downstream tasks See Wiring Inputs and Outputs for details correlationId Unique Id that correlates multiple Workflow executions optional taskToDomain See Task Domains for more information. optional workflowDef An adhoc Workflow Definition to run, without registering. See Dynamic Workflows. optional externalInputPayloadStoragePath This is taken care of by Java client. See External Payload Storage for more info. optional priority Priority level for the tasks within this workflow execution. Possible values are between 0 - 99. optional"},{"location":"reference/api/startworkflow.html#output","title":"Output","text":"<p>On success, this API returns the ID of the workflow.</p>"},{"location":"reference/api/startworkflow.html#basic-example","title":"Basic Example","text":"<p><code>POST http://localhost:8080/api/workflow</code> with payload body:</p> <pre><code>{\n  \"name\": \"myWorkflow\", // Name of the workflow\n  \"version\": 1, // Version\n  \"correlationId\": \"corr1\", // Correlation Id\n  \"priority\": 1, // Priority\n    \"input\": { // Input Value Map\n      \"param1\": \"value1\",\n      \"param2\": \"value2\"\n    },\n  \"taskToDomain\": {\n    // Task to domain map\n  }\n}\n</code></pre>"},{"location":"reference/api/startworkflow.html#dynamic-workflows","title":"Dynamic Workflows","text":"<p>If the need arises to run a one-time workflow, and it doesn't make sense to register Task and Workflow definitions in Conductor Server, as it could change dynamically for each execution, dynamic workflow executions can be used.</p> <p>This enables you to provide a workflow definition embedded with the required task definitions to the Start Workflow Request in the <code>workflowDef</code> parameter, avoiding the need to register the blueprints before execution.</p> <p>Example:</p> <p>Send a <code>POST</code> request to <code>/api/workflow</code> with payload like: <pre><code>{\n  \"name\": \"my_adhoc_unregistered_workflow\",\n  \"workflowDef\": {\n    \"ownerApp\": \"my_owner_app\",\n    \"ownerEmail\": \"my_owner_email@test.com\",\n    \"createdBy\": \"my_username\",\n    \"name\": \"my_adhoc_unregistered_workflow\",\n    \"description\": \"Test Workflow setup\",\n    \"version\": 1,\n    \"tasks\": [\n        {\n            \"name\": \"fetch_data\",\n            \"type\": \"HTTP\",\n            \"taskReferenceName\": \"fetch_data\",\n            \"inputParameters\": {\n              \"http_request\": {\n                \"connectionTimeOut\": \"3600\",\n                \"readTimeOut\": \"3600\",\n                \"uri\": \"${workflow.input.uri}\",\n                \"method\": \"GET\",\n                \"accept\": \"application/json\",\n                \"content-Type\": \"application/json\",\n                \"headers\": {\n                }\n              }\n            },\n            \"taskDefinition\": {\n                \"name\": \"fetch_data\",\n                \"retryCount\": 0,\n                \"timeoutSeconds\": 3600,\n                \"timeoutPolicy\": \"TIME_OUT_WF\",\n                \"retryLogic\": \"FIXED\",\n                \"retryDelaySeconds\": 0,\n                \"responseTimeoutSeconds\": 3000\n            }\n        }\n    ],\n    \"outputParameters\": {\n    }\n  },\n  \"input\": {\n    \"uri\": \"http://www.google.com\"\n  }\n}\n</code></pre></p> <p>Note</p> <p>If the <code>taskDefinition</code> is defined with Metadata API, it doesn't have to be added in above dynamic workflow definition.</p>"},{"location":"reference/api/task.html","title":"Task API","text":""},{"location":"reference/api/task.html#manage-tasks","title":"Manage Tasks","text":"Endpoint Description <code>GET /api/tasks/{taskId}</code> Get task details. <code>GET /api/tasks/queue/all</code> List the pending task sizes. <code>GET /api/tasks/queue/all/verbose</code> Same as above, includes the size per shard <code>GET /api/tasks/queue/sizes?taskType=&amp;taskType=&amp;taskType</code> Return the size of pending tasks for given task types"},{"location":"reference/api/task.html#polling-ack-and-update-task","title":"Polling, Ack and Update Task","text":"<p>These endpoints are used by the worker to poll for task, send ack (after polling) and finally updating the task result. They typically should not be invoked manually.</p> Endpoint Description <code>GET /api/tasks/poll/{taskType}?workerid=&amp;domain=</code> Poll for a task. <code>workerid</code> identifies the worker that polled for the job and <code>domain</code> allows the poller to poll for a task in a specific domain <code>GET /api/tasks/poll/batch/{taskType}?count=&amp;timeout=&amp;workerid=&amp;domain</code> Poll for a task in a batch specified by <code>count</code>.  This is a long poll and the connection will wait until <code>timeout</code> or if there is at-least 1 item available, whichever comes first.<code>workerid</code> identifies the worker that polled for the job and <code>domain</code> allows the poller to poll for a task in a specific domain <code>POST /api/tasks</code> Update the result of task execution.  See the schema below."},{"location":"reference/api/task.html#schema-for-updating-task-result","title":"Schema for updating Task Result","text":"<pre><code>{\n    \"workflowInstanceId\": \"Workflow Instance Id\",\n    \"taskId\": \"ID of the task to be updated\",\n    \"reasonForIncompletion\" : \"If failed, reason for failure\",\n    \"callbackAfterSeconds\": 0,\n    \"status\": \"IN_PROGRESS|FAILED|COMPLETED\",\n    \"outputData\": {\n        //JSON document representing Task execution output     \n    }\n\n}\n</code></pre> <p>Acknowledging tasks after poll</p> <p>If the worker fails to ack the task after polling, the task is re-queued and put back in queue and is made available during subsequent poll.</p>"},{"location":"reference/api/workflow.html","title":"Workflow API","text":""},{"location":"reference/api/workflow.html#retrieve-workflows","title":"Retrieve Workflows","text":"Endpoint Description <code>GET /api/workflow/{workflowId}?includeTasks=true                               | false</code> Get Workflow State by workflow Id.  If includeTasks is set, then also includes all the tasks executed and scheduled. <code>GET /api/workflow/running/{name}</code> Get all the running workflows of a given type <code>GET /api/workflow/running/{name}/correlated/{correlationId}?includeClosed=true | false&amp;includeTasks=true                       |false</code> Get all the running workflows filtered by correlation Id.  If includeClosed is set, also includes workflows that have completed running. <code>GET /api/workflow/search</code> Search for workflows.  See Below."},{"location":"reference/api/workflow.html#workflow-search","title":"Workflow Search","text":"<p>Conductor uses Elasticsearch for indexing workflow execution and is used by search APIs.</p> <p><code>GET /api/workflow/search?start=&amp;size=&amp;sort=&amp;freeText=&amp;query=</code></p> Parameter Description start Page number.  Defaults to 0 size Number of results to return sort Sorting.  Format is: <code>ASC:&lt;fieldname&gt;</code> or <code>DESC:&lt;fieldname&gt;</code> to sort in ascending or descending order by a field freeText Elasticsearch supported query. e.g. workflowType:\"name_of_workflow\" query SQL like where clause.  e.g. workflowType = 'name_of_workflow'.  Optional if freeText is provided."},{"location":"reference/api/workflow.html#output","title":"Output","text":"<p>Search result as described below: <pre><code>{\n  \"totalHits\": 0,\n  \"results\": [\n    {\n      \"workflowType\": \"string\",\n      \"version\": 0,\n      \"workflowId\": \"string\",\n      \"correlationId\": \"string\",\n      \"startTime\": \"string\",\n      \"updateTime\": \"string\",\n      \"endTime\": \"string\",\n      \"status\": \"RUNNING\",\n      \"input\": \"string\",\n      \"output\": \"string\",\n      \"reasonForIncompletion\": \"string\",\n      \"executionTime\": 0,\n      \"event\": \"string\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"reference/api/workflow.html#manage-workflows","title":"Manage Workflows","text":"Endpoint Description <code>PUT /api/workflow/{workflowId}/pause</code> Pause.  No further tasks will be scheduled until resumed.  Currently running tasks are not paused. <code>PUT /api/workflow/{workflowId}/resume</code> Resume normal operations after a pause. <code>POST /api/workflow/{workflowId}/rerun</code> See Below. <code>POST /api/workflow/{workflowId}/restart</code> Restart workflow execution from the start.  Current execution history is wiped out. <code>POST /api/workflow/{workflowId}/retry</code> Retry the last failed task. <code>PUT /api/workflow/{workflowId}/skiptask/{taskReferenceName}</code> See below. <code>DELETE /api/workflow/{workflowId}</code> Terminates the running workflow. <code>DELETE /api/workflow/{workflowId}/remove</code> Deletes the workflow from system.  Use with caution."},{"location":"reference/api/workflow.html#rerun","title":"Rerun","text":"<p>Re-runs a completed workflow from a specific task. </p> <p><code>POST /api/workflow/{workflowId}/rerun</code></p> <pre><code>{\n  \"reRunFromWorkflowId\": \"string\",\n  \"workflowInput\": {},\n  \"reRunFromTaskId\": \"string\",\n  \"taskInput\": {}\n}\n</code></pre>"},{"location":"reference/api/workflow.html#skip-task","title":"Skip Task","text":"<p>Skips a task execution (specified as <code>taskReferenceName</code> parameter) in a running workflow and continues forward. Optionally updating task's input and output as specified in the payload. <code>PUT /api/workflow/{workflowId}/skiptask/{taskReferenceName}?workflowId=&amp;taskReferenceName=</code> <pre><code>{\n  \"taskInput\": {},\n  \"taskOutput\": {}\n}\n</code></pre></p>"},{"location":"reference/operators/index.html","title":"Operators","text":"<p>Operators are built-in primitives in Conductor that allow you to define the control flow in the workflow. Operators are similar to programming constructs such as for loops, decisions, etc. Conductor has support for most of the programing primitives allowing you to define the most advanced workflows.</p> <p>Conductor supports the following programming language constructs: </p> Language Construct Conductor Operator Do-While or For Loops Do While Task Function Pointer Dynamic Task Dynamic Parallel execution Dynamic Fork Task Static Parallel execution Fork Task Map Join Task Subroutine / Fork Process Sub Workflow Task Switch/if..then...else Switch Task Exit Terminate Task Global Variables Variable Task Wait Wait Task"},{"location":"reference/operators/do-while-task.html","title":"Do-While","text":"<pre><code>\"type\" : \"DO_WHILE\"\n</code></pre> <p>The <code>DO_WHILE</code> task sequentially executes a list of tasks as long as a given condition is true.  The list of tasks is executed first, before the condition is checked (the first iteration will always execute).</p> <p>When scheduled, each task of this loop will see its <code>taskReferenceName</code> concatenated with __i, with i being the iteration number, starting at 1. Warning: taskReferenceName containing arithmetic operators must not be used.</p> <p>Each task output is stored as part of the DO_WHILE task, indexed by the iteration value (see example below), allowing the condition to reference the output of a task for a specific iteration (eg. $.LoopTask['iteration]['first_task'])</p> <p>The DO_WHILE task is set to <code>FAILED</code> as soon as one of the loopOver fails. In such case retry, iteration starts from 1.</p>"},{"location":"reference/operators/do-while-task.html#limitations","title":"Limitations","text":"<ul> <li>Domain or isolation group execution is unsupported; </li> <li>Nested DO_WHILE is unsupported, however, DO_WHILE task supports SUB_WORKFLOW as loopOver task, so we can achieve similar functionality.</li> <li>Since loopover tasks will be executed in loop inside scope of parent do while task, crossing branching outside of DO_WHILE task is not respected.</li> </ul> <p>Branching inside loopOver task is supported.</p>"},{"location":"reference/operators/do-while-task.html#configuration","title":"Configuration","text":"<p>The following fields must be specified at the top level of the task configuration.</p> name type description loopCondition String Condition to be evaluated after every iteration. This is a Javascript expression, evaluated using the Nashorn engine. If an exception occurs during evaluation, the DO_WHILE task is set to FAILED_WITH_TERMINAL_ERROR. loopOver List[Task] List of tasks that needs to be executed as long as the condition is true."},{"location":"reference/operators/do-while-task.html#output","title":"Output","text":"name type description iteration Integer Iteration number: the current one while executing; the final one once the loop is finished <code>i</code> Map[String, Any] Iteration number as a string, mapped to the task references names and their output. * Any Any state can be stored here if the <code>loopCondition</code> does so. For example <code>storage</code> will exist if <code>loopCondition</code> is <code>if ($.LoopTask['iteration'] &lt;= 10) {$.LoopTask.storage = 3; true } else {false}</code>"},{"location":"reference/operators/do-while-task.html#examples","title":"Examples","text":""},{"location":"reference/operators/do-while-task.html#basic-example","title":"Basic Example","text":"<p>The following definition: <pre><code>{\n    \"name\": \"Loop Task\",\n    \"taskReferenceName\": \"LoopTask\",\n    \"type\": \"DO_WHILE\",\n    \"inputParameters\": {\n      \"value\": \"${workflow.input.value}\"\n    },\n    \"loopCondition\": \"if ( ($.LoopTask['iteration'] &lt; $.value ) || ( $.first_task['response']['body'] &gt; 10)) { false; } else { true; }\",\n    \"loopOver\": [\n        {\n            \"name\": \"first task\",\n            \"taskReferenceName\": \"first_task\",\n            \"inputParameters\": {\n                \"http_request\": {\n                    \"uri\": \"http://localhost:8082\",\n                    \"method\": \"POST\"\n                }\n            },\n            \"type\": \"HTTP\"\n        },{\n            \"name\": \"second task\",\n            \"taskReferenceName\": \"second_task\",\n            \"inputParameters\": {\n                \"http_request\": {\n                    \"uri\": \"http://localhost:8082\",\n                    \"method\": \"POST\"\n                }\n            },\n            \"type\": \"HTTP\"\n        }\n    ],\n    \"startDelay\": 0,\n    \"optional\": false\n}\n</code></pre></p> <p>will produce the following execution, assuming 3 executions occurred (alongside <code>first_task__1</code>, <code>first_task__2</code>, <code>first_task__3</code>, <code>second_task__1</code>, <code>second_task__2</code> and <code>second_task__3</code>):</p> <pre><code>{\n    \"taskType\": \"DO_WHILE\",\n    \"outputData\": {\n        \"iteration\": 3,\n        \"1\": {\n            \"first_task\": {\n                \"response\": {},\n                \"headers\": {\n                    \"Content-Type\": \"application/json\"\n                }\n            },\n            \"second_task\": {\n                \"response\": {},\n                \"headers\": {\n                    \"Content-Type\": \"application/json\"\n                }\n            }\n        },\n        \"2\": {\n            \"first_task\": {\n                \"response\": {},\n                \"headers\": {\n                    \"Content-Type\": \"application/json\"\n                }\n            },\n            \"second_task\": {\n                \"response\": {},\n                \"headers\": {\n                    \"Content-Type\": \"application/json\"\n                }\n            }\n        },\n        \"3\": {\n            \"first_task\": {\n                \"response\": {},\n                \"headers\": {\n                    \"Content-Type\": \"application/json\"\n                }\n            },\n            \"second_task\": {\n                \"response\": {},\n                \"headers\": {\n                    \"Content-Type\": \"application/json\"\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"reference/operators/do-while-task.html#example-using-iteration-key","title":"Example using iteration key","text":"<p>Sometimes, you may want to use the iteration value/counter in the tasks used in the loop.  In this example, an API call is made to GitHub (to the Conductor repository), but each loop increases the pagination.</p> <p>The Loop <code>taskReferenceName</code> is \"get_all_stars_loop_ref\".</p> <p>In the <code>loopCondition</code> the term <code>$.get_all_stars_loop_ref['iteration']</code> is used.</p> <p>In tasks embedded in the loop, <code>${get_all_stars_loop_ref.output.iteration}</code> is used.  In this case, it is used to define which page of results the API should return.</p> <pre><code>{\n      \"name\": \"get_all_stars\",\n      \"taskReferenceName\": \"get_all_stars_loop_ref\",\n      \"inputParameters\": {\n        \"stargazers\": \"4000\"\n      },\n      \"type\": \"DO_WHILE\",\n      \"decisionCases\": {},\n      \"defaultCase\": [],\n      \"forkTasks\": [],\n      \"startDelay\": 0,\n      \"joinOn\": [],\n      \"optional\": false,\n      \"defaultExclusiveJoinTask\": [],\n      \"asyncComplete\": false,\n      \"loopCondition\": \"if ($.get_all_stars_loop_ref['iteration'] &lt; Math.ceil($.stargazers/100)) { true; } else { false; }\",\n      \"loopOver\": [\n        {\n          \"name\": \"100_stargazers\",\n          \"taskReferenceName\": \"hundred_stargazers_ref\",\n          \"inputParameters\": {\n            \"counter\": \"${get_all_stars_loop_ref.output.iteration}\",\n            \"http_request\": {\n              \"uri\": \"https://api.github.com/repos/ntflix/conductor/stargazers?page=${get_all_stars_loop_ref.output.iteration}&amp;per_page=100\",\n              \"method\": \"GET\",\n              \"headers\": {\n                \"Authorization\": \"token ${workflow.input.gh_token}\",\n                \"Accept\": \"application/vnd.github.v3.star+json\"\n              }\n            }\n          },\n          \"type\": \"HTTP\",\n          \"decisionCases\": {},\n          \"defaultCase\": [],\n          \"forkTasks\": [],\n          \"startDelay\": 0,\n          \"joinOn\": [],\n          \"optional\": false,\n          \"defaultExclusiveJoinTask\": [],\n          \"asyncComplete\": false,\n          \"loopOver\": [],\n          \"retryCount\": 3\n        }\n      ]\n    }\n</code></pre>"},{"location":"reference/operators/dynamic-fork-task.html","title":"Dynamic Fork","text":"<pre><code>\"type\" : \"FORK_JOIN_DYNAMIC\"\n</code></pre> <p>The <code>DYNAMIC_FORK</code> operation in Conductor lets you execute a list of tasks or sub-workflows in parallel. This list will be determined at run-time and be of variable length.</p> <p>A <code>DYNAMIC_FORK</code> is typically followed by <code>JOIN</code> task that collects outputs from each of the forked tasks or sub workflows.</p> <p>There are three things that are needed to configure a <code>FORK_JOIN_DYNAMIC</code> task.</p> <ol> <li>A list of tasks or sub-workflows that needs to be forked and run in parallel.</li> <li>A list of inputs to each of these forked tasks or sub-workflows</li> <li>A task prior to the <code>FORK_JOIN_DYNAMIC</code> tasks outputs 1 and 2 above that can be wired in as in input to    the <code>FORK_JOIN_DYNAMIC</code> tasks</li> </ol>"},{"location":"reference/operators/dynamic-fork-task.html#use-cases","title":"Use Cases","text":"<p>A <code>FORK_JOIN_DYNAMIC</code> is useful, when a set of tasks or sub-workflows needs to be executed and the number of tasks or sub-workflows are determined at run time. E.g. Let's say we have a task that resizes an image, and we need to create a workflow that will resize an image into multiple sizes. In this case, a task can be created prior to the <code>FORK_JOIN_DYNAMIC</code> task that will prepare the input that needs to be passed into the <code>FORK_JOIN_DYNAMIC</code> task. The single image resize task does one job. The <code>FORK_JOIN_DYNAMIC</code> and the following <code>JOIN</code> will manage the multiple invokes of the single image resize task. Here, the responsibilities are clearly broken out, where the single image resize task does the core job and <code>FORK_JOIN_DYNAMIC</code> manages the orchestration and fault tolerance aspects.</p>"},{"location":"reference/operators/dynamic-fork-task.html#configuration","title":"Configuration","text":"<p>To use the <code>DYNAMIC_FORK</code> task, you need to provide the following attributes at the top level of the task configuration, as well as corresponding values inside <code>inputParameters</code>.</p> Attribute Description dynamicForkTasksParam Name of attribute in <code>inputParameters</code> to read array of task or subworkflow names from. dynamicForkTasksInputParamName Name of attribute in <code>inputParameters</code> to read map of inputs to use for spawned tasks or subworkflows."},{"location":"reference/operators/dynamic-fork-task.html#inputparameters","title":"inputParameters","text":"Attribute Description *dynamicForkTasksParam This is a JSON array of tasks or sub-workflow objects that needs to be forked and run in parallel (Note: This has a different format for <code>SUB_WORKFLOW</code> compared to <code>CUSTOM</code> tasks.) *dynamicForkTasksInputParamName A JSON map, where the keys are task or sub-workflow names, and the values are the <code>inputParameters</code> to be passed into the corresponding spawned tasks or sub-workflows. <p>Note: * means the de-referenced name.</p>"},{"location":"reference/operators/dynamic-fork-task.html#examples","title":"Examples","text":""},{"location":"reference/operators/dynamic-fork-task.html#example-1","title":"Example 1","text":"<p>Here is an example of a <code>FORK_JOIN_DYNAMIC</code> task followed by a <code>JOIN</code> task</p> <pre><code>[\n  {\n    \"inputParameters\": {\n      \"dynamicTasks\": \"${fooBarTask.output.dynamicTasksJSON}\",\n      \"dynamicTasksInput\": \"${fooBarTask.output.dynamicTasksInputJSON}\"\n    },\n    \"type\": \"FORK_JOIN_DYNAMIC\",\n    \"dynamicForkTasksParam\": \"dynamicTasks\",\n    \"dynamicForkTasksInputParamName\": \"dynamicTasksInput\"\n  },\n  {\n    \"name\": \"image_multiple_convert_resize_join\",\n    \"taskReferenceName\": \"image_multiple_convert_resize_join_ref\",\n    \"type\": \"JOIN\"\n  }\n]\n</code></pre> <p>Dissecting into this example above, let's look at the three things that are needed to configured for the <code>FORK_JOIN_DYNAMIC</code> task</p> <p><code>dynamicForkTasksParam</code> This is a JSON array of task or sub-workflow objects that specifies the list of tasks or sub-workflows that needs to be forked and run in parallel <code>dynamicForkTasksInputParamName</code> This is a JSON map of task or sub-workflow objects that specifies the list of tasks or sub-workflows that needs to be forked and run in parallel fooBarTask This is a task that is defined prior to the FORK_JOIN_DYNAMIC in the workflow definition. This task will need to output (outputParameters) 1 and 2 above so that it can be wired into inputParameters of the FORK_JOIN_DYNAMIC tasks. (dynamicTasks and dynamicTasksInput)</p>"},{"location":"reference/operators/dynamic-fork-task.html#example-2","title":"Example 2","text":"<p>Let's say we have a task that resizes an image, and we need to create a workflow that will resize an image into multiple sizes. In this case, a task can be created prior to the <code>FORK_JOIN_DYNAMIC</code> task that will prepare the input that needs to be passed into the <code>FORK_JOIN_DYNAMIC</code> task. These will be:</p> <ul> <li><code>dynamicForkTasksParam</code> the JSON array of tasks/subworkflows to be run in parallel. Each JSON object will have: </li> <li>A unique <code>taskReferenceName</code>.</li> <li>The name of the Task/Subworkflow to be called (note - the location of this key:value is different for a subworkflow).</li> <li>The type of the task (This is optional for CUSTOM tasks).</li> <li><code>dynamicForkTasksInputParamName</code> a JSON map of input parameters for each task. The keys will be the unique <code>taskReferenceName</code> defined in the first JSON array, and the values will be the specific input parameters for the task/subworkflow.</li> </ul> <p>The <code>image_resize</code> task works to resize just one image. The <code>FORK_JOIN_DYNAMIC</code> and the following <code>JOIN</code> will manage the multiple invocations of the single <code>image_resize</code> task. The responsibilities are clearly broken out, where the individual  <code>image_resize</code> tasks do the core job and <code>FORK_JOIN_DYNAMIC</code> manages the orchestration and fault tolerance aspects of handling multiple invocations of the task.</p>"},{"location":"reference/operators/dynamic-fork-task.html#workflow-definition-task-configuration","title":"Workflow Definition - Task Configuration","text":"<p>Here is an example of a <code>FORK_JOIN_DYNAMIC</code> task followed by a <code>JOIN</code> task.  The fork is named and given a taskReferenceName, but all of the input parameters are JSON variables that we will discuss next:</p> <pre><code>[\n  {      \n    \"name\": \"image_multiple_convert_resize_fork\",\n    \"taskReferenceName\": \"image_multiple_convert_resize_fork_ref\",\n    \"inputParameters\": {\n      \"dynamicTasks\": \"${fooBarTask.output.dynamicTasksJSON}\",\n      \"dynamicTasksInput\": \"${fooBarTask.output.dynamicTasksInputJSON}\"\n    },\n    \"type\": \"FORK_JOIN_DYNAMIC\",\n    \"dynamicForkTasksParam\": \"dynamicTasks\",\n    \"dynamicForkTasksInputParamName\": \"dynamicTasksInput\"\n  },\n  {\n    \"name\": \"image_multiple_convert_resize_join\",\n    \"taskReferenceName\": \"image_multiple_convert_resize_join_ref\",\n    \"type\": \"JOIN\"\n  }\n]\n</code></pre> <p>This appears in the UI as follows:</p> <p></p> <p>Let's assume this data is sent to the workflow:</p> <pre><code>{\n    \"fileLocation\": \"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&amp;name=900x900\",\n    \"outputFormats\": [\"png\",\"jpg\"],\n\n    \"outputSizes\": [\n        {\"width\":300,\n        \"height\":300},\n        {\"width\":200,\n        \"height\":200}\n    ],\n    \"maintainAspectRatio\": \"true\"\n}\n</code></pre> <p>With 2 file formats and 2 sizes in the input, we'll be creating 4 images total.  The first task will generate the tasks and the parameters for these tasks:</p> <ul> <li><code>dynamicForkTasksParam</code> This is a JSON array of task or sub-workflow objects that specifies the list of tasks or sub-workflows that needs to be forked and run in parallel. This JSON varies depeding oon the type of task.  </li> </ul>"},{"location":"reference/operators/dynamic-fork-task.html#dynamicforktasksparam-simple-task","title":"<code>dynamicForkTasksParam</code> Simple task","text":"<p>In this case, our fork is running a CUSTOM task: <code>image_convert_resize</code>:</p> <pre><code>{ \"dynamicTasks\": [\n  {\n    \"name\": :\"image_convert_resize\",\n    \"taskReferenceName\": \"image_convert_resize_png_300x300_0\",\n    ...\n  },\n  {\n    \"name\": :\"image_convert_resize\",\n    \"taskReferenceName\": \"image_convert_resize_png_200x200_1\",\n    ...\n  },\n  {\n    \"name\": :\"image_convert_resize\",\n    \"taskReferenceName\": \"image_convert_resize_jpg_300x300_2\",\n    ...\n  },\n  {\n    \"name\": :\"image_convert_resize\",\n    \"taskReferenceName\": \"image_convert_resize_jpg_200x200_3\",\n    ...\n  }\n]}\n</code></pre>"},{"location":"reference/operators/dynamic-fork-task.html#dynamicforktasksparam-subworkflow-task","title":"<code>dynamicForkTasksParam</code> SubWorkflow task","text":"<p>In this case, our Dynamic fork is running a SUB_WORKFLOW task: <code>image_convert_resize_subworkflow</code></p> <pre><code>{ \"dynamicTasks\": [\n  {\n    \"subWorkflowParam\" : {\n      \"name\": :\"image_convert_resize_subworkflow\",\n      \"version\": \"1\"\n    },\n    \"type\" : \"SUB_WORKFLOW\",\n    \"taskReferenceName\": \"image_convert_resize_subworkflow_png_300x300_0\",\n    ...\n  },\n  {\n    \"subWorkflowParam\" : {\n      \"name\": :\"image_convert_resize_subworkflow\",\n      \"version\": \"1\"\n    },\n    \"type\" : \"SUB_WORKFLOW\",\n    \"taskReferenceName\": \"image_convert_resize_subworkflow_png_200x200_1\",\n    ...\n  },\n  {\n    \"subWorkflowParam\" : {\n      \"name\": :\"image_convert_resize_subworkflow\",\n      \"version\": \"1\"\n    },\n    \"type\" : \"SUB_WORKFLOW\",\n    \"taskReferenceName\": \"image_convert_resize_subworkflow_jpg_300x300_2\",\n    ...\n  },\n  {\n    \"subWorkflowParam\" : {\n      \"name\": :\"image_convert_resize_subworkflow\",\n      \"version\": \"1\"\n    },\n    \"type\" : \"SUB_WORKFLOW\",\n    \"taskReferenceName\": \"image_convert_resize_subworkflow_jpg_200x200_3\",\n    ...\n  }\n]}\n</code></pre> <ul> <li><code>dynamicForkTasksInputParamName</code> This is a JSON map of task or sub-workflow objects and all the input parameters that these tasks will need to run.</li> </ul> <pre><code>\"dynamicTasksInput\":{\n\"image_convert_resize_jpg_300x300_2\":{\n\"outputWidth\":300\n\"outputHeight\":300\n\"fileLocation\":\"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&amp;name=900x900\"\n\"outputFormat\":\"jpg\"\n\"maintainAspectRatio\":true\n}\n\"image_convert_resize_jpg_200x200_3\":{\n\"outputWidth\":200\n\"outputHeight\":200\n\"fileLocation\":\"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&amp;name=900x900\"\n\"outputFormat\":\"jpg\"\n\"maintainAspectRatio\":true\n}\n\"image_convert_resize_png_200x200_1\":{\n\"outputWidth\":200\n\"outputHeight\":200\n\"fileLocation\":\"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&amp;name=900x900\"\n\"outputFormat\":\"png\"\n\"maintainAspectRatio\":true\n}\n\"image_convert_resize_png_300x300_0\":{\n\"outputWidth\":300\n\"outputHeight\":300\n\"fileLocation\":\"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&amp;name=900x900\"\n\"outputFormat\":\"png\"\n\"maintainAspectRatio\":true\n}\n</code></pre>"},{"location":"reference/operators/dynamic-fork-task.html#join-task","title":"Join Task","text":"<p>The JOIN task will run after all of the dynamic tasks, collecting the output for all of the tasks.</p>"},{"location":"reference/operators/dynamic-task.html","title":"Dynamic","text":"<p><pre><code>\"type\" : \"DYNAMIC\"\n</code></pre> The <code>DYNAMIC</code> task allows one to execute a task whose name is resolved dynamically at run-time. The task name to execute is specified as <code>taskToExecute</code> in <code>inputParameters</code>.</p>"},{"location":"reference/operators/dynamic-task.html#use-cases","title":"Use Cases","text":"<p>Consider a scenario, when we have to make decision of executing a task dynamically i.e. while the workflow is still running. In such cases, Dynamic Task would be useful.</p>"},{"location":"reference/operators/dynamic-task.html#configuration","title":"Configuration","text":"<p>To use the <code>DYNAMIC</code> task, you need to provide <code>dynamicTaskNameParam</code> at the top level of the task configuration, as well as an attribute in <code>inputParameters</code> matching the value you selected for <code>dynamicTaskNameParam</code>.</p> name description dynamicTaskNameParam Name of the parameter from <code>inputParameters</code> whose value is used to schedule the task. e.g. <code>\"taskToExecute\"</code>"},{"location":"reference/operators/dynamic-task.html#inputparameters","title":"inputParameters","text":"name description *dynamicTaskNameParam e.g. <code>taskToExecute</code> Name of task to execute."},{"location":"reference/operators/dynamic-task.html#example","title":"Example","text":"<p>Suppose in a workflow, we have to take decision to ship the courier with the shipping service providers on the basis of Post Code.</p> <p>Consider the following 3 task definitions.</p> <p>The following task <code>shipping_info</code> generates an output on the basis of which decision would be taken to run the next task.</p> <pre><code>{\n  \"name\": \"shipping_info\",\n  \"retryCount\": 3,\n  \"timeoutSeconds\": 600,\n  \"pollTimeoutSeconds\": 1200,\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 300,\n  \"responseTimeoutSeconds\": 300,\n  \"concurrentExecLimit\": 100,\n  \"rateLimitFrequencyInSeconds\": 60,\n  \"ownerEmail\":\"abc@example.com\",\n  \"rateLimitPerFrequency\": 1\n}\n</code></pre> <p>The following are the two custom tasks. Only one these tasks will run based on the output generated by the <code>shipping_info</code> task:</p> <pre><code>{\n  \"name\": \"ship_via_fedex\",\n  \"retryCount\": 3,\n  \"timeoutSeconds\": 600,\n  \"pollTimeoutSeconds\": 1200,\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 300,\n  \"responseTimeoutSeconds\": 300,\n  \"concurrentExecLimit\": 100,\n  \"rateLimitFrequencyInSeconds\": 60,\n  \"ownerEmail\":\"abc@example.com\",\n  \"rateLimitPerFrequency\": 2\n},\n{\n  \"name\": \"ship_via_ups\",\n  \"retryCount\": 3,\n  \"timeoutSeconds\": 600,\n  \"pollTimeoutSeconds\": 1200,\n  \"timeoutPolicy\": \"TIME_OUT_WF\",\n  \"retryLogic\": \"FIXED\",\n  \"retryDelaySeconds\": 300,\n  \"responseTimeoutSeconds\": 300,\n  \"concurrentExecLimit\": 100,\n  \"rateLimitFrequencyInSeconds\": 60,\n  \"ownerEmail\":\"abc@example.com\",\n  \"rateLimitPerFrequency\": 2\n}\n</code></pre> <p>We will create a workflow with the following definition :</p> <pre><code>{\n  \"name\": \"Shipping_Flow\",\n  \"description\": \"Ships smartly on the basis of Shipping info\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"shipping_info\",\n      \"taskReferenceName\": \"shipping_info\",\n      \"inputParameters\": {\n      },\n      \"type\": \"CUSTOM\"\n    },\n    {\n      \"name\": \"shipping_task\",\n      \"taskReferenceName\": \"shipping_task\",\n      \"inputParameters\": {\n        \"taskToExecute\": \"${shipping_info.output.shipping_service}\"\n      },\n      \"type\": \"DYNAMIC\",\n      \"dynamicTaskNameParam\": \"taskToExecute\"\n    }\n\n  ],\n  \"restartable\": true,\n  \"ownerEmail\":\"abc@example.com\",\n  \"workflowStatusListenerEnabled\": true,\n  \"schemaVersion\": 2\n}\n</code></pre> <p>The workflow created is shown in the below diagram.</p> <p></p> <p>Note : <code>shipping_task</code> is a <code>DYNAMIC</code> task and the <code>taskToExecute</code> parameter can be set with input value provided while running the workflow or with the output of previous tasks.</p> <p>Here, it is set to the output provided by the previous task i.e. <code>${shipping_info.output.shipping_service}</code>.</p> <p>If the input value is provided while running the workflow it can be accessed by <code>${workflow.input.shipping_service}</code>.</p> <pre><code>{\n  \"shipping_service\": \"ship_via_fedex\"\n}\n</code></pre> <p>We can see in the below example that on the basis of Post Code the shipping service is being decided.</p> <p>Based on given set of inputs i.e. Post Code starts with '9' hence, <code>ship_via_fedex</code> is executed -</p> <p></p> <p>If the Post Code startes with anything other than 9 <code>ship_via_ups</code> is executed -</p> <p></p> <p>If the incorrect task name or the task that doesn't exist is provided then the workflow fails and we get the error <code>Invalid task specified. Cannot find task by name in the task definitions.</code></p> <p>If null reference is provided in the task name the workflow also fails and we get the error <code>Cannot map a dynamic task based on the parameter and input. Parameter= taskToExecute, input= {taskToExecute=null}</code></p>"},{"location":"reference/operators/fork-task.html","title":"Fork","text":"<pre><code>\"type\" : \"FORK_JOIN\"\n</code></pre> <p>A <code>FORK_JOIN</code> operation lets you run a specified list of tasks or sub workflows in parallel. A <code>FORK_JOIN</code> task is followed by a <code>JOIN</code> operation that waits on the forked tasks or sub workflows to finish and collects their outputs.</p> <p>This is also known as a Static Fork to distinguish it from the DYNAMIC_FORK.</p>"},{"location":"reference/operators/fork-task.html#use-cases","title":"Use Cases","text":"<p><code>FORK_JOIN</code> tasks are typically used when a list of tasks can be run in parallel. E.g In a notification workflow, there could be multiple ways of sending notifications, i,e e-mail, SMS, HTTP etc. These notifications are not dependent on each other, and so they can be run in parallel. In such cases, you can create 3 sub-lists of forked tasks for each of these operations.</p>"},{"location":"reference/operators/fork-task.html#configuration","title":"Configuration","text":"<p>A <code>FORK_JOIN</code> task has a <code>forkTasks</code> attribute at the top level of the task configuration that is an array of arrays (<code>[[...], [...]]</code>);</p> Attribute Description forkTasks A list of lists of tasks. Each of the outer list will be invoked in parallel. The inner list can be a graph of other tasks and sub-workflows <p>Each element of <code>forkTasks</code> is itself a list of tasks. These sub-lists are invoked in parallel. The tasks defined within each sublist can be sequential or even more nested forks.</p> <p>A FORK_JOIN is typically followed by a JOIN operation. </p> <p>The behavior of a <code>FORK_JOIN</code> task is not affected by <code>inputParameters</code>.</p>"},{"location":"reference/operators/fork-task.html#output","title":"Output","text":"<p><code>FORK_JOIN</code> has no output. </p> <p>The <code>FORK_JOIN</code> task is used in conjunction with the JOIN task, which aggregates the output from the parallelized workflows.</p>"},{"location":"reference/operators/fork-task.html#example","title":"Example","text":"<p>Imagine a workflow that sends 3 notifications: email, SMS and HTTP. Since none of these steps are dependant on the others, they can be run in parallel with a fork.</p> <p>The diagram will appear as:</p> <p></p> <p>Here's the JSON definition for the workflow:</p> <pre><code>[\n  {\n    \"name\": \"fork_join\",\n    \"taskReferenceName\": \"my_fork_join_ref\",\n    \"type\": \"FORK_JOIN\",\n    \"forkTasks\": [\n      [\n        {\n          \"name\": \"process_notification_payload\",\n          \"taskReferenceName\": \"process_notification_payload_email\",\n          \"type\": \"CUSTOM\"\n        },\n        {\n          \"name\": \"email_notification\",\n          \"taskReferenceName\": \"email_notification_ref\",\n          \"type\": \"CUSTOM\"\n        }\n      ],\n      [\n        {\n          \"name\": \"process_notification_payload\",\n          \"taskReferenceName\": \"process_notification_payload_sms\",\n          \"type\": \"CUSTOM\"\n        },\n        {\n          \"name\": \"sms_notification\",\n          \"taskReferenceName\": \"sms_notification_ref\",\n          \"type\": \"CUSTOM\"\n        }\n      ],\n      [\n        {\n          \"name\": \"process_notification_payload\",\n          \"taskReferenceName\": \"process_notification_payload_http\",\n          \"type\": \"CUSTOM\"\n        },\n        {\n          \"name\": \"http_notification\",\n          \"taskReferenceName\": \"http_notification_ref\",\n          \"type\": \"CUSTOM\"\n        }\n      ]\n    ]\n  },\n  {\n    \"name\": \"notification_join\",\n    \"taskReferenceName\": \"notification_join_ref\",\n    \"type\": \"JOIN\",\n    \"joinOn\": [\n      \"email_notification_ref\",\n      \"sms_notification_ref\"\n    ]\n  }\n]\n</code></pre> <p>Note</p> <p>There are three parallel 'tines' to this fork, but only two of the outputs are required for the JOIN to continue, owing to the definition of <code>joinOn</code>. The diagram does draw an arrow from <code>http_notification_ref</code> to the <code>notification_join</code>, but it is not required for the workflow to continue. </p> <p>Here is how the output of notification_join will look like. The output is a map, where the keys are the names of task references that were being <code>joinOn</code>. The corresponding values are the outputs of those tasks.</p> <pre><code>{\n  \"email_notification_ref\": {\n    \"email_sent_at\": \"2021-11-06T07:37:17+0000\",\n    \"email_sent_to\": \"test@example.com\"\n  },\n  \"sms_notification_ref\": {\n    \"smm_sent_at\": \"2021-11-06T07:37:17+0129\",\n    \"sms_sen\": \"+1-425-555-0189\"\n  }\n}\n</code></pre> <p>See JOIN for more details on the JOIN aspect of the FORK.</p>"},{"location":"reference/operators/join-task.html","title":"Join","text":"<pre><code>\"type\" : \"JOIN\"\n</code></pre> <p>A <code>JOIN</code> task is used in conjunction with a <code>FORK_JOIN</code> or <code>FORK_JOIN_DYNAMIC</code> task. Each of the aggregated task outputs is given a corresponding key in the <code>JOIN</code> task output.</p> <ul> <li>When used with a <code>FORK_JOIN</code> task, it waits for a user provided list of zero or more of the forked tasks to be completed. </li> <li>When used with a <code>FORK_JOIN_DYNAMIC</code> task, it implicitly waits for all of the dynamically forked tasks to complete.</li> </ul> <p><code>JOIN</code> used in this context is loosely analogous to the Map phase of the Map-Reduce programming pattern. In Conductor, the reduce step could  be implemented as a subsequent task that references the output of the <code>JOIN</code> task.</p>"},{"location":"reference/operators/join-task.html#use-cases","title":"Use Cases","text":"<p>FORK_JOIN and FORK_JOIN_DYNAMIC task are used to execute a collection of other tasks or sub workflows in parallel. In such cases, there is a need to collect the output from the forked tasks before moving to the next stage in the workflow. </p>"},{"location":"reference/operators/join-task.html#configuration","title":"Configuration","text":"<p>When used with <code>FORK_JOIN</code> (Static Fork), The <code>joinOn</code> attribute is provided at the top level of the task configuration. </p> Attribute Description joinOn A list of task reference names that this <code>JOIN</code> task will wait for completion. Omitted for <code>DYNAMIC_FORK</code> <p>The <code>JOIN</code> task does not utilize <code>inputParameters</code>. </p>"},{"location":"reference/operators/join-task.html#output","title":"Output","text":"Attribute Description task_ref_name_1 A task reference name that was being <code>joinOn</code>. The value is the output of that task task_ref_name_2 A task reference name that was being <code>joinOn</code>. The value is the output of that task ... ... task_ref_name_N A task reference name that was being <code>joinOn</code>. The value is the output of that task"},{"location":"reference/operators/join-task.html#examples","title":"Examples","text":""},{"location":"reference/operators/join-task.html#simple-example","title":"Simple Example","text":"<p>Here is an example of a <code>JOIN</code> task. This task will wait for the completion of tasks <code>my_task_ref_1</code> and <code>my_task_ref_2</code> as specified by the <code>joinOn</code> attribute.</p> <pre><code>{\n  \"name\": \"join_task\",\n  \"taskReferenceName\": \"my_join_task_ref\",\n  \"type\": \"JOIN\",\n  \"joinOn\": [\n    \"my_task_ref_1\",\n    \"my_task_ref_2\"\n  ]\n}\n</code></pre>"},{"location":"reference/operators/join-task.html#example-ignoring-one-fork","title":"Example - Ignoring one fork","text":"<p>Here is an example of a <code>JOIN</code> task used in conjunction with a <code>FORK_JOIN</code> task. The 'FORK_JOIN' spawns 3 tasks. An <code>email_notification</code> task, a <code>sms_notification</code> task and a  <code>http_notification</code> task. Email and SMS are usually best effort delivery systems. However, in case of a http based notification you get a return code and you can retry until it succeeds or eventually give up. When you setup a notification workflow, you may decide to continue ,if you kicked off an email and sms notification. Im that case, you can decide to <code>joinOn</code> those specific tasks. However, the <code>http_notification</code> task will still continue to execute, but it will not block the rest of the workflow from proceeding.</p> <pre><code>[\n  {\n    \"name\": \"fork_join\",\n    \"taskReferenceName\": \"my_fork_join_ref\",\n    \"type\": \"FORK_JOIN\",\n    \"forkTasks\": [\n      [\n        {\n          \"name\": \"email_notification\",\n          \"taskReferenceName\": \"email_notification_ref\",\n          \"type\": \"CUSTOM\"\n        }\n      ],\n      [\n        {\n          \"name\": \"sms_notification\",\n          \"taskReferenceName\": \"sms_notification_ref\",\n          \"type\": \"CUSTOM\"\n        }\n      ],\n      [\n        {\n          \"name\": \"http_notification\",\n          \"taskReferenceName\": \"http_notification_ref\",\n          \"type\": \"CUSTOM\"\n        }\n      ]\n    ]\n  },\n  {\n    \"name\": \"notification_join\",\n    \"taskReferenceName\": \"notification_join_ref\",\n    \"type\": \"JOIN\",\n    \"joinOn\": [\n      \"email_notification_ref\",\n      \"sms_notification_ref\"\n    ]\n  }\n]\n</code></pre> <p>Here is how the output of notification_join will look like. The output is a map, where the keys are the names of task references that were being <code>joinOn</code>. The corresponding values are the outputs of those tasks.</p> <pre><code>{\n  \"email_notification_ref\": {\n    \"email_sent_at\": \"2021-11-06T07:37:17+0000\",\n    \"email_sent_to\": \"test@example.com\"\n  },\n  \"sms_notification_ref\": {\n    \"smm_sent_at\": \"2021-11-06T07:37:17+0129\",\n    \"sms_sen\": \"+1-425-555-0189\"\n  }\n}\n</code></pre>"},{"location":"reference/operators/set-variable-task.html","title":"Set Variable","text":"<pre><code>\"type\" : \"SET_VARIABLE\"\n</code></pre> <p>The <code>SET_VARIABLE</code> task allows users to create global workflow variables, and update them with new values.</p> <p>Variables can be initialized in the workflow definition as well as during the workflow run. Once a variable is initialized it can be read using the expression <code>${workflow.variables.NAME}</code> by any other task.</p> <p>It can be overwritten by a subsequent <code>SET_VARIABLE</code> task.</p> <p>Warning</p> <p>There is a hard barrier for variables payload size in KB defined in the JVM system properties (<code>conductor.max.workflow.variables.payload.threshold.kb</code>) the default value is <code>256</code>. Passing this barrier will fail the task and the workflow.</p>"},{"location":"reference/operators/set-variable-task.html#use-cases","title":"Use Cases","text":"<p>For example, you might want to track shared state at the workflow level, and have the state be accessible by any task executed as part of the workflow.</p>"},{"location":"reference/operators/set-variable-task.html#configuration","title":"Configuration","text":"<p>Global variables can be set in <code>inputParameters</code> using the desired variable names and their respective values.</p>"},{"location":"reference/operators/set-variable-task.html#example","title":"Example","text":"<p>Suppose in a workflow, we have to store a value in a variable and then later in workflow reuse the value stored in the variable just as we do in programming, in such scenarios <code>Set Variable</code> task can be used.</p> <p>Following is the workflow definition with <code>SET_VARIABLE</code> task.</p> <pre><code>{\n  \"name\": \"Set_Variable_Workflow\",\n  \"description\": \"Set a value to a variable and then reuse it later in the workflow\",\n  \"version\": 1,\n  \"tasks\": [\n    {\n      \"name\": \"Set_Name\",\n      \"taskReferenceName\": \"Set_Name\",\n      \"type\": \"SET_VARIABLE\",\n      \"inputParameters\": {\n        \"name\": \"Foo\"\n      }\n    },\n    {\n      \"name\": \"Read_Name\",\n      \"taskReferenceName\": \"Read_Name\",\n      \"inputParameters\": {\n        \"saved_name\" : \"${workflow.variables.name}\"\n      },\n      \"type\": \"CUSTOM\"\n    }\n  ],\n  \"restartable\": true,\n  \"ownerEmail\":\"abc@example.com\",\n  \"workflowStatusListenerEnabled\": true,\n  \"schemaVersion\": 2\n}\n</code></pre> <p>In the above example, it can be seen that the task <code>Set_Name</code> is a Set Variable Task and the variable <code>name</code> is set to <code>Foo</code> and later in the workflow it is referenced by <code>\"${workflow.variables.name}\"</code> in another task.</p>"},{"location":"reference/operators/start-workflow-task.html","title":"Start Workflow","text":"<pre><code>\"type\" : \"START_WORKFLOW\"\n</code></pre> <p>The <code>START_WORKFLOW</code> task starts another workflow. Unlike <code>SUB_WORKFLOW</code>, <code>START_WORKFLOW</code> does not create a relationship between starter and the started workflow. It also does not wait for the started workflow to complete. A <code>START_WORKFLOW</code> is  considered successful once the requested workflow is started successfully. In other words, <code>START_WORKFLOW</code> is marked as <code>COMPLETED</code> once the started  workflow is in <code>RUNNING</code> state.</p> <p>There is no ability to access the <code>output</code> of the started workflow.</p>"},{"location":"reference/operators/start-workflow-task.html#use-cases","title":"Use Cases","text":"<p>When another workflow needs to be started from the current workflow, <code>START_WORKFLOW</code> can be used. </p>"},{"location":"reference/operators/start-workflow-task.html#configuration","title":"Configuration","text":"<p>The workflow invocation payload is passed into <code>startWorkflow</code> under <code>inputParameters</code>.</p>"},{"location":"reference/operators/start-workflow-task.html#inputparameters","title":"inputParameters","text":"name type description startWorkflow Map[String, Any] The value of this parameter is Start Workflow Request."},{"location":"reference/operators/start-workflow-task.html#output","title":"Output","text":"name type description workflowId String The id of the started workflow <p>Note: <code>START_WORKFLOW</code> will neither wait for the completion of, nor pass back the <code>output</code> of the spawned workflow.</p>"},{"location":"reference/operators/sub-workflow-task.html","title":"Sub Workflow","text":"<p><pre><code>\"type\" : \"SUB_WORKFLOW\"\n</code></pre> Sub Workflow task allows for nesting a workflow within another workflow. Nested workflows contain a reference to their parent.</p>"},{"location":"reference/operators/sub-workflow-task.html#use-cases","title":"Use Cases","text":"<p>Suppose we want to include another workflow inside our current workflow. In that case, Sub Workflow Task would be used.</p>"},{"location":"reference/operators/sub-workflow-task.html#configuration","title":"Configuration","text":"<p><code>subWorkflowParam</code> is provided at the top level of the task configuration.</p> name type description subWorkflowParam Map[String, Any] See below <p><code>inputParameters</code> will be passed down to the invoked sub-workflow.</p>"},{"location":"reference/operators/sub-workflow-task.html#subworkflowparam","title":"subWorkflowParam","text":"name type description name String Name of the workflow to execute version Integer Version of the workflow to execute taskToDomain Map[String, String] Allows scheduling the sub workflow's tasks per given mappings.  See Task Domains for instructions to configure taskDomains. workflowDefinition WorkflowDefinition Allows starting a subworkflow with a dynamic workflow definition."},{"location":"reference/operators/sub-workflow-task.html#output","title":"Output","text":"name type description subWorkflowId String Sub-workflow execution Id generated when running the sub-workflow"},{"location":"reference/operators/sub-workflow-task.html#examples","title":"Examples","text":"<p>Imagine we have a workflow that has a fork in it. In the example below, we input one image, but using a fork to create 2 images simultaneously:</p> <p></p> <p>The left fork will create a JPG, and the right fork a WEBP image. Maintaining this workflow might be difficult, as changes made to one side of the fork do not automatically propagate the other.  Rather than using 2 tasks, we can define a <code>image_convert_resize</code> workflow that we can call for both forks as a sub-workflow:</p> <pre><code>{\n    \"name\": \"image_convert_resize_subworkflow1\",\n    \"description\": \"Image Processing Workflow\",\n    \"version\": 1,\n    \"tasks\": [{\n            \"name\": \"image_convert_resize_multipleformat_fork\",\n            \"taskReferenceName\": \"image_convert_resize_multipleformat_ref\",\n            \"inputParameters\": {},\n            \"type\": \"FORK_JOIN\",\n            \"decisionCases\": {},\n            \"defaultCase\": [],\n            \"forkTasks\": [\n                [{\n                    \"name\": \"image_convert_resize_sub\",\n                    \"taskReferenceName\": \"subworkflow_jpg_ref\",\n                    \"inputParameters\": {\n                        \"fileLocation\": \"${workflow.input.fileLocation}\",\n                        \"recipeParameters\": {\n                            \"outputSize\": {\n                                \"width\": \"${workflow.input.recipeParameters.outputSize.width}\",\n                                \"height\": \"${workflow.input.recipeParameters.outputSize.height}\"\n                            },\n                            \"outputFormat\": \"jpg\"\n                        }\n                    },\n                    \"type\": \"SUB_WORKFLOW\",\n                    \"subWorkflowParam\": {\n                        \"name\": \"image_convert_resize\",\n                        \"version\": 1\n                    }\n                }],\n                [{\n                        \"name\": \"image_convert_resize_sub\",\n                        \"taskReferenceName\": \"subworkflow_webp_ref\",\n                        \"inputParameters\": {\n                            \"fileLocation\": \"${workflow.input.fileLocation}\",\n                            \"recipeParameters\": {\n                                \"outputSize\": {\n                                    \"width\": \"${workflow.input.recipeParameters.outputSize.width}\",\n                                    \"height\": \"${workflow.input.recipeParameters.outputSize.height}\"\n                                },\n                                \"outputFormat\": \"webp\"\n                            }\n                        },\n                        \"type\": \"SUB_WORKFLOW\",\n                        \"subWorkflowParam\": {\n                            \"name\": \"image_convert_resize\",\n                            \"version\": 1\n                        }\n                    }\n\n                ]\n            ]\n        },\n        {\n            \"name\": \"image_convert_resize_multipleformat_join\",\n            \"taskReferenceName\": \"image_convert_resize_multipleformat_join_ref\",\n            \"inputParameters\": {},\n            \"type\": \"JOIN\",\n            \"decisionCases\": {},\n            \"defaultCase\": [],\n            \"forkTasks\": [],\n            \"startDelay\": 0,\n            \"joinOn\": [\n                \"subworkflow_jpg_ref\",\n                \"upload_toS3_webp_ref\"\n            ],\n            \"optional\": false,\n            \"defaultExclusiveJoinTask\": [],\n            \"asyncComplete\": false,\n            \"loopOver\": []\n        }\n    ],\n    \"inputParameters\": [],\n    \"outputParameters\": {\n        \"fileLocationJpg\": \"${subworkflow_jpg_ref.output.fileLocation}\",\n        \"fileLocationWebp\": \"${subworkflow_webp_ref.output.fileLocation}\"\n    },\n    \"schemaVersion\": 2,\n    \"restartable\": true,\n    \"workflowStatusListenerEnabled\": true,\n    \"ownerEmail\": \"conductor@example.com\",\n    \"timeoutPolicy\": \"ALERT_ONLY\",\n    \"timeoutSeconds\": 0,\n    \"variables\": {},\n    \"inputTemplate\": {}\n}\n</code></pre> <p>Now our diagram will appear as: </p> <p>The inputs to both sides of the workflow are identical before and after - but we've abstracted the tasks into the sub-workflow. Any change to the sub-workflow will automatically occur in bth sides of the fork.</p> <p>Looking at the subworkflow (the WEBP version):</p> <pre><code>{\n    \"name\": \"image_convert_resize_sub\",\n    \"taskReferenceName\": \"subworkflow_webp_ref\",\n    \"inputParameters\": {\n        \"fileLocation\": \"${workflow.input.fileLocation}\",\n        \"recipeParameters\": {\n            \"outputSize\": {\n                \"width\": \"${workflow.input.recipeParameters.outputSize.width}\",\n                \"height\": \"${workflow.input.recipeParameters.outputSize.height}\"\n            },\n            \"outputFormat\": \"webp\"\n        }\n    },\n    \"type\": \"SUB_WORKFLOW\",\n    \"subWorkflowParam\": {\n        \"name\": \"image_convert_resize\",\n        \"version\": 1\n    }\n}\n</code></pre> <p>The <code>subWorkflowParam</code> tells conductor which workflow to call. The task is marked as completed upon the completion of the spawned workflow.  If the sub-workflow is terminated or fails the task is marked as failure and retried if configured. </p>"},{"location":"reference/operators/sub-workflow-task.html#optional-sub-workflow-task","title":"Optional Sub Workflow Task","text":"<p>If the Sub Workflow task is defined as optional in the parent workflow task definition, the parent workflow task will not be retried if sub-workflow is terminated or failed. In addition, even if the sub-workflow is retried/rerun/restarted after reaching to a terminal status, the parent workflow task status will remain as it is.</p>"},{"location":"reference/operators/switch-task.html","title":"Switch","text":"<pre><code>\"type\" : \"SWITCH\"\n</code></pre> <p>A <code>SWITCH</code> task is similar to the <code>switch...case</code> statement in a programming language. Two evaluators are supported. In either case, the <code>expression</code> is evaluated (this could be a simply a referefence to <code>inputParameters</code>, or a more complex Javascript expression), and the appropriate task is executed based on the output of the <code>expression</code>, and the <code>decisionCases</code> defined in the task configuration.</p>"},{"location":"reference/operators/switch-task.html#use-cases","title":"Use Cases","text":"<p>Useful in any situation where we have to execute one of many task options.</p>"},{"location":"reference/operators/switch-task.html#configuration","title":"Configuration","text":"<p>The following parameters are specified at the top level of the task configuration.</p> name type description evaluatorType String (enum) Type of the evaluator used. Supported types: <code>value-param</code>, <code>javascript</code>. expression String Depends on the evaluatorType decisionCases Map[String, List[task]] Map where the keys are the possible values that can result from <code>expression</code> being evaluated by <code>evaluatorType</code> with values being lists of tasks to be executed. defaultCase List[task] List of tasks to be executed when no matching value if found in <code>decisionCases</code> <p>Note: If the evaluation type is <code>value-param</code>, <code>inputParameters</code> must be populated with the key specified in <code>expression</code>. </p>"},{"location":"reference/operators/switch-task.html#evaluator-types-and-expression","title":"Evaluator Types and Expression","text":"evaluatorType expression description value-param String (name of key) Reference to provided key in <code>inputParameters</code> javascript String (JavaScript expression) Evaluate JavaScript expressions and compute value"},{"location":"reference/operators/switch-task.html#output","title":"Output","text":"name type description evaluationResult List[String] A List of string representing the list of cases that matched."},{"location":"reference/operators/switch-task.html#examples","title":"Examples","text":"<p>In this example workflow, we have to ship a package with the shipping service providers on the basis of input provided while running the workflow.</p> <p>Let's create a Workflow with the following switch task definition that uses <code>value-param</code> evaluatorType:</p> <pre><code>{\n  \"name\": \"switch_task\",\n  \"taskReferenceName\": \"switch_task\",\n  \"inputParameters\": {\n    \"switchCaseValue\": \"${workflow.input.service}\"\n  },\n  \"type\": \"SWITCH\",\n  \"evaluatorType\": \"value-param\",\n  \"expression\": \"switchCaseValue\",\n  \"defaultCase\": [\n    {\n      ...\n    }\n  ],\n  \"decisionCases\": {\n    \"fedex\": [\n      {\n        ...\n      }\n    ],\n    \"ups\": [\n      {\n        ...\n      }\n    ]\n  }\n}\n</code></pre> <p>In the definition above the value of the parameter <code>switch_case_value</code> is used to determine the switch-case. The evaluator type is <code>value-param</code> and the expression is a direct reference to the name of an input parameter. If the value of <code>switch_case_value</code> is <code>fedex</code> then the decision case <code>ship_via_fedex</code>is executed as shown below.</p> <p></p> <p>In a similar way - if the input was <code>ups</code>, then <code>ship_via_ups</code> will be executed. If none of the cases match then the default option is executed.</p> <p>Here is an example using the <code>javascript</code> evaluator type:</p> <pre><code>{\n  \"name\": \"switch_task\",\n  \"taskReferenceName\": \"switch_task\",\n  \"inputParameters\": {\n    \"inputValue\": \"${workflow.input.service}\"\n  },\n  \"type\": \"SWITCH\",\n  \"evaluatorType\": \"javascript\",\n  \"expression\": \"$.inputValue == 'fedex' ? 'fedex' : 'ups'\",\n  \"defaultCase\": [\n    {\n      ...\n    }\n  ],\n  \"decisionCases\": {\n    \"fedex\": [\n      {\n        ...\n      }\n    ],\n    \"ups\": [\n      {\n        ...\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"reference/operators/terminate-task.html","title":"Terminate","text":"<pre><code>\"type\" : \"TERMINATE\"\n</code></pre> <p>The <code>TERMINATE</code> task can terminate the workflow with a given status and set the workflow's output with the provided value.  It can act as a <code>return</code> statement for conditions where you simply want to terminate your workflow. </p>"},{"location":"reference/operators/terminate-task.html#use-cases","title":"Use Cases","text":"<p>Use it when you want to terminate the workflow without continuing the execution. For example, if you have a decision where the first condition is met, you want to execute some tasks,  otherwise you want to finish your workflow.</p>"},{"location":"reference/operators/terminate-task.html#configuration","title":"Configuration","text":"<p>The <code>TERMINATE</code> task is configured with the following attributes inside <code>inputParameters</code>.</p>"},{"location":"reference/operators/terminate-task.html#inputparameters","title":"inputParameters","text":"name type description notes terminationStatus String Either <code>COMPLETED</code> or <code>FAILED</code> required workflowOutput Any Workflow output to be set terminationReason String For failed tasks, this reason is recorded and passed to any configured <code>failureWorkflow</code>"},{"location":"reference/operators/terminate-task.html#output","title":"Output","text":"name type description output Map The content of <code>workflowOutput</code> from the inputParameters. An empty object if <code>workflowOutput</code> is not set."},{"location":"reference/operators/terminate-task.html#examples","title":"Examples","text":""},{"location":"reference/operators/terminate-task.html#basic-example","title":"Basic Example","text":"<p>Terminate task is defined directly inside the workflow with type <code>TERMINATE</code>.</p> <pre><code>{\n  \"name\": \"terminate\",\n  \"taskReferenceName\": \"terminate0\",\n  \"inputParameters\": {\n      \"terminationStatus\": \"COMPLETED\",\n      \"workflowOutput\": \"${task0.output}\"\n  },\n  \"type\": \"TERMINATE\",\n  \"startDelay\": 0,\n  \"optional\": false\n}\n</code></pre>"},{"location":"reference/operators/terminate-task.html#use-with-switch","title":"Use with SWITCH","text":"<p>Let's consider the same example we had in Switch Task.</p> <p>Suppose in a workflow, we have to take decision to ship the courier with the shipping service providers on the basis of input provided while running the workflow. If the input provided while running workflow does not match with the available shipping providers then the workflow will fail and return. If input provided  matches then it goes ahead.</p> <p>Here is a snippet that shows the default switch case terminating the workflow:</p> <pre><code>{\n  \"name\": \"switch_task\",\n  \"taskReferenceName\": \"switch_task\",\n  \"type\": \"SWITCH\",\n  \"defaultCase\": [\n      {\n      \"name\": \"terminate\",\n      \"taskReferenceName\": \"terminate\",\n      \"type\": \"TERMINATE\",\n      \"inputParameters\": {\n          \"terminationStatus\": \"FAILED\",\n          \"terminationReason\":\"Shipping provider not found.\"\n      }      \n    }\n   ]\n}\n</code></pre> <p>Workflow gets created as shown in the diagram.</p> <p></p>"},{"location":"reference/operators/terminate-task.html#best-practices","title":"Best Practices","text":"<ol> <li>Include termination reason when terminating the workflow with failure status to make it easy to understand the cause.</li> <li>Include any additional details (e.g. output of the tasks, switch case etc) that helps understand the path taken to termination.</li> </ol>"},{"location":"reference/operators/wait-task.html","title":"Wait","text":"<p>The WAIT task is a no-op task that will remain <code>IN_PROGRESS</code> until after a certain duration or timestamp, at which point it will be marked as <code>COMPLETED</code>.</p> <pre><code>\"type\" : \"WAIT\"\n</code></pre>"},{"location":"reference/operators/wait-task.html#use-cases","title":"Use Cases","text":"<p>WAIT is used when the workflow needs to wait and pause for an external signal such as a human intervention  (like manual approval) or an event coming from external source such as Kafka, SQS or Conductor's internal queueing mechanism.</p> <p>Some use cases where WAIT task is used:</p> <ol> <li>Wait for a certain amount of time (e.g. 2 minutes) or until a certain date time (e.g. 12/25/2022 00:00)</li> <li>To wait for and external signal coming from an event queue mechanism supported by Conductor</li> </ol>"},{"location":"reference/operators/wait-task.html#configuration","title":"Configuration","text":"<p>The <code>WAIT</code> task is configured using either <code>duration</code> or <code>until</code> in <code>inputParameters</code>.</p>"},{"location":"reference/operators/wait-task.html#inputparameters","title":"inputParameters","text":"name type description duration String Duration to wait for until String Timestamp to wait until"},{"location":"reference/operators/wait-task.html#wait-for-time-duration","title":"Wait For time duration","text":"<p>Format duration as <code>XhYmZs</code>, using the <code>duration</code> key.</p> <pre><code>{\n    \"type\": \"WAIT\",\n    \"inputParameters\": {\n        \"duration\": \"10m20s\"\n    }\n}\n</code></pre>"},{"location":"reference/operators/wait-task.html#wait-until-specific-datetime","title":"Wait until specific date/time","text":"<p>Specify the timestamp using one of the formats, using the <code>until</code> key.</p> <ol> <li><code>yyyy-MM-dd HH:mm</code></li> <li><code>yyyy-MM-dd HH:mm z</code></li> <li><code>yyyy-MM-dd</code></li> </ol> <pre><code>{\n    \"type\": \"WAIT\",\n    \"inputParameters\": {\n        \"until\": \"2022-12-31 11:59\"\n    }\n}\n</code></pre>"},{"location":"reference/operators/wait-task.html#external-triggers","title":"External Triggers","text":"<p>The task endpoint <code>POST /api/tasks</code> can be used to update the status of a task to COMPLETED prior to the configured timeout. This is same technique as prescribed for the HUMAN task.</p> <p>For cases where no timeout is necessary it is recommended that you use the HUMAN task directly.</p>"},{"location":"reference/systemtasks/index.html","title":"System Tasks","text":"<p>System Tasks (Workers) are built-in tasks that are general purpose and re-usable. They run on the Conductor servers. Such tasks allow you to get started without having to write custom workers.</p> Task Description Use Case Event Publishing Event Task External eventing system integration. e.g. amqp, sqs, nats HTTP HTTP Task Invoke any HTTP(S) endpoints Inline Code Execution Inline Task Execute arbitrary lightweight javascript code JQ Transform JQ Task Use JQ to transform task input/output Kafka Publish Kafka Task Publish messages to Kafka"},{"location":"reference/systemtasks/event-task.html","title":"Event Task","text":"<pre><code>\"type\" : \"EVENT\"\n</code></pre> <p>The <code>EVENT</code> task is a task used to publish an event into one of the supported eventing systems in Conductor.</p>"},{"location":"reference/systemtasks/event-task.html#use-cases","title":"Use Cases","text":"<p>Consider a use case where at some point in the execution, an event is published to an external eventing system such as SQS. Event tasks are useful for creating event based dependencies for workflows and tasks.</p>"},{"location":"reference/systemtasks/event-task.html#supported-queuing-systems","title":"Supported Queuing Systems","text":"<p>Conductor supports the the following eventing models:</p> <ol> <li>Conductor internal events (prefix: <code>conductor</code>)</li> <li>SQS (prefix: <code>sqs</code>)</li> <li>NATS (prefix: <code>nats</code>)</li> <li>AMQP (prefix: <code>amqp_queue or amqp_exchange</code>)</li> </ol>"},{"location":"reference/systemtasks/event-task.html#configuration","title":"Configuration","text":"<p>The following parameters are specified at the top level of the task configuration.</p> Attribute Description sink External event queue in the format of <code>prefix:location</code>.  Prefix is either <code>sqs</code> or <code>conductor</code> and <code>location</code> specifies the actual queue name. e.g. <code>sqs:send_email_queue</code> asyncComplete Boolean. See below"},{"location":"reference/systemtasks/event-task.html#asynccomplete","title":"asyncComplete","text":"<ul> <li><code>false</code> to mark status COMPLETED upon execution </li> <li><code>true</code> to keep it IN_PROGRESS, wait for an external event (via Conductor or SQS or EventHandler) to complete it. </li> </ul>"},{"location":"reference/systemtasks/event-task.html#conductor-sink","title":"Conductor sink","text":"<p>When producing an event with Conductor as sink, the event name follows the structure: <code>conductor:&lt;workflow_name&gt;:&lt;task_reference_name&gt;</code></p> <p>When using Conductor as sink, you have two options: defining the sink as <code>conductor</code> in which case the queue name will default to the taskReferenceName of the Event Task, or specifying the queue name in the sink, as <code>conductor:&lt;queue_name&gt;</code>. The queue name is in the <code>event</code> value of the event Handler, as <code>conductor:&lt;workflow_name&gt;:&lt;queue_name&gt;</code>.</p>"},{"location":"reference/systemtasks/event-task.html#sqs-sink","title":"SQS sink","text":"<p>For SQS, use the name of the queue and NOT the URI.  Conductor looks up the URI based on the name.</p> <p>Warning</p> <p>When using SQS add the ContribsModule to the deployment.  The module needs to be configured with AWSCredentialsProvider for Conductor to be able to use AWS APIs.</p>"},{"location":"reference/systemtasks/event-task.html#output","title":"Output","text":"<p>Tasks's output are sent as a payload to the external event. In case of SQS the task's output is sent to the SQS message a a payload.</p> name type description workflowInstanceId String Workflow id workflowType String Workflow Name workflowVersion Integer Workflow Version correlationId String Workflow CorrelationId sink String Copy of the input data \"sink\" asyncComplete Boolean Copy of the input data \"asyncComplete event_produced String Name of the event produced <p>The published event's payload is identical to the output of the task (except \"event_produced\").</p>"},{"location":"reference/systemtasks/event-task.html#examples","title":"Examples","text":"<p>Consider an example where we want to publish an event into SQS to notify an external system. </p> <pre><code>{\n    \"type\": \"EVENT\",\n    \"sink\": \"sqs:sqs_queue_name\",\n    \"asyncComplete\": false\n}\n</code></pre> <p>An example where we want to publish a messase to conductor's internal queuing system. <pre><code>{\n    \"type\": \"EVENT\",\n    \"sink\": \"conductor:internal_event_name\",\n    \"asyncComplete\": false\n}\n</code></pre></p>"},{"location":"reference/systemtasks/http-task.html","title":"HTTP Task","text":"<pre><code>\"type\" : \"HTTP\"\n</code></pre> <p>The <code>HTTP</code> task is useful when you have a requirements such as:</p> <ol> <li>Making calls to another service that exposes an API via HTTP</li> <li>Fetch any resource or data present on an endpoint</li> </ol> <p>The <code>HTTP</code> task is moved to <code>COMPLETED</code> status once the remote service responds successfully.</p>"},{"location":"reference/systemtasks/http-task.html#use-cases","title":"Use Cases","text":"<p>If we have a scenario where we need to make an HTTP call into another service, we can make use of HTTP tasks. You can use the data returned from the HTTP call in your subsequent tasks as inputs. Using HTTP tasks you can avoid having to write the code that talks to these services and instead let Conductor manage it directly. This can reduce the code you have to maintain and allows for a lot of flexibility.</p>"},{"location":"reference/systemtasks/http-task.html#configuration","title":"Configuration","text":"<p>HTTP task is configured using the following key inside the <code>inputParameters</code>  of a task with type <code>HTTP</code>.</p>"},{"location":"reference/systemtasks/http-task.html#inputparameters","title":"inputParameters","text":"name type description http_request HttpRequest JSON object (see below)"},{"location":"reference/systemtasks/http-task.html#http_request","title":"http_request","text":"Name Type Description uri String URI for the service. Can be a partial when using vipAddress or includes the server address. method String HTTP method. GET, PUT, POST, DELETE, OPTIONS, HEAD accept String Accept header. Default:  <code>application/json</code> contentType String Content Type - supported types are <code>text/plain</code>, <code>text/html</code>, and <code>application/json</code> (Default) headers Map[String, Any] A map of additional http headers to be sent along with the request. body Map[] Request body asyncComplete Boolean <code>false</code> to mark status COMPLETED upon execution ; <code>true</code> to keep it IN_PROGRESS, wait for an external event (via Conductor or SQS or EventHandler) to complete it. connectionTimeOut Integer Connection Time Out in milliseconds. If set to 0, equivalent to infinity. Default: 100. readTimeOut Integer Read Time Out in milliseconds. If set to 0, equivalent to infinity. Default: 150. <p>Tip</p> <p>In the case that remote service sends an asynchronous event to signal the completion of the request, consider setting the <code>asyncComplete</code> flag on the HTTP task to <code>true</code>. In this case, you will need to transition the HTTP task to COMPLETED manually.</p> <p>Tip</p> <p>If the remote address that you are connecting to is a secure location, add the Authorization header with <code>Bearer &lt;access_token&gt;</code> to headers.</p>"},{"location":"reference/systemtasks/http-task.html#output","title":"Output","text":"name type description response Map JSON body containing the response if one is present headers Map[String, Any] Response Headers statusCode Integer Http Status Code reasonPhrase String Http Status Code's reason phrase"},{"location":"reference/systemtasks/http-task.html#examples","title":"Examples","text":""},{"location":"reference/systemtasks/http-task.html#get-method","title":"GET Method","text":"<p>We can use variables in our URI as show in the example below. </p> <pre><code>{\n  \"name\": \"Get Example\",\n  \"taskReferenceName\": \"get_example\",\n  \"inputParameters\": {\n    \"http_request\": {\n      \"uri\": \"https://jsonplaceholder.typicode.com/posts/${workflow.input.queryid}\",\n      \"method\": \"GET\"\n    }\n  },\n  \"type\": \"HTTP\"\n}\n</code></pre>"},{"location":"reference/systemtasks/http-task.html#post-method","title":"POST Method","text":"<p>Here we are using variables for our POST body which happens to be data from a previous task. This is an example of how you can chain HTTP calls to make complex flows happen without writing any additional code.</p> <pre><code>{\n  \"name\": \"http_post_example\",\n  \"taskReferenceName\": \"post_example\",\n  \"inputParameters\": {\n    \"http_request\": {\n      \"uri\": \"https://jsonplaceholder.typicode.com/posts/\",\n      \"method\": \"POST\",\n      \"body\": {\n        \"title\": \"${get_example.output.response.body.title}\",\n        \"userId\": \"${get_example.output.response.body.userId}\",\n        \"action\": \"doSomething\"\n      }\n    }\n  },\n  \"type\": \"HTTP\"\n}\n</code></pre>"},{"location":"reference/systemtasks/http-task.html#put-method","title":"PUT Method","text":"<pre><code>{\n  \"name\": \"http_put_example\",\n  \"taskReferenceName\": \"put_example\",\n  \"inputParameters\": {\n    \"http_request\": {\n      \"uri\": \"https://jsonplaceholder.typicode.com/posts/1\",\n      \"method\": \"PUT\",\n      \"body\": {\n        \"title\": \"${get_example.output.response.body.title}\",\n        \"userId\": \"${get_example.output.response.body.userId}\",\n        \"action\": \"doSomethingDifferent\"\n      }\n    }\n  },\n  \"type\": \"HTTP\"\n}\n</code></pre>"},{"location":"reference/systemtasks/http-task.html#delete-method","title":"DELETE Method","text":"<pre><code>{\n  \"name\": \"DELETE Example\",\n  \"taskReferenceName\": \"delete_example\",\n  \"inputParameters\": {\n    \"http_request\": {\n      \"uri\": \"https://jsonplaceholder.typicode.com/posts/1\",\n      \"method\": \"DELETE\"\n    }\n  },\n  \"type\": \"HTTP\"\n}\n</code></pre>"},{"location":"reference/systemtasks/http-task.html#isolation-groups","title":"Isolation Groups","text":"<p>Why are my HTTP tasks not getting picked up?</p> <p>We might have too many HTTP tasks in the queue. There is a concept called Isolation Groups that you can rely on for prioritizing certain HTTP tasks over others. </p>"},{"location":"reference/systemtasks/human-task.html","title":"Human Task","text":"<pre><code>\"type\" : \"HUMAN\"\n</code></pre> <p>The <code>HUMAN</code> task is used when the workflow needs to be paused for an external signal to continue. It acts as a gate that  remains in the <code>IN_PROGRESS</code> state until marked as <code>COMPLETED</code> or <code>FAILED</code> by an external trigger.</p>"},{"location":"reference/systemtasks/human-task.html#use-cases","title":"Use Cases","text":"<p>The HUMAN is can be used when the workflow needs to pause and wait for human intervention, such as manual approval. It can also be used with an event coming from external source such as Kafka, SQS or Conductor's internal queueing mechanism.</p>"},{"location":"reference/systemtasks/human-task.html#configuration","title":"Configuration","text":"<p>No parameters are required</p>"},{"location":"reference/systemtasks/human-task.html#completing","title":"Completing","text":""},{"location":"reference/systemtasks/human-task.html#task-update-api","title":"Task Update API","text":"<p>To conclude a <code>HUMAN</code> task, the <code>POST /api/tasks</code> API can be used.</p> <p>You'll need to provide the<code>taskId</code>, the task status (generally <code>COMPLETED</code> or <code>FAILED</code>), and the desired task output.</p>"},{"location":"reference/systemtasks/human-task.html#event-handler","title":"Event Handler","text":"<p>If SQS integration is enabled, the <code>HUMAN</code> task can also be resolved using the <code>/api/queue</code> API.</p> <p>You'll need the  <code>workflowId</code> and <code>taskRefName</code> or <code>taskId</code>.</p> <ol> <li>POST <code>/api/queue/update/{workflowId}/{taskRefName}/{status}</code> </li> <li>POST <code>/api/queue/update/{workflowId}/task/{taskId}/{status}</code> </li> </ol> <p>An event handler using the <code>complete_task</code> action can also be configured.</p> <p>Any parameter that is sent in the body of the POST message will be repeated as the output of the task.  For example, if we send a COMPLETED message as follows:</p> <pre><code>curl -X \"POST\" \"http://localhost:8080/api/queue/update/{workflowId}/waiting_around_ref/COMPLETED\" -H 'Content-Type: application/json' -d '{\"data_key\":\"somedatatoWait1\",\"data_key2\":\"somedatatoWAit2\"}'\n</code></pre> <p>The output of the task will be:</p> <pre><code>{\n  \"data_key\":\"somedatatoWait1\",\n  \"data_key2\":\"somedatatoWAit2\"\n}\n</code></pre>"},{"location":"reference/systemtasks/inline-task.html","title":"Inline Task","text":"<pre><code>\"type\": \"INLINE\"\n</code></pre> <p>The <code>INLINE</code> task helps execute necessary logic at workflow runtime, using an evaluator. There are two supported evaluators as of now:</p>"},{"location":"reference/systemtasks/inline-task.html#configuration","title":"Configuration","text":"<p>The <code>INLINE</code> task is configured by specifying the following keys inside <code>inputParameters</code>, along side any other input values required for the evaluation.</p>"},{"location":"reference/systemtasks/inline-task.html#inputparameters","title":"inputParameters","text":"Name Type Description Notes evaluatorType String Type of the evaluator. Supported evaluators: <code>value-param</code>, <code>javascript</code> which evaluates javascript expression. Must be non-empty. expression String Expression associated with the type of evaluator. For <code>javascript</code> evaluator, Javascript evaluation engine is used to evaluate expression defined as a string. Must return a value. Must be non-empty. <p>Besides <code>expression</code>, any value is accessible as <code>$.value</code> for the <code>expression</code> to evaluate.</p>"},{"location":"reference/systemtasks/inline-task.html#outputs","title":"Outputs","text":"Name Type Description result Map Contains the output returned by the evaluator based on the <code>expression</code>"},{"location":"reference/systemtasks/inline-task.html#examples","title":"Examples","text":""},{"location":"reference/systemtasks/inline-task.html#example-1","title":"Example 1","text":"<pre><code>{\n  \"name\": \"INLINE_TASK\",\n  \"taskReferenceName\": \"inline_test\",\n  \"type\": \"INLINE\",\n  \"inputParameters\": {\n      \"inlineValue\": \"${workflow.input.inlineValue}\",\n      \"evaluatorType\": \"javascript\",\n      \"expression\": \"function scriptFun(){if ($.inlineValue == 1){ return {testvalue: true} } else { return\n      {testvalue: false} }} scriptFun();\"\n  }\n}\n</code></pre> <p>The task output can then be referenced in downstream tasks using an expression: <code>\"${inline_test.output.result.testvalue}\"</code></p> <p>Note</p> <p>The JavaScript evaluator accepts JS code written to the ECMAScript 5.1(ES5) standard</p>"},{"location":"reference/systemtasks/inline-task.html#example-2","title":"Example 2","text":"<p>Perhaps a weather API sometimes returns Celcius, and sometimes returns Farenheit temperature values.  This task ensures that the downstream tasks ONLY receive Celcius values:</p> <pre><code>{\n  \"name\": \"INLINE_TASK\",\n  \"taskReferenceName\": \"inline_test\",\n  \"type\": \"INLINE\",\n  \"inputParameters\": {\n      \"scale\": \"${workflow.input.tempScale}\",\n        \"temperature\": \"${workflow.input.temperature}\",\n      \"evaluatorType\": \"javascript\",\n      \"expression\": \"function SIvaluesOnly(){if ($.scale === \"F\"){ centigrade = ($.temperature -32)*5/9; return {temperature: centigrade} } else { return \n      {temperature: $.temperature} }} SIvaluesOnly();\"\n  }\n}\n</code></pre>"},{"location":"reference/systemtasks/json-jq-transform-task.html","title":"JSON JQ Transform Task","text":"<pre><code>\"type\" : \"JSON_JQ_TRANSFORM\"\n</code></pre> <p><code>JSON_JQ_TRANSFORM</code> is a System task that allows processing of JSON data that is supplied to the task, by using the popular JQ processing tool\u2019s query expression language.</p>"},{"location":"reference/systemtasks/json-jq-transform-task.html#use-cases","title":"Use Cases","text":"<p>JSON is a popular format of choice for data-interchange. It is widely used in web and server applications, document storage, API I/O etc. It\u2019s also used within Conductor to define workflow and task definitions and passing data and state between tasks and workflows. This makes a tool like JQ a natural fit for processing task related data. Some common usages within Conductor includes, working with HTTP task, JOIN tasks or standalone tasks that try to transform data from the output of one task to the input of another.</p>"},{"location":"reference/systemtasks/json-jq-transform-task.html#configuration","title":"Configuration","text":"<p><code>queryExpression</code> is appended to the <code>inputParameters</code> of <code>JSON_JQ_TRANSFORM</code>, along side any other input values needed for the evaluation.</p>"},{"location":"reference/systemtasks/json-jq-transform-task.html#inputparameters","title":"inputParameters","text":"name description queryExpression JQ query expression"},{"location":"reference/systemtasks/json-jq-transform-task.html#about-jq","title":"About JQ","text":"<p>Check out the JQ Manual.</p>"},{"location":"reference/systemtasks/json-jq-transform-task.html#output","title":"Output","text":"Attribute Description result The first results returned by the JQ expression resultList A List of results returned by the JQ expression error An optional error message, indicating that the JQ query failed processing"},{"location":"reference/systemtasks/json-jq-transform-task.html#example","title":"Example","text":""},{"location":"reference/systemtasks/json-jq-transform-task.html#example-1","title":"Example 1","text":"<p>Here is an example of a <code>JSON_JQ_TRANSFORM</code> task. The <code>inputParameters</code> attribute is expected to have a value object that has the following</p> <ol> <li> <p>A list of key value pair objects denoted key1/value1, key2/value2 in the example below. Note the key1/value1 are    arbitrary names used in this example.</p> </li> <li> <p>A key with the name <code>queryExpression</code>, whose value is a JQ expression. The expression will operate on the value of    the <code>inputParameters</code> attribute. In the example below, the <code>inputParameters</code> has 2 inner objects named by attributes    <code>key1</code> and <code>key2</code>, each of which has an object that is named <code>value1</code> and <code>value2</code>. They have an associated array of    strings as values, <code>\"a\", \"b\"</code> and <code>\"c\", \"d\"</code>. The expression <code>key3: (.key1.value1 + .key2.value2)</code> concat's the 2    string arrays into a single array against an attribute named <code>key3</code></p> </li> </ol> <pre><code>{\n  \"name\": \"jq_example_task\",\n  \"taskReferenceName\": \"my_jq_example_task\",\n  \"type\": \"JSON_JQ_TRANSFORM\",\n  \"inputParameters\": {\n    \"key1\": {\n      \"value1\": [\n        \"a\",\n        \"b\"\n      ]\n    },\n    \"key2\": {\n      \"value2\": [\n        \"c\",\n        \"d\"\n      ]\n    },\n    \"queryExpression\": \"{ key3: (.key1.value1 + .key2.value2) }\"\n  }\n}\n</code></pre> <p>The execution of this example task above will provide the following output. The <code>resultList</code> attribute stores the full list of the <code>queryExpression</code> result. The <code>result</code> attribute stores the first element of the resultList. An optional <code>error</code> attribute along with a string message will be returned if there was an error processing the query expression.</p> <pre><code>{\n  \"result\": {\n    \"key3\": [\n      \"a\",\n      \"b\",\n      \"c\",\n      \"d\"\n    ]\n  },\n  \"resultList\": [\n    {\n      \"key3\": [\n        \"a\",\n        \"b\",\n        \"c\",\n        \"d\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"reference/systemtasks/json-jq-transform-task.html#example-2","title":"Example 2","text":"<p>A HTTP Task makes an API call to GitHub to request a list of \"stargazers\" (users who have starred a repository).  The API response (for just one user) looks like:</p> <p>Snippet of <code>${hundred_stargazers_ref.output}</code></p> <pre><code>\"body\":[\n  {\n  \"starred_at\":\"2016-12-14T19:55:46Z\",\n  \"user\":{\n    \"login\":\"lzehrung\",\n    \"id\":924226,\n    \"node_id\":\"MDQ6VXNlcjkyNDIyNg==\",\n    \"avatar_url\":\"https://avatars.githubusercontent.com/u/924226?v=4\",\n    \"gravatar_id\":\"\",\n    \"url\":\"https://api.github.com/users/lzehrung\",\n    \"html_url\":\"https://github.com/lzehrung\",\n    \"followers_url\":\"https://api.github.com/users/lzehrung/followers\",\n    \"following_url\":\"https://api.github.com/users/lzehrung/following{/other_user}\",\n    \"gists_url\":\"https://api.github.com/users/lzehrung/gists{/gist_id}\",\n    \"starred_url\":\"https://api.github.com/users/lzehrung/starred{/owner}{/repo}\",\n    \"subscriptions_url\":\"https://api.github.com/users/lzehrung/subscriptions\",\n    \"organizations_url\":\"https://api.github.com/users/lzehrung/orgs\",\n    \"repos_url\":\"https://api.github.com/users/lzehrung/repos\",\n    \"events_url\":\"https://api.github.com/users/lzehrung/events{/privacy}\",\n    \"received_events_url\":\"https://api.github.com/users/lzehrung/received_events\",\n    \"type\":\"User\",\n    \"site_admin\":false\n  }\n}\n]\n</code></pre> <p>We only need the <code>starred_at</code> and <code>login</code> parameters for users who starred the repository AFTER a given date (provided as an input to the workflow <code>${workflow.input.cutoff_date}</code>).  We'll use the JQ Transform to simplify the output:</p> <pre><code>{\n  \"name\": \"jq_cleanup_stars\",\n  \"taskReferenceName\": \"jq_cleanup_stars_ref\",\n  \"inputParameters\": {\n    \"starlist\": \"${hundred_stargazers_ref.output.response.body}\",\n    \"queryExpression\": \"[.starlist[] | select (.starred_at &gt; \\\"${workflow.input.cutoff_date}\\\") |{occurred_at:.starred_at, member: {github:  .user.login}}]\"\n  },\n  \"type\": \"JSON_JQ_TRANSFORM\",\n  \"decisionCases\": {},\n  \"defaultCase\": [],\n  \"forkTasks\": [],\n  \"startDelay\": 0,\n  \"joinOn\": [],\n  \"optional\": false,\n  \"defaultExclusiveJoinTask\": [],\n  \"asyncComplete\": false,\n  \"loopOver\": []\n}\n</code></pre> <p>The JSON is stored in <code>starlist</code>.  The <code>queryExpression</code> reads in the JSON, selects only entries where the <code>starred_at</code> value meets the date criteria, and generates output JSON of the form:</p> <pre><code>{\n  \"occurred_at\": \"date from JSON\",\n  \"member\":{\n    \"github\" : \"github Login from JSON\"\n  }\n}\n</code></pre> <p>The entire expression is wrapped in <code>[]</code> to indicate that the response should be an array.</p>"},{"location":"reference/systemtasks/kafka-publish-task.html","title":"Kafka Publish Task","text":"<p><pre><code>\"type\" : \"KAFKA_PUBLISH\"\n</code></pre> The <code>KAFKA_PUBLISH</code> task is used to push messages to another microservice via Kafka.</p>"},{"location":"reference/systemtasks/kafka-publish-task.html#configuration","title":"Configuration","text":"<p>The task expects a field named <code>kafka_request</code> as part of the task's <code>inputParameters</code>.</p>"},{"location":"reference/systemtasks/kafka-publish-task.html#inputparameters","title":"inputParameters","text":"name description bootStrapServers bootStrapServers for connecting to given kafka. key Key to be published keySerializer Serializer used for serializing the key published to kafka.  One of the following can be set : 1. <code>org.apache.kafka.common.serialization.IntegerSerializer</code>2. <code>org.apache.kafka.common.serialization.LongSerializer</code>3. <code>org.apache.kafka.common.serialization.StringSerializer</code>. Default is <code>StringSerializer</code> value Value published to kafka requestTimeoutMs Request timeout while publishing to kafka. If this value is not given the value is read from the property <code>kafka.publish.request.timeout.ms</code>. If the property is not set the value defaults to 100 ms maxBlockMs maxBlockMs while publishing to kafka. If this value is not given the value is read from the property <code>kafka.publish.max.block.ms</code>. If the property is not set the value defaults to 500 ms headers A map of additional kafka headers to be sent along with the request. topic Topic to publish"},{"location":"reference/systemtasks/kafka-publish-task.html#task-output","title":"Task Output","text":"<p>Task status transitions to <code>COMPLETED</code> on success.</p> <p>The task is marked as <code>FAILED</code> if the message could not be published to the Kafka queue.</p>"},{"location":"reference/systemtasks/kafka-publish-task.html#example","title":"Example","text":"<pre><code>{\n  \"name\": \"call_kafka\",\n  \"taskReferenceName\": \"call_kafka\",\n  \"inputParameters\": {\n    \"kafka_request\": {\n      \"topic\": \"userTopic\",\n      \"value\": \"Message to publish\",\n      \"bootStrapServers\": \"localhost:9092\",\n      \"headers\": {\n    \"x-Auth\":\"Auth-key\"    \n      },\n      \"key\": \"123\",\n      \"keySerializer\": \"org.apache.kafka.common.serialization.IntegerSerializer\"\n    }\n  },\n  \"type\": \"KAFKA_PUBLISH\"\n}\n</code></pre> <p>The task expects an input parameter named <code>\"kafka_request\"</code> as part of the task's input with the following details:</p> <ol> <li><code>\"bootStrapServers\"</code> - bootStrapServers for connecting to given kafka.</li> <li><code>\"key\"</code> - Key to be published.</li> <li><code>\"keySerializer\"</code> - Serializer used for serializing the key published to kafka.  One of the following can be set : a. org.apache.kafka.common.serialization.IntegerSerializer b. org.apache.kafka.common.serialization.LongSerializer c. org.apache.kafka.common.serialization.StringSerializer. Default is String serializer.</li> <li><code>\"value\"</code> - Value published to kafka</li> <li><code>\"requestTimeoutMs\"</code> - Request timeout while publishing to kafka.  If this value is not given the value is read from the property  kafka.publish.request.timeout.ms. If the property is not set the value defaults to 100 ms.</li> <li><code>\"maxBlockMs\"</code> - maxBlockMs while publishing to kafka. If this value is not given the value is read from the property kafka.publish.max.block.ms. If the property is not set the value defaults to 500 ms.</li> <li><code>\"headers\"</code> - A map of additional kafka headers to be sent along with the request.</li> <li><code>\"topic\"</code> - Topic to publish.</li> </ol> <p>The producer created in the kafka task is cached. By default the cache size is 10 and expiry time is 120000 ms. To change the defaults following can be modified  kafka.publish.producer.cache.size, kafka.publish.producer.cache.time.ms respectively.</p>"},{"location":"resources/code-of-conduct.html","title":"Contributor Covenant Code of Conduct","text":""},{"location":"resources/code-of-conduct.html#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"resources/code-of-conduct.html#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"resources/code-of-conduct.html#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"resources/code-of-conduct.html#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"resources/code-of-conduct.html#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at hello@swiftsoftwaregroup.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"resources/code-of-conduct.html#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p> <p>For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq</p>"},{"location":"resources/contributing.html","title":"Contributing","text":"<p>Thanks for your interest in Conductor! This guide helps to find the most efficient way to contribute, ask questions, and report issues.</p>"},{"location":"resources/contributing.html#code-of-conduct","title":"Code of conduct","text":"<p>Please review our code of conduct.</p>"},{"location":"resources/contributing.html#i-have-a-question","title":"I have a question!","text":"<p>We have a dedicated discussion forum for asking \"how to\" questions and to discuss ideas. The discussion forum is a great place to start if you're considering creating a feature request or work on a Pull Request. Please do not create issues to ask questions.</p>"},{"location":"resources/contributing.html#i-want-to-contribute","title":"I want to contribute!","text":"<p>We welcome Pull Requests and already had many outstanding community contributions! Creating and reviewing Pull Requests take considerable time. This section helps you to set up a smooth Pull Request experience.</p> <p>The stable branch is main.</p> <p>Please create pull requests for your contributions against main only.</p> <p>It's a great idea to discuss the new feature you're considering on the discussion forum before writing any code. There are often different ways you can implement a feature. Getting some discussion about different options helps shape the best solution. When starting directly with a Pull Request, there is the risk of having to make considerable changes. Sometimes that is the best approach, though! Showing an idea with code can be very helpful; be aware that it might be throw-away work. Some of our best Pull Requests came out of multiple competing implementations, which helped shape it to perfection.</p> <p>Also, consider that not every feature is a good fit for Conductor. A few things to consider are:</p> <ul> <li>Is it increasing complexity for the user, or might it be confusing?</li> <li>Does it, in any way, break backward compatibility (this is seldom acceptable)</li> <li>Does it require new dependencies (this is rarely acceptable for core modules)</li> <li>Should the feature be opt-in or enabled by default. For integration with a new Queuing recipe or persistence module, a separate module which can be optionally enabled is the right choice.  </li> <li>Should the feature be implemented in the main Conductor repository, or would it be better to set up a separate repository? Especially for integration with other systems, a separate repository is often the right choice because the life-cycle of it will be different.</li> </ul> <p>Of course, for more minor bug fixes and improvements, the process can be more light-weight.</p> <p>We'll try to be responsive to Pull Requests. Do keep in mind that because of the inherently distributed nature of open source projects, responses to a PR might take some time because of time zones, weekends, and other things we may be working on.</p>"},{"location":"resources/contributing.html#i-want-to-report-an-issue","title":"I want to report an issue","text":"<p>If you found a bug, it is much appreciated if you create an issue. Please include clear instructions on how to reproduce the issue, or even better, include a test case on a branch. Make sure to come up with a descriptive title for the issue because this helps while organizing issues.</p>"},{"location":"resources/contributing.html#i-have-a-great-idea-for-a-new-feature","title":"I have a great idea for a new feature","text":"<p>Many features in Conductor have come from ideas from the community. If you think something is missing or certain use cases could be supported better, let us know! You can do so by opening a discussion on the discussion forum. Provide as much relevant context to why and when the feature would be helpful. Providing context is especially important for \"Support XYZ\" issues since we might not be familiar with what \"XYZ\" is and why it's useful. If you have an idea of how to implement the feature, include that as well.</p> <p>Once we have decided on a direction, it's time to summarize the idea by creating a new issue.</p>"},{"location":"resources/contributing.html#code-style","title":"Code Style","text":"<p>We use spotless to enforce consistent code style for the project, so make sure to run <code>gradlew spotlessApply</code> to fix any violations after code changes.</p>"},{"location":"resources/contributing.html#license","title":"License","text":"<p>By contributing your code, you agree to license your contribution under the terms of the APLv2: https://github.com/swift-conductor/conductor/blob/master/LICENSE</p> <p>All files are released with the Apache 2.0 license, and the following license header will be automatically added to your new file if none present:</p> <pre><code>/**\n * Copyright $YEAR Swift Software Group, Inc.\n * (Code and content before December 13, 2023, Copyright Netflix, Inc.)\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n */\n</code></pre>"},{"location":"resources/license.html","title":"License","text":"<p>Copyright 2023 Swift Software Group, Inc.</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>Copyright Netflix, Inc. (before Dec 13, 2023)</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"resources/related.html","title":"Community projects related to Conductor","text":""},{"location":"resources/related.html#client-sdks","title":"Client SDKs","text":"<ul> <li>Python</li> <li>[Java]https://github.com/swift-conductor/conductor/tree/main/java-sdk)</li> <li>.NET</li> <li>Golang</li> </ul> <p>Contributions on the above SDKs can be made in the corresponding repositories.</p>"},{"location":"resources/related.html#microservices-operations","title":"Microservices operations","text":"<ul> <li> <p>https://github.com/flaviostutz/schellar - Schellar is a scheduler tool for instantiating Conductor workflows from time to time, mostly like a cron job, but with transport of input/output variables between calls.</p> </li> <li> <p>https://github.com/flaviostutz/backtor - Backtor is a backup scheduler tool that uses Conductor workers to handle backup operations and decide when to expire backups (ex.: keep backup 3 days, 2 weeks, 2 months, 1 semester)</p> </li> <li> <p>https://github.com/cquon/conductor-tools - Conductor CLI for launching workflows, polling tasks, listing running tasks etc</p> </li> </ul>"},{"location":"resources/related.html#conductor-deployment","title":"Conductor deployment","text":"<ul> <li> <p>https://github.com/flaviostutz/conductor-server - Docker container for running Conductor with  Prometheus metrics plugin installed and some tweaks to ease provisioning of workflows from json files embedded to the container</p> </li> <li> <p>https://github.com/flaviostutz/conductor-ui - Docker container for running Conductor UI so that you can easily scale UI independently</p> </li> <li> <p>https://github.com/flaviostutz/elasticblast - \"Elasticsearch to Bleve\" bridge tailored for running Conductor on top of Bleve indexer. The footprint of Elasticsearch may cost too much for small deployments on Cloud environment.</p> </li> <li> <p>https://github.com/mohelsaka/conductor-prometheus-metrics - Conductor plugin for exposing Prometheus metrics over path '/metrics'</p> </li> </ul>"},{"location":"resources/related.html#oauth20-security-configuration","title":"OAuth2.0 Security Configuration","text":"<p>Forked Repository - Conductor (Secure)</p> <p>OAuth2.0 Role Based Security! - Spring Security with easy configuration to secure the Conductor server APIs.</p> <p>Docker image published to Docker Hub</p>"},{"location":"resources/related.html#conductor-worker-utilities","title":"Conductor Worker utilities","text":"<ul> <li> <p>https://github.com/ggrcha/conductor-client-golang-client - Conductor Golang client for writing Workers in Golang</p> </li> <li> <p>https://github.com/courosh12/conductor-dotnet-client - Conductor DOTNET client for writing Workers in DOTNET</p> </li> <li> <p>https://github.com/TwoUnderscorez/serilog-sinks-conductor-task-log - Serilog sink for sending worker log events to Conductor</p> </li> <li> <p>https://github.com/davidwadden/conductor-workers - Various ready made Conductor workers for common operations on some platforms (ex.: Jira, Github, Concourse)</p> </li> </ul>"},{"location":"resources/related.html#conductor-web-ui","title":"Conductor Web UI","text":"<ul> <li>https://github.com/maheshyaddanapudi/conductor-ng-ui - Angular based - Conductor Workflow Management UI</li> </ul>"},{"location":"resources/related.html#conductor-persistence","title":"Conductor Persistence","text":""},{"location":"resources/related.html#mongo-persistence","title":"Mongo Persistence","text":"<ul> <li>https://github.com/maheshyaddanapudi/conductor/tree/mongo_persistence - With option to use Mongo Database as persistence unit.</li> <li>Mongo Persistence / Option to use Mongo Database as persistence unit.</li> <li>Docker Compose example with MongoDB Container.</li> </ul>"},{"location":"resources/related.html#oracle-persistence","title":"Oracle Persistence","text":"<ul> <li>https://github.com/maheshyaddanapudi/conductor/tree/oracle_persistence - With option to use Oracle Database as persistence unit.</li> <li>Oracle Persistence / Option to use Oracle Database as persistence unit : version &gt; 12.2 - Tested well with 19C</li> <li>Docker Compose example with Oracle Container.</li> </ul>"},{"location":"resources/related.html#schedule-conductor-workflow","title":"Schedule Conductor Workflow","text":"<ul> <li>https://github.com/jas34/scheduledwf - It solves the following problem statements:<ul> <li>At times there are use cases in which we need to run some tasks/jobs only at a scheduled time.</li> <li>In microservice architecture maintaining schedulers in various microservices is a pain.</li> <li>We should have a central dedicate service that can do scheduling for us and provide a trigger to a microservices at expected time.</li> </ul> </li> <li>It offers an additional module <code>io.github.jas34.scheduledwf.config.ScheduledWfServerModule</code> built on the existing core  of conductor and does not require deployment of any additional service.</li> </ul> <p>For more details refer: Schedule Conductor Workflows and Capability In Conductor To Schedule Workflows</p>"}]}